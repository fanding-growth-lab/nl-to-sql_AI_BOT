"""
LangGraph Node Components for NL-to-SQL Pipeline

This module implements the individual nodes that make up the LangGraph pipeline.
"""

import re
import logging
import sqlparse
import random
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

from .prompts import GeminiSQLGenerator, SQLPromptTemplate
from .fanding_sql_templates import FandingSQLTemplates
# Removed unused imports: enhanced_rag_mapper, data_insight_analyzer, dynamic_schema_expander

from .state import (
    GraphState, Entity, SchemaMapping, SQLResult, 
    QueryIntent, QueryComplexity,
    set_sql_result, set_rag_mapping_result, set_dynamic_pattern, 
    set_fanding_template, set_conversation_response, clear_sql_generation,
    get_effective_sql, is_sql_generation_skipped
)
from core.config import get_settings
from core.db import get_db_session, get_cached_db_schema, extract_table_names, extract_column_names, validate_sql_syntax
from core.logging import get_logger

logger = get_logger(__name__)


# Constants and Configuration for Agentic Flow
# Intent Classification Patterns
GREETING_PATTERNS = [
    "ì•ˆë…•", "ë°˜ê°€ì›Œ", "hello", "hi", "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ ì €ë…", 
    "í™˜ì˜", "ì¸ì‚¬", "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì•ˆë…•í•˜ì„¸ìš”",
    "ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤", "ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "ì¢‹ì€ í•˜ë£¨", "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”", "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”"
]

HELP_REQUEST_PATTERNS = [
    "ë„ì›€", "ì‚¬ìš©ë²•", "ì–´ë–»ê²Œ", "help", "ëª…ë ¹ì–´", 
    "ë„ì™€ì¤˜", "ì„¤ëª…", "ê°€ì´ë“œ", "ì‚¬ìš©ë²•", "ë„ì›€ë§",
    "ì‚¬ìš©ë²• ì•Œë ¤ì¤˜", "ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”", "ê¸°ëŠ¥", "ê¸°ëŠ¥ì´ ë­ì•¼",
    "ë­ê°€ ìˆì–´", "ë­˜ í•  ìˆ˜ ìˆì–´", "í•  ìˆ˜ ìˆëŠ” ê²ƒ", "ê¸°ëŠ¥ ì„¤ëª…",
    "ë„ˆê°€ í•  ìˆ˜ ìˆëŠ” ì¼", "ë­ì•¼", "ë­ì§€", "ë­”ê°€", "ë­”ë°"
]

GENERAL_CHAT_PATTERNS = [
    "ì–´ë•Œ", "ì–´ë– ", "ì¢‹ì•„", "ë‚˜ì˜", "ì¬ë¯¸", "ì¬ë¯¸ìˆ", "ì§€ë£¨", "í”¼ê³¤",
    "ë‚ ì”¨", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì£¼ë§", "íœ´ì¼", "ì¼", "ì¼ì •",
    "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë¯¸ì•ˆ", "ì£„ì†¡", "ê´œì°®", "ê´œì°®ì•„", "ê´œì°®ìŠµë‹ˆë‹¤",
    "ë­ì•¼", "ë­ì§€", "ë­”ê°€", "ë­”ë°", "ë­”ê°€ìš”", "ë­”ê°€ìš”?"
]

FANDING_DATA_KEYWORDS = [
    "ë©¤ë²„ì‹­", "ì„±ê³¼", "íšŒì›", "ë§¤ì¶œ", "ë°©ë¬¸ì", "ë¦¬í…ì…˜", "í¬ìŠ¤íŠ¸", 
    "ì¡°íšŒìˆ˜", "ì¸ê¸°", "ë¶„ì„", "í†µê³„", "ë¦¬í¬íŠ¸", "ì›”ê°„", 
    "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„", "í¬ë¦¬ì—ì´í„°", "í€ë”©", "í”„ë¡œì íŠ¸",
    "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”", "1ì›”", "2ì›”", "3ì›”", 
    "4ì›”", "5ì›”", "6ì›”", "7ì›”", "ì˜¬í•´", "ì‘ë…„", "ì§€ë‚œë‹¬", "ì´ë²ˆë‹¬",
    "ì‹ ê·œ", "ì´íƒˆ", "í™œì„±", "êµ¬ë…", "ê²°ì œ", "ìˆ˜ìµ", "ë§¤ì¶œì•¡",
    "í˜„í™©", "ìƒí™©", "ê²°ê³¼", "ì„±ê³¼ë¶„ì„", "ì„±ê³¼", "ë¶„ì„í•´ì¤˜", "ë³´ê³ ì„œ",
    "ìš”ì•½", "ì •ë¦¬", "í˜„ì¬", "ìµœê·¼", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼"
]

DATA_QUERY_PATTERNS = [
    "ì¡°íšŒ", "ê²€ìƒ‰", "ë³´ì—¬ì¤˜", "ì°¾ì•„", "í…Œì´ë¸”", "ì¿¼ë¦¬",
    "ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„",
    "ì•Œë ¤ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ê°€ì ¸ì™€", "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ",
    "ëª‡ ëª…", "ì–¼ë§ˆ", "ì–´ëŠ ì •ë„"
]

QUESTION_PATTERNS = [
    "ë­", "ë¬´ì—‡", "ì–´ë–¤", "ì–´ë””", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ëˆ„êµ¬",
    "ë­”ê°€", "ë­”ì§€", "ë­”ë°", "ë­ì•¼", "ë­ì§€", "ë­”ê°€ìš”", "ë­”ê°€ìš”?"
]

GRATITUDE_PATTERNS = [
    "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§ˆì›Œìš”", "ê³ ë§™ìŠµë‹ˆë‹¤",
    "ìˆ˜ê³ ", "ìˆ˜ê³ í•˜ì…¨", "ìˆ˜ê³ í•˜ì…¨ì–´ìš”", "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤"
]

# Response Templates
GREETING_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ Fanding Data Report ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë°ì´í„° ë¶„ì„ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤! ğŸ¤– ë©¤ë²„ì‹­ ì„±ê³¼ë‚˜ íšŒì› ë°ì´í„°ë¥¼ ì¡°íšŒí•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”.",
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ“Š Fanding ë°ì´í„°ë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
]

GENERAL_CHAT_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!",
    "ë„¤, ë“£ê³  ìˆì–´ìš”! ğŸ“Š Fanding ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.",
    "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ¤– ë©¤ë²„ì‹­ ì„±ê³¼ë‚˜ íšŒì› ë°ì´í„°ê°€ ê¶ê¸ˆí•˜ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.",
    "ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š ë°ì´í„° ë¶„ì„ì„ ë„ì™€ë“œë¦´ ì¤€ë¹„ê°€ ë˜ì–´ìˆì–´ìš”."
]

# SQL Security Keywords
DANGEROUS_SQL_KEYWORDS = [
    'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 
    'TRUNCATE', 'EXEC', 'EXECUTE', 'UNION', 'SCRIPT',
    'GRANT', 'REVOKE', 'COMMIT', 'ROLLBACK'
]

# Table Name Mapping
TABLE_NAME_MAPPING = {
    # íšŒì› ê´€ë ¨
    "users": "t_member",
    "user": "t_member",
    "members": "t_member",
    "member": "t_member",
    "íšŒì›": "t_member",
    "ì‚¬ìš©ì": "t_member",
    
    # íšŒì› ì •ë³´ ê´€ë ¨
    "user_info": "t_member_info",
    "member_info": "t_member_info",
    "íšŒì›ì •ë³´": "t_member_info",
    
    # íšŒì› í”„ë¡œí•„ ê´€ë ¨
    "user_profile": "t_member_profile",
    "member_profile": "t_member_profile",
    "profiles": "t_member_profile",
    "í”„ë¡œí•„": "t_member_profile",
    
    # í¬ë¦¬ì—ì´í„° ê´€ë ¨
    "creators": "t_creator",
    "creator": "t_creator",
    "í¬ë¦¬ì—ì´í„°": "t_creator",
    "ì°½ì‘ì": "t_creator",
    
    # í€ë”© ê´€ë ¨
    "fundings": "t_funding",
    "funding": "t_funding",
    "í€ë”©": "t_funding",
    "projects": "t_funding",
    "í”„ë¡œì íŠ¸": "t_funding",
    
    # í€ë”© ì°¸ì—¬ì
    "funding_members": "t_funding_member",
    "backers": "t_funding_member",
    "supporters": "t_funding_member",
    "í›„ì›ì": "t_funding_member",
    
    # íŒ”ë¡œìš° ê´€ê³„
    "follows": "t_follow",
    "follow": "t_follow",
    "íŒ”ë¡œìš°": "t_follow",
    
    # ì£¼ë¬¸ ê´€ë ¨
    "orders": "t_order",
    "order": "t_order",
    "ì£¼ë¬¸": "t_order",
}

# Korean to English Mappings
KOREAN_MAPPINGS = {
    'ë³´ì—¬ì¤˜': 'show',
    'ì°¾ì•„ì¤˜': 'find',
    'ê°€ì ¸ì™€': 'get',
    'ê°œìˆ˜': 'count',
    'í•©ê³„': 'sum',
    'í‰ê· ': 'average',
    'ìµœëŒ€': 'max',
    'ìµœì†Œ': 'min'
}

# Entity Extraction Keywords
MEMBER_KEYWORDS = ["íšŒì›", "ë©¤ë²„", "ì‚¬ìš©ì", "ìœ ì €", "member", "user", "íšŒì›ìˆ˜", "ë©¤ë²„ìˆ˜"]
CREATOR_KEYWORDS = ["í¬ë¦¬ì—ì´í„°", "ì°½ì‘ì", "ì‘ê°€", "ì•„í‹°ìŠ¤íŠ¸", "ì œì‘ì", "creator"]
DATE_KEYWORDS = ["ì‹ ê·œ", "í˜„í™©", "ì›”ê°„", "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„"]
LOGIN_KEYWORDS = ["ë¡œê·¸ì¸", "login", "ì ‘ì†"]
RANKING_KEYWORDS = ["top", "top5", "top10", "ìƒìœ„", "ìµœê³ ", "ë§ì€", "ì ì€", "ìˆœìœ„"]
STATISTICS_KEYWORDS = ["ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„", "ë¶„ì„"]

# Confidence Thresholds
LLM_CONFIDENCE_THRESHOLD_HIGH = 0.8
LLM_CONFIDENCE_THRESHOLD_MEDIUM = 0.6
LLM_CONFIDENCE_THRESHOLD_LOW = 0.3
RAG_CONFIDENCE_THRESHOLD = 0.6
SQL_GENERATION_CONFIDENCE_THRESHOLD = 0.7


# Response Generator Functions

def generate_greeting_response(user_query: str) -> str:
    """Generate a random greeting response."""
    return random.choice(GREETING_RESPONSES)

def generate_help_response(user_query: str) -> str:
    """Generate a help response."""
    return """ğŸ¤– **Fanding Data Report ë´‡ ì‚¬ìš©ë²•**

**ğŸ“Š ë°ì´í„° ì¡°íšŒ ê¸°ëŠ¥:**
â€¢ "í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜" - í™œì„± íšŒì› ìˆ˜ í™•ì¸
â€¢ "8ì›” ë©¤ë²„ì‹­ ì„±ê³¼ ë¶„ì„í•´ì¤˜" - íŠ¹ì • ì›” ì„±ê³¼ ë¶„ì„
â€¢ "ì „ì²´ íšŒì› ìˆ˜ ë³´ì—¬ì¤˜" - ì „ì²´ íšŒì› ìˆ˜ í™•ì¸
â€¢ "ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜" - ì‹ ê·œ íšŒì› í˜„í™©

**ğŸ’¡ ì‚¬ìš© íŒ:**
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš” (ì˜ˆ: "8ì›” ì„±ê³¼", "í™œì„± íšŒì›")
â€¢ ë‚ ì§œë‚˜ ê¸°ê°„ì„ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì˜ˆ: "ì´ë²ˆ ë‹¬", "ì§€ë‚œ ì£¼")
â€¢ ë©¤ë²„ì‹­, íšŒì›, ì„±ê³¼ ë“± í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”

**â“ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!**"""

def generate_general_chat_response(user_query: str) -> str:
    """Generate a random general chat response."""
    return random.choice(GENERAL_CHAT_RESPONSES)

def generate_error_response(error: Exception) -> str:
    """Generate user-friendly error response."""
    error_type = type(error).__name__
    
    # íŠ¹ì • ì—ëŸ¬ íƒ€ì…ë³„ ë§ì¶¤í˜• ì‘ë‹µ
    if "UnicodeEncodeError" in error_type:
        return """ğŸ˜… **ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì£„ì†¡í•©ë‹ˆë‹¤. íŠ¹ìˆ˜ ë¬¸ìë‚˜ ì´ëª¨ì§€ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.
ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ¤–"""
    
    elif "ConnectionError" in error_type or "TimeoutError" in error_type:
        return """ğŸŒ **ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ìˆì–´ìš”.
ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸ”„"""
    
    elif "ValueError" in error_type or "TypeError" in error_type:
        return """âš ï¸ **ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ë° ë¬¸ì œê°€ ìˆì—ˆì–´ìš”.
ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”! ğŸ’¡"""
    
    else:
        return """ğŸ˜” **ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.
ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ê¸°ìˆ íŒ€ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”! ğŸ› ï¸

**ğŸ’¡ ë„ì›€ë§:** "ì‚¬ìš©ë²• ì•Œë ¤ì¤˜"ë¼ê³  ë§ì”€í•´ì£¼ì‹œë©´ ì‚¬ìš©ë²•ì„ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."""

def generate_clarification_question(user_query: str) -> str:
    """Generate a clarification question for ambiguous queries."""
    q = user_query.lower()
    needs_topk = ("top" in q or "ìƒìœ„" in q or "top5" in q)
    needs_period = any(k in q for k in ["ì´ë²ˆ", "ì§€ë‚œ", "ì´ë²ˆë‹¬", "ì§€ë‚œë‹¬", "ì›”", "ë¶„ê¸°", "ì£¼", "week", "month", "quarter"])
    needs_metric = any(k in q for k in ["íšŒì›ìˆ˜", "ì‹ ê·œ", "í™œì„±", "ë¡œê·¸ì¸", "ì¡°íšŒìˆ˜", "ë§¤ì¶œ", "íŒë§¤"]) 
    
    parts = []
    if needs_period:
        parts.append("ê¸°ê°„(ì˜ˆ: 2025-08, ì§€ë‚œë‹¬)ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
    if needs_topk:
        parts.append("ìƒìœ„ K ê°œ(ì˜ˆ: Top5)ëŠ” ëª‡ ê°œë¥¼ ì›í•˜ì‹œë‚˜ìš”?")
    if needs_metric:
        parts.append("ì–´ë–¤ ì§€í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë­í‚¹ì„ ì›í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ì‹ ê·œ íšŒì›ìˆ˜)")
    if not parts:
        parts.append("ê¸°ê°„/ì§€í‘œ/Top-K ì¤‘ í•„ìš”í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.")
    
    return "ì§ˆì˜ë¥¼ ì •í™•íˆ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•´ ì£¼ì„¸ìš”: " + " ".join(parts)


class BaseNode(ABC):
    """Base class for all pipeline nodes."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(self.__class__.__name__)
    
    @abstractmethod
    def process(self, state: GraphState) -> GraphState:
        """Process the current state and return updated state."""
        pass
    
    def _log_processing(self, state: GraphState, component: str):
        """Log processing information."""
        self.logger.info(
            f"Processing {component}",
            user_id=state.get("user_id"),
            channel_id=state.get("channel_id"),
            query=state.get("user_query", "")[:100]
        )


class NLProcessor(BaseNode):
    """Natural Language Processing node for query analysis."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.llm = self._initialize_llm()
        self.fanding_templates = FandingSQLTemplates()
        # Removed: EnhancedRAGMapper and DynamicSchemaExpander (deleted modules)
        # self.rag_mapper = EnhancedRAGMapper(config)
        # self.schema_expander = DynamicSchemaExpander(config)
    
    def _initialize_llm(self):
        """Initialize the LLM for natural language processing."""
        settings = get_settings()
        try:
            return ChatGoogleGenerativeAI(
                model=settings.llm.model,
                google_api_key=settings.llm.api_key,
                temperature=settings.llm.temperature,
                max_output_tokens=settings.llm.max_tokens,
                request_timeout=10.0  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            )
        except Exception as e:
            self.logger.warning(f"Failed to initialize LLM: {str(e)}. Using mock LLM.")
            # Try to create a simple LLM instance for testing
            try:
                import os
                return ChatGoogleGenerativeAI(
                    model="gemini-2.5-pro",
                    google_api_key=os.environ.get('GOOGLE_API_KEY', ''),
                    temperature=0.1,
                    max_output_tokens=1024,
                    request_timeout=10.0  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                )
            except:
                return None
    
    def process(self, state: GraphState) -> GraphState:
        """Process natural language query and extract intent and entities."""
        self._log_processing(state, "NLProcessor")
        
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            user_query = state.get("user_query")
            if not user_query:
                self.logger.error("user_query is None or empty")
                state["conversation_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ˜Š"
                state["skip_sql_generation"] = True
                state["success"] = False
                return state
            
            # Normalize query
            normalized_query = self._normalize_query(user_query)
            
            # ì •ê·œí™”ëœ ì¿¼ë¦¬ ê²€ì¦
            if not normalized_query or len(normalized_query.strip()) == 0:
                self.logger.error("normalized_query is empty after processing")
                state["conversation_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ¤”"
                state["skip_sql_generation"] = True
                state["success"] = False
                return state
            
            # Extract intent and entities (LLM ê²°ê³¼ í¬í•¨)
            llm_intent_result = state.get("llm_intent_result")
            intent, entities = self._extract_intent_and_entities(normalized_query, llm_intent_result)   # NOTE: confidenceê°€ ë†’ë‹¤ë©´ LLMì˜ ê²ƒì„, ë‚®ë‹¤ë©´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ intentë¥¼ ë¶„ë¥˜í•˜ê³  í•„ìš”í•˜ë‹¤ë©´ entityë¥¼ ì¶”ì¶œí•¨
            
            # Update state
            state["normalized_query"] = normalized_query
            state["intent"] = intent
            state["entities"] = entities
            
            # ì¸ì‚¬ë§ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 1)
            if intent == QueryIntent.GREETING:
                response = self._handle_greeting(user_query)
                set_conversation_response(state, response, skip_sql=True)
                state["success"] = True
                self.logger.info(f"Greeting handled: {user_query}")
                return state
            
            # ë„ì›€ë§ ìš”ì²­ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 2)
            if intent == QueryIntent.HELP_REQUEST:
                response = self._handle_help_request(user_query)
                set_conversation_response(state, response, skip_sql=True)
                state["success"] = True
                self.logger.info(f"Help request handled: {user_query}")
                return state
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ìš”ì²­ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 3 - SHOW/DESCRIBE ëŒ€ì•ˆ)
            schema_info_response = self.fanding_templates.get_schema_info(user_query)
            if schema_info_response:
                state["conversation_response"] = schema_info_response
                state["intent"] = QueryIntent.HELP_REQUEST
                state["skip_sql_generation"] = True
                state["success"] = True
                self.logger.info(f"Schema information request handled: {user_query}")
                return state
            
            # ì¸í…íŠ¸ë³„ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
            if intent == QueryIntent.GENERAL_CHAT:
                # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
                response = self._handle_general_chat(user_query)
                set_conversation_response(state, response, skip_sql=True)
                clear_sql_generation(state)
                state["success"] = True
                self.logger.info(f"General chat handled: {intent}")
                
            elif intent == QueryIntent.DATA_QUERY:
                # ë°ì´í„° ì¡°íšŒ ì˜ë„ - Fanding í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„
                self.logger.info(f"Data query intent detected: {user_query}")
                self._handle_data_query(state, user_query)
                state["success"] = True
            
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ì¸í…íŠ¸ - ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                self.logger.warning(f"Unknown intent: {intent}, treating as general chat")
                state["skip_sql_generation"] = True
                state["conversation_response"] = self._handle_general_chat(user_query)
                state["sql_query"] = None
                state["validated_sql"] = None
                state["success"] = True
                self.logger.info(f"Unknown intent handled as general chat: {intent}")
            
            # Log confidence
            confidence = self._calculate_confidence(normalized_query, intent, entities)
            state["confidence_scores"]["nl_processing"] = confidence
            
            self.logger.info(f"Processed query: {normalized_query}")
            self.logger.info(f"Intent: {intent}, Entities: {len(entities)}")
            
        except Exception as e:
            self.logger.error(f"Error in NLProcessor: {str(e)}", exc_info=True)
            # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            error_response = self._generate_error_response(e)
            state["conversation_response"] = error_response
            state["skip_sql_generation"] = True
            state["sql_query"] = None
            state["validated_sql"] = None
            state["success"] = False
            state["error_message"] = f"Natural language processing failed: {str(e)}"
        
        return state
    
    def _handle_greeting(self, user_query: str) -> str:
        """ì¸ì‚¬ë§ ì²˜ë¦¬ (ëœë¤ ì‘ë‹µ)"""
        return generate_greeting_response(user_query)
    
    def _handle_help_request(self, user_query: str) -> str:
        """ë„ì›€ë§ ìš”ì²­ ì²˜ë¦¬"""
        return generate_help_response(user_query)
    
    def _handle_general_chat(self, user_query: str) -> str:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ëœë¤ ì‘ë‹µ)"""
        return generate_general_chat_response(user_query)
    
    def _generate_error_response(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return generate_error_response(error)
    
    def _handle_data_query(self, state: GraphState, user_query: str) -> None:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ ì²˜ë¦¬ (RAG + ë™ì  ìŠ¤í‚¤ë§ˆ í™•ì¥ í†µí•©)"""
        # ì• ë§¤í•œ ì¿¼ë¦¬ì¸ì§€ ë¨¼ì € í™•ì¸
        if self.fanding_templates.is_ambiguous_query(user_query):
            self.logger.info(f"ğŸ” Ambiguous query detected - requesting clarification: {user_query}")
            clarification_question = self.fanding_templates.generate_clarification_question(user_query)
            state["conversation_response"] = clarification_question
            state["skip_sql_generation"] = True
            self.logger.info("âœ… Generated clarification question for ambiguous query (this is normal behavior)")
            return
        
        # 1. RAG ë§¤í•‘ ì‹œë„ (ìš°ì„ ìˆœìœ„ ë†’ìŒ) - DISABLED: EnhancedRAGMapper deleted
        # try:
        #     rag_result = self.rag_mapper.map_query_to_schema(user_query, context={"prefer_detailed": True})
        #     if rag_result and rag_result.confidence > LLM_CONFIDENCE_THRESHOLD_HIGH:
        #         self.logger.info(f"RAG mapping successful: {rag_result.source.value} (confidence: {rag_result.confidence:.2f})")
        #         set_rag_mapping_result(state, rag_result)
        #         state["skip_sql_generation"] = False
        #         self.logger.info(f"RAG SQL applied: {rag_result.sql_template[:100]}...")
        #         return
        # except Exception as e:
        #     self.logger.warning(f"RAG mapping failed: {str(e)}")
        
        # Skip RAG mapping and go directly to Fanding templates
        
        # 2. Fanding í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„ (í´ë°±)
        fanding_template = self.fanding_templates.match_query_to_template(user_query)
        if fanding_template:
            self.logger.info(f"Fanding template matched: {fanding_template.name}")
            set_fanding_template(state, fanding_template)
            state["skip_sql_generation"] = False
            self.logger.info(f"SQL template applied: {fanding_template.sql_template}")
        else:
            # 3. ë™ì  ì›”ë³„ í…œí”Œë¦¿ ìƒì„± ì‹œë„ (ë©¤ë²„ì‹­ ì„±ê³¼ ê´€ë ¨)
            try:
                dynamic_template = self.fanding_templates.create_dynamic_monthly_template(user_query)
                if dynamic_template:
                    self.logger.info(f"Dynamic monthly template created: {dynamic_template.name}")
                    set_fanding_template(state, dynamic_template)
                    state["skip_sql_generation"] = False
                    self.logger.info(f"Dynamic SQL applied: {dynamic_template.sql_template[:100]}...")
                    return
            except Exception as e:
                self.logger.warning(f"Dynamic monthly template creation failed: {str(e)}")
            
            # 4. ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
            self.logger.info("No template/pattern matched, proceeding with general SQL generation")
            state["skip_sql_generation"] = False

    def _generate_conversation_response(self, intent: QueryIntent, query: str) -> str:
        """ì¸í…íŠ¸ë³„ ëŒ€í™” ì‘ë‹µ ìƒì„± (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        if intent == QueryIntent.GREETING:
            return """ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ íŒ¬ë”© ë°ì´í„° ë¦¬í¬íŠ¸ ì‹œìŠ¤í…œì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸, PF_bearbotì´ë¼ê³  í•´ìš”.

ì €ëŠ” í¬ë¦¬ì—ì´í„°ë‹˜ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ ê¶ê¸ˆí•œ ì ë“¤ì„ ë°”ë¡œë°”ë¡œ ì•Œë ¤ë“œë¦¬ëŠ” ì—­í• ì„ í•˜ê³  ìˆì–´ìš”.

ì œê°€ ì£¼ë¡œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒë“¤ì´ì—ìš”.

ë©¤ë²„ì‹­ ë°ì´í„° ë¶„ì„: íšŒì› ìˆ˜ë‚˜ ì‹ ê·œ/ì´íƒˆ í˜„í™©ì´ ì–´ë–¤ì§€ ì•Œë ¤ë“œë ¤ìš”.

ì›”ê°„ ì„±ê³¼ ë¦¬í¬íŠ¸: ë§¤ì¶œì´ë‚˜ ë°©ë¬¸ì, ë¦¬í…ì…˜ ê°™ì€ í•µì‹¬ ì„±ê³¼ë¥¼ ì •ë¦¬í•´ ë“œë ¤ìš”.

ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„: ì–´ë–¤ í¬ìŠ¤íŠ¸ê°€ ì¸ê¸°ê°€ ë§ì•˜ëŠ”ì§€ ì¡°íšŒìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”.

ìë™ ë¦¬í¬íŠ¸ ìƒì„±: ë§¤ì›” í¬ë¦¬ì—ì´í„°ë‹˜ê»˜ ê¼­ ë§ëŠ” ë¦¬í¬íŠ¸ë¥¼ ë§Œë“¤ì–´ ë“œë ¤ìš”.

ì˜ˆë¥¼ ë“¤ì–´, ì €ì—ê²Œ ì´ë ‡ê²Œ í•œë²ˆ ë¬¼ì–´ë³´ì„¸ìš”.

"8ì›” ë©¤ë²„ì‹­ ì„±ê³¼ ì–´ë• ì–´?"

"íšŒì› ìˆ˜ ë³€í™” ì¶”ì´ ë³´ì—¬ì¤˜"

"ì¸ê¸° í¬ìŠ¤íŠ¸ TOP5 ì•Œë ¤ì¤˜"

"ë¦¬í…ì…˜ í˜„í™©ì€?"

ë¬¼ë¡  "ì›”ë³„ ë§¤ì¶œ ì„±ì¥ë¥ "ì´ë‚˜ "ê³ ê° í‰ê·  ìˆ˜ëª…(LTV) ë¶„ì„" ê°™ì€ ì¢€ ë” ê¹Šì´ ìˆëŠ” ë¶„ì„ë„ ê°€ëŠ¥í•˜ë‹µë‹ˆë‹¤.

ê¶ê¸ˆí•œ ê²Œ ìƒê¸°ë©´ ì–¸ì œë“  í¸í•˜ê²Œ ì €ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”! ğŸ¤–"""
        
        elif intent == QueryIntent.HELP_REQUEST:
            return """ğŸ” **PF_bearbot ë„ì›€ë§ - Fanding Data Report**

**ğŸš€ ì£¼ìš” ê¸°ëŠ¥:**
â€¢ ğŸ“Š **ë©¤ë²„ì‹­ ë°ì´í„° ë¶„ì„**: íšŒì› ìˆ˜, ì‹ ê·œ/ì´íƒˆ, í™œì„±ë„ ë¶„ì„
â€¢ ğŸ“ˆ **ì„±ê³¼ ë¦¬í¬íŠ¸**: ì›”ê°„ ë§¤ì¶œ, ë°©ë¬¸ì, ë¦¬í…ì…˜ ë¶„ì„
â€¢ ğŸ” **ì½˜í…ì¸  ì„±ê³¼**: í¬ìŠ¤íŠ¸ ì¡°íšŒìˆ˜, ì¸ê¸° ì½˜í…ì¸  ë¶„ì„
â€¢ ğŸ“‹ **ìë™ ë¦¬í¬íŠ¸**: í¬ë¦¬ì—ì´í„° ë§ì¶¤í˜• ì›”ê°„ ë¦¬í¬íŠ¸ ìƒì„±

**ğŸ’¡ ê¸°ë³¸ ì‚¬ìš©ë²•:**
```
"8ì›” ë©¤ë²„ì‹­ ì„±ê³¼" â†’ ì›”ê°„ ì„±ê³¼ ë¦¬í¬íŠ¸
"íšŒì› ìˆ˜ ë³€í™” ì¶”ì´" â†’ íšŒì› ì¦ê° ë¶„ì„
"ì¸ê¸° í¬ìŠ¤íŠ¸ TOP5" â†’ ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„
"ë¦¬í…ì…˜ í˜„í™©" â†’ íšŒì› ìœ ì§€ìœ¨ ë¶„ì„
```

**ğŸ¯ ê³ ê¸‰ ë¶„ì„ ì˜ˆì‹œ:**
â€¢ "ì›”ë³„ ë§¤ì¶œ ì„±ì¥ë¥  ë¶„ì„"
â€¢ "ë©¤ë²„ì‹­ êµ¬ë… ê¸°ê°„ ë¶„í¬"
â€¢ "ê³ ê° í‰ê·  ìˆ˜ëª… ë¶„ì„"
â€¢ "í¬ìŠ¤íŠ¸ ë°œí–‰ê³¼ ë°©ë¬¸ì ìƒê´€ê´€ê³„"
â€¢ "ì‹ ê·œ vs ê¸°ì¡´ íšŒì› ë¹„ìœ¨"

**âš¡ ë¹ ë¥¸ ëª…ë ¹ì–´:**
â€¢ "ë„ì›€ë§" â†’ ì´ ë„ì›€ë§ í‘œì‹œ
â€¢ "ì„±ê³¼" â†’ ìµœê·¼ ì„±ê³¼ ìš”ì•½
â€¢ "ë¶„ì„" â†’ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ ëª©ë¡

ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤–"""
        
        elif intent == QueryIntent.GENERAL_CHAT:
            return """ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š

ì €ëŠ” **PF_bearbot**ì…ë‹ˆë‹¤! í¬ë¦¬ì—ì´í„°ë¥¼ ìœ„í•œ **Fanding Data Report** ì‹œìŠ¤í…œì„ ë„ì™€ë“œë¦¬ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ˆìš”.

**ğŸš€ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?**
â€¢ ğŸ“Š **ë©¤ë²„ì‹­ ë°ì´í„° ë¶„ì„**: íšŒì› ìˆ˜, ì‹ ê·œ/ì´íƒˆ, í™œì„±ë„ ë¶„ì„
â€¢ ğŸ“ˆ **ì„±ê³¼ ë¦¬í¬íŠ¸**: ì›”ê°„ ë§¤ì¶œ, ë°©ë¬¸ì, ë¦¬í…ì…˜ ë¶„ì„
â€¢ ğŸ” **ì½˜í…ì¸  ì„±ê³¼**: í¬ìŠ¤íŠ¸ ì¡°íšŒìˆ˜, ì¸ê¸° ì½˜í…ì¸  ë¶„ì„
â€¢ ğŸ“‹ **ìë™ ë¦¬í¬íŠ¸**: í¬ë¦¬ì—ì´í„° ë§ì¶¤í˜• ì›”ê°„ ë¦¬í¬íŠ¸ ìƒì„±

**ğŸ’¡ ê°„ë‹¨í•œ ì˜ˆì‹œ:**
â€¢ "8ì›” ë©¤ë²„ì‹­ ì„±ê³¼"
â€¢ "íšŒì› ìˆ˜ ë³€í™” ì¶”ì´"
â€¢ "ì¸ê¸° í¬ìŠ¤íŠ¸ TOP5"
â€¢ "ë¦¬í…ì…˜ í˜„í™©"

êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”! ğŸ¤–"""
        
        else:
            return """ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹

ì €ëŠ” **PF_bearbot**ì…ë‹ˆë‹¤! í¬ë¦¬ì—ì´í„°ë¥¼ ìœ„í•œ **Fanding Data Report** ì‹œìŠ¤í…œì„ ë„ì™€ë“œë¦¬ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ˆìš”.

**ğŸš€ ì£¼ìš” ê¸°ëŠ¥:**
â€¢ ğŸ“Š ë©¤ë²„ì‹­ ë°ì´í„° ë¶„ì„
â€¢ ğŸ“ˆ ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
â€¢ ğŸ” ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„
â€¢ ğŸ“‹ ìë™ ë¦¬í¬íŠ¸ ìƒì„±

êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”! ğŸ¤–"""
    
    def _normalize_query(self, query: str) -> str:
        """Normalize the user query."""
        # Remove extra whitespace
        normalized = re.sub(r'\s+', ' ', query.strip())
        
        # Convert to lowercase for consistency
        normalized = normalized.lower()
        
        # Handle common Korean database terms (ìƒìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        for korean, english in KOREAN_MAPPINGS.items():
            normalized = normalized.replace(korean, english)
        
        return normalized
    
    def _extract_intent_and_entities(self, query: str, llm_intent_result: Optional[Dict] = None) -> Tuple[QueryIntent, List[Entity]]:
        """Extract intent and entities from the query."""
        
        # 1. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (MEDIUM ì„ê³„ê°’ 0.6 ì‚¬ìš©)
        # MEDIUM ì„ê³„ê°’: LLM ë¶„ë¥˜ê°€ ìƒë‹¹íˆ í™•ì‹¤í•  ë•Œë§Œ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¶„ë¥˜ ë°©ì§€
        if llm_intent_result and llm_intent_result.get("confidence", 0) >= LLM_CONFIDENCE_THRESHOLD_MEDIUM:
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(f"Using LLM intent classification: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})")
                # ì—”í‹°í‹°ë„ ì¶”ì¶œ
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                self.logger.warning(f"Invalid LLM intent: {llm_intent_result.get('intent')}")
        
        # 2. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì°¸ê³  (LOW ì„ê³„ê°’ 0.3 ì‚¬ìš©)
        # LOW ì„ê³„ê°’: LLMì´ ë¶ˆí™•ì‹¤í•´ë„ ê·œì¹™ ê¸°ë°˜ë³´ë‹¤ëŠ” ë‚˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œí•œì˜ ì‹ ë¢°ë„ë¡œ ì°¸ê³ 
        if llm_intent_result and llm_intent_result.get("confidence", 0) >= LLM_CONFIDENCE_THRESHOLD_LOW:
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(f"Using LLM intent as fallback: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})")
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                pass
        
        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (LLM ì‹¤íŒ¨ ì‹œ fallback)
        if self._has_data_query_indicators(query):
            # ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆìœ¼ë©´ DATA_QUERYë¡œ ë¶„ë¥˜ (LLM ì‹¤íŒ¨í•´ë„)
            self.logger.info(f"Data query indicators detected, classifying as DATA_QUERY: {query}")
            entities = self._extract_entities_from_query(query)
            return QueryIntent.DATA_QUERY, entities
        
        # 4. ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹œë„ (ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        rule_based_intent = self._classify_intent_by_rules(query)
        
        if rule_based_intent != QueryIntent.UNKNOWN:
            # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ëœ ê²½ìš° (ì¸ì‚¬, ì¼ë°˜ ëŒ€í™” ë“±)
            return rule_based_intent, []
        
        # 5. ëª¨ë“  ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        return QueryIntent.GENERAL_CHAT, []
    
    def _extract_entities_from_query(self, query: str) -> List[Entity]:
        """ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
        query_lower = query.lower()
        
        # íšŒì› ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in MEMBER_KEYWORDS):
            entities.append(Entity(name="member", type="table", confidence=0.9))
        
        # í¬ë¦¬ì—ì´í„° ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in CREATOR_KEYWORDS):
            entities.append(Entity(name="creator", type="table", confidence=0.9))
        
        # ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in DATE_KEYWORDS):
            entities.append(Entity(name="date", type="column", confidence=0.8))
        
        # ë¡œê·¸ì¸ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in LOGIN_KEYWORDS):
            entities.append(Entity(name="login", type="table", confidence=0.8))
        
        # Top/ìˆœìœ„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in RANKING_KEYWORDS):
            entities.append(Entity(name="ranking", type="aggregation", confidence=0.8))
        
        # í†µê³„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in STATISTICS_KEYWORDS):
            entities.append(Entity(name="statistics", type="aggregation", confidence=0.8))
        
        return entities
    
    def _classify_intent_by_rules(self, query: str) -> QueryIntent:
        """ê·œì¹™ ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜ (ê°œì„ ëœ ë²„ì „)"""
        query_lower = query.lower().strip()
        
        # 1. ë¨¼ì € ëª…í™•í•œ ë¹„ë°ì´í„° ì˜ë„ íŒ¨í„´ í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        
        # 1-1. ì¸ì‚¬ë§ íŒ¨í„´ (ìµœìš°ì„ )
        if any(pattern in query_lower for pattern in GREETING_PATTERNS):
            return QueryIntent.GREETING
        
        # 1-2. ë„ì›€ë§ ìš”ì²­ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in HELP_REQUEST_PATTERNS):
            return QueryIntent.HELP_REQUEST
        
        # 1-3. ì¼ë°˜ ëŒ€í™” íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in GENERAL_CHAT_PATTERNS):
            return QueryIntent.GENERAL_CHAT
        
        # 2. Fanding ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ í™•ì¸ (ëª…í™•í•œ ë°ì´í„° ì¡°íšŒ ì˜ë„ë§Œ)
        if any(keyword in query_lower for keyword in FANDING_DATA_KEYWORDS):
            return QueryIntent.DATA_QUERY
        
        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ í‚¤ì›Œë“œ (ëª…í™•í•œ ì¡°íšŒ ì˜ë„)
        if any(pattern in query_lower for pattern in DATA_QUERY_PATTERNS):
            return QueryIntent.DATA_QUERY
        
        # 4. ì§ˆë¬¸ íŒ¨í„´ (ì˜ë¬¸ì‚¬ ê¸°ë°˜) - ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        if any(pattern in query_lower for pattern in QUESTION_PATTERNS):
            # ì§ˆë¬¸ì´ì§€ë§Œ ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
            return QueryIntent.GENERAL_CHAT
        
        # 5. ê°ì‚¬/ì¸ì‚¬ í‘œí˜„
        if any(pattern in query_lower for pattern in GRATITUDE_PATTERNS):
            return QueryIntent.GENERAL_CHAT
        
        # 6. ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜ (UNKNOWN ëŒ€ì‹ )
        return QueryIntent.GENERAL_CHAT
    
    def _has_data_query_indicators(self, query: str) -> bool:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        query_lower = query.lower()
        
        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ (ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •)
        data_keywords = [
            "ì¡°íšŒ", "ê²€ìƒ‰", "ë°ì´í„°", "í…Œì´ë¸”", "ì¿¼ë¦¬",
            "ì‚¬ìš©ì", "íšŒì›", "í¬ë¦¬ì—ì´í„°", "í€ë”©", "í”„ë¡œì íŠ¸", "ì£¼ë¬¸",
            "ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„",
            "ë©¤ë²„ì‹­", "ì„±ê³¼", "ë§¤ì¶œ", "ë°©ë¬¸ì", "ë¦¬í…ì…˜", "í¬ìŠ¤íŠ¸",
            "ì¡°íšŒìˆ˜", "ì¸ê¸°", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì›”ê°„", "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„",
            # ì¶”ê°€ í‚¤ì›Œë“œ
            "ë½‘ì•„ì¤˜", "ë½‘ì•„", "ì¶”ì¶œ", "ì„ íƒ", "ê³ ë¥´", "ì •ë ¬", "ìˆœìœ„",
            "top", "top5", "top10", "ìƒìœ„", "ìµœê³ ", "ë§ì€", "ì ì€",
            "íšŒì›ìˆ˜", "ë©¤ë²„ìˆ˜", "ì‚¬ìš©ììˆ˜", "ê°€ì…ì", "í™œì„±", "ì‹ ê·œ",
            "í¬ë¦¬ì—ì´í„°", "ì°½ì‘ì", "ì‘ê°€", "ì•„í‹°ìŠ¤íŠ¸", "ì œì‘ì"
        ]
        
        # ë°ì´í„° ì¡°íšŒì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´
        data_question_patterns = [
            "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ", "ëª‡ ëª…", "ëª‡ ê±´", "ëª‡ ê°œì˜", "ëª‡ ëª…ì˜",
            "ê°€ì ¸ì™€", "ì°¾ì•„ì¤˜", "ë³´ì—¬ì¤˜", "ì•Œë ¤ì¤˜"  # ë°ì´í„° ê´€ë ¨ ë§¥ë½ì—ì„œë§Œ
        ]
        
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ì œì™¸
        general_question_patterns = [
            "ë­ì•¼", "ë­”ê°€", "ë­”ì§€", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ",
            "í•  ìˆ˜ ìˆëŠ”", "í•  ìˆ˜ ìˆëŠ”ì§€", "í•  ìˆ˜ ìˆëŠ”ê²Œ", "í•  ìˆ˜ ìˆëŠ”ê²ƒ"
        ]
        
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë°ì´í„° ì¡°íšŒê°€ ì•„ë‹˜
        if any(pattern in query_lower for pattern in general_question_patterns):
            return False
            
        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œë‚˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš°ë§Œ
        return (any(keyword in query_lower for keyword in data_keywords) or
                any(pattern in query_lower for pattern in data_question_patterns))
    
    def _extract_with_llm(self, query: str) -> Tuple[QueryIntent, List[Entity]]:
        """LLMì„ ì‚¬ìš©í•œ ì¸í…íŠ¸ ë° ì—”í‹°í‹° ì¶”ì¶œ"""
        system_prompt = """
        You are a database query analyzer. Analyze the given natural language query and extract:
        1. Query intent (SELECT, COUNT, AGGREGATE, FILTER, JOIN, UNKNOWN)
        2. Entities (tables, columns, values, conditions)
        
        Return your analysis in JSON format:
        {
            "intent": "SELECT",
            "entities": [
                {"name": "users", "type": "table", "confidence": 0.9},
                {"name": "email", "type": "column", "confidence": 0.8}
            ]
        }
        """
        
        try:
            if self.llm is None:
                # LLMì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° ì¡°íšŒë¡œ ë¶„ë¥˜
                return QueryIntent.SELECT, []
            
            # ìµœì‹  LangChain ë°©ì‹: SystemMessage ëŒ€ì‹  HumanMessageì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
            messages = [
                HumanMessage(content=f"{system_prompt}\n\nAnalyze this query: {query}")
            ]
            
            response = self.llm.invoke(messages)
            result = self._parse_llm_response(response.content)
            
            intent = QueryIntent(result.get("intent", "UNKNOWN"))
            entities = [
                Entity(
                    name=entity["name"],
                    type=entity["type"],
                    confidence=entity["confidence"],
                    context=entity.get("context")
                )
                for entity in result.get("entities", [])
            ]
            
            return intent, entities
            
        except Exception as e:
            self.logger.error(f"Error extracting intent and entities: {str(e)}")
            return QueryIntent.UNKNOWN, []
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response and extract JSON."""
        try:
            import json
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {"intent": "UNKNOWN", "entities": []}
        except Exception as e:
            self.logger.error(f"Error parsing LLM response: {str(e)}")
            return {"intent": "UNKNOWN", "entities": []}
    
    def _calculate_confidence(self, query: str, intent: QueryIntent, entities: List[Entity]) -> float:
        """Calculate confidence score for the processing."""
        base_confidence = 0.8
        
        # Adjust based on intent clarity
        if intent != QueryIntent.UNKNOWN:
            base_confidence += 0.1
        
        # Adjust based on entity extraction
        if entities:
            avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
            base_confidence = (base_confidence + avg_entity_confidence) / 2
        
        return min(base_confidence, 1.0)


class SchemaMapper(BaseNode):
    """Database schema mapping node."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # ìºì‹±ëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
        self.db_schema = get_cached_db_schema()
        
        # í…Œì´ë¸” ì´ë¦„ ë§¤í•‘: ì¼ë°˜ì ì¸ ì´ë¦„ -> ì‹¤ì œ DB í…Œì´ë¸”ëª…
        self.table_name_mapping = TABLE_NAME_MAPPING
    
    def process(self, state: GraphState) -> GraphState:
        """Map entities to database schema."""
        self._log_processing(state, "SchemaMapper")
        
        try:
            entities = state.get("entities", [])
            intent = state.get("intent")
            
            # Map entities to schema
            relevant_tables = self._find_relevant_tables(entities)
            relevant_columns = self._find_relevant_columns(entities, relevant_tables)
            relationships = self._find_relationships(relevant_tables)
            
            # Calculate confidence
            confidence = self._calculate_mapping_confidence(
                entities, relevant_tables, relevant_columns
            )
            
            # Create schema mapping
            schema_mapping = SchemaMapping(
                relevant_tables=relevant_tables,
                relevant_columns=relevant_columns,
                relationships=relationships,
                confidence=confidence
            )
            
            state["schema_mapping"] = schema_mapping
            state["confidence_scores"]["schema_mapping"] = confidence
            
            self.logger.info(f"Mapped to {len(relevant_tables)} tables, {len(relevant_columns)} columns")
            
        except Exception as e:
            self.logger.error(f"Error in SchemaMapper: {str(e)}")
            state["error_message"] = f"Schema mapping failed: {str(e)}"
        
        return state
    
    def _find_relevant_tables(self, entities: List[Entity]) -> List[str]:
        """Find relevant tables based on entities."""
        relevant_tables = []
        
        for entity in entities:
            if entity.type == "table":
                # Direct table mention
                table_name = self._normalize_table_name(entity.name)
                if table_name in self.db_schema:
                    relevant_tables.append(table_name)
            elif entity.type == "column":
                # Find tables containing this column
                for table_name, table_info in self.db_schema.items():
                    if entity.name in table_info.get("columns", {}):
                        relevant_tables.append(table_name)
        
        return list(set(relevant_tables))
    
    def _find_relevant_columns(self, entities: List[Entity], tables: List[str]) -> List[str]:
        """Find relevant columns based on entities and tables."""
        relevant_columns = []
        
        for entity in entities:
            if entity.type == "column":
                relevant_columns.append(entity.name)
        
        # Add columns from relevant tables
        for table in tables:
            if table in self.db_schema:
                table_columns = list(self.db_schema[table].get("columns", {}).keys())
                relevant_columns.extend(table_columns)
        
        return list(set(relevant_columns))
    
    def _find_relationships(self, tables: List[str]) -> List[Dict[str, str]]:
        """Find relationships between tables."""
        relationships = []
        
        # Simple relationship detection based on common patterns
        for table1 in tables:
            for table2 in tables:
                if table1 != table2:
                    # Check for foreign key relationships
                    if self._has_foreign_key_relationship(table1, table2):
                        relationships.append({
                            "from_table": table1,
                            "to_table": table2,
                            "type": "foreign_key"
                        })
        
        return relationships
    
    def _has_foreign_key_relationship(self, table1: str, table2: str) -> bool:
        """Check if there's a foreign key relationship between tables."""
        if table1 not in self.db_schema or table2 not in self.db_schema:
            return False
        
        # Simple heuristic: check if table1 has a column that references table2
        table1_columns = self.db_schema[table1].get("columns", {})
        for column_name, column_info in table1_columns.items():
            if column_info.get("type") == "foreign_key" and table2 in str(column_info):
                return True
        
        return False
    
    def _normalize_table_name(self, name: str) -> str:
        """Normalize table name to match schema."""
        # Remove common prefixes/suffixes
        name = name.lower().strip()
        
        # ë¨¼ì € ë§¤í•‘ í…Œì´ë¸”ì—ì„œ í™•ì¸
        if name in self.table_name_mapping:
            return self.table_name_mapping[name]
        
        # Handle common variations
        if name.endswith('s'):
            singular_name = name[:-1]
            if singular_name in self.table_name_mapping:
                return self.table_name_mapping[singular_name]
        
        # Check if it matches any table in the schema
        for table_name in self.db_schema.keys():
            if name in table_name.lower() or table_name.lower() in name:
                return table_name
        
        return name
    
    
    def _calculate_mapping_confidence(self, entities: List[Entity], tables: List[str], columns: List[str]) -> float:
        """Calculate confidence for schema mapping."""
        if not entities:
            return 0.0
        
        # Base confidence from entity extraction
        avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
        
        # Bonus for finding relevant tables/columns
        mapping_bonus = 0.0
        if tables:
            mapping_bonus += 0.2
        if columns:
            mapping_bonus += 0.1
        
        return min(avg_entity_confidence + mapping_bonus, 1.0)


class SQLGenerationNode(BaseNode):
    """SQL ìƒì„± ì—ì´ì „íŠ¸ ë…¸ë“œ - ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.db_schema = config.get("db_schema", {})
    
        # GeminiSQLGenerator ì´ˆê¸°í™”
        self.sql_generator = self._initialize_sql_generator()
        
        # SQLPromptTemplate ì´ˆê¸°í™”
        self.prompt_template = SQLPromptTemplate(db_schema=self.db_schema)
        
        # FandingSQLTemplates ì´ˆê¸°í™”
        from .fanding_sql_templates import FandingSQLTemplates
        self.fanding_templates = FandingSQLTemplates()
        
    def _initialize_sql_generator(self):
        """SQL ìƒì„±ê¸° ì´ˆê¸°í™”"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            import os
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv("GOOGLE_API_KEY")
            
            return GeminiSQLGenerator(api_key=api_key)
        except Exception as e:
            self.logger.error(f"Failed to initialize SQL generator: {e}")
            return None
    
    def process(self, state: GraphState) -> GraphState:
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜"""
        self._log_processing(state, "SQLGenerationNode")
        
        try:
            # ì¼ë°˜ ëŒ€í™”ì¸ ê²½ìš° SQL ìƒì„± ê±´ë„ˆë›°ê¸°
            skip_flag = state.get("skip_sql_generation", False)
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            
            self.logger.info(f"SQLGenerationNode - skip_sql_generation: {skip_flag}")
            self.logger.info(f"SQLGenerationNode - conversation_response: {conversation_response is not None}")
            self.logger.info(f"SQLGenerationNode - intent: {intent}")
            
            # ëŒ€í™” ì¸í…íŠ¸ì¸ ê²½ìš° SQL ìƒì„± ê±´ë„ˆë›°ê¸°
            if (skip_flag or conversation_response or 
                intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]):
                # ì• ë§¤í•œ ì¿¼ë¦¬ë¡œ ì¸í•œ ëª…í™•í™” ì§ˆë¬¸ì¸ì§€ í™•ì¸
                if state.get("conversation_response") and "ì–´ë–¤" in str(state.get("conversation_response", "")):
                    self.logger.info("Skipping SQL generation - clarification question for ambiguous query")
                else:
                    self.logger.info("Skipping SQL generation for conversation intent")
                state["sql_query"] = None
                state["validated_sql"] = None
                state["confidence_scores"]["sql_generation"] = 1.0
                return state
            
            user_query = state["user_query"]
            schema_mapping = state.get("schema_mapping")
            
            # ëˆ„ì  ìŠ¬ë¡¯ ë³‘í•© (ì´ì „ state + í˜„ì¬ ì§ˆì˜)
            prior_slots = state.get("slots") or {}
            new_slots = self._extract_simple_slots(user_query)
            slots = {**prior_slots, **{k: v for k, v in new_slots.items() if v}}
            state["slots"] = slots
            
            # ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ë³´ë¥¼ SQL ìƒì„±ê¸°ì— ì„¤ì •
            if schema_mapping and self.sql_generator:
                # ê´€ë ¨ í…Œì´ë¸” ì •ë³´ë¥¼ ìŠ¤í‚¤ë§ˆì— ì¶”ê°€
                relevant_schema = {}
                for table_name in schema_mapping.relevant_tables:
                    if table_name in self.db_schema:
                        relevant_schema[table_name] = self.db_schema[table_name]
                
                self.sql_generator.set_schema(relevant_schema)
            
            # 1. ë™ì  SQL ìƒì„± ì‹œë„ (ì›”ë³„ ì¿¼ë¦¬ ë“±)
            dynamic_sql_result = state.get("dynamic_sql_result")
            # SQL ìƒì„± ì„ê³„ê°’(0.7) ì‚¬ìš©: ë™ì  SQL ìƒì„±ì€ ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•˜ë¯€ë¡œ ë†’ì€ ì‹ ë¢°ë„ ìš”êµ¬
            if dynamic_sql_result and dynamic_sql_result.get("confidence", 0) >= SQL_GENERATION_CONFIDENCE_THRESHOLD:
                self.logger.info("Using dynamic SQL generation result")
                set_sql_result(state, dynamic_sql_result["sql_query"], dynamic_sql_result["confidence"])
                return state
            
            # 2. RAG ë§¤í•‘ ê²°ê³¼ í™•ì¸ (ìµœìš°ì„ ) - ì‹ ë¢°ë„ ì„ê³„ê°’ ë‚®ì¶¤
            rag_result = state.get("rag_mapping_result")
            # RAG ì„ê³„ê°’(0.6) ì‚¬ìš©: RAG ë§¤í•‘ ê²°ê³¼ëŠ” ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì‹ ë¢°ë„ë¡œë„ ì‚¬ìš© ê°€ëŠ¥
            if rag_result and rag_result.confidence > RAG_CONFIDENCE_THRESHOLD:
                self.logger.info(f"Using RAG mapping result: {rag_result.source.value} (confidence: {rag_result.confidence:.2f})")
                set_sql_result(state, rag_result.sql_template, rag_result.confidence)
                return state
            
            # 2.5 ìŠ¬ë¡¯ ê¸°ë°˜ ê²°ì •ì  ë¹Œë“œ (ëˆ„ì  ìŠ¬ë¡¯ ì‚¬ìš©)
            slots = state.get("slots") or {}
            # intent ì¶”ë¡  ë³´ê°•: metricì´ active_membersì´ê³  creator/top_k/ì›”ì´ ì¡´ì¬í•˜ë©´ active_membersìš© intent
            if (slots.get("group_by") == "creator" or ("í¬ë¦¬ì—ì´í„°" in user_query)) and slots.get("top_k") and slots.get("month"):
                metric = slots.get("metric") or ("active_members" if ("í™œì„±" in user_query) else "new_members")
                # ëˆ„ì  ë°˜ì˜
                slots["metric"] = metric
                state["slots"] = slots
                month = slots.get("month")
                k = int(slots.get("top_k", 5))
                creator_col = self._guess_creator_column()
                if creator_col:
                    if metric == "active_members":
                        sql = (
                            "SELECT {creator_col}, COUNT(*) AS active_members "
                            "FROM t_member WHERE status = 'A' "
                            "GROUP BY {creator_col} ORDER BY active_members DESC LIMIT {k}"
                        ).format(creator_col=creator_col, k=k)
                    else:
                        sql = (
                            "SELECT {creator_col}, COUNT(DISTINCT member_no) AS new_members "
                            "FROM t_member_login_log "
                            "WHERE DATE_FORMAT(ins_datetime, '%Y-%m') = '{month}' "
                            "GROUP BY {creator_col} ORDER BY new_members DESC LIMIT {k}"
                        ).format(creator_col=creator_col, month=month, k=k)
                    state["sql_query"] = sql
                    state["confidence_scores"]["sql_generation"] = 0.8
                    self.logger.info("Built deterministic SQL using accumulated slots")
                    return state
                else:
                    clarification = (
                        "í¬ë¦¬ì—ì´í„° ì‹ë³„ ì»¬ëŸ¼ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ì»¬ëŸ¼ìœ¼ë¡œ ê·¸ë£¹í•‘í• ê¹Œìš”? ì˜ˆ: creator_id/creator_no"
                    )
                    state["clarification_question"] = clarification
                    state["conversation_response"] = True
                    state["conversation_text"] = clarification
                    state["confidence_scores"]["sql_generation"] = 0.0
                    return state
            
            # 3. ì´ë¯¸ SQLì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (Fanding í…œí”Œë¦¿ ë“±)
            existing_sql = state.get("sql_query")
            sql_validation_failed = state.get("sql_validation_failed", False)
            
            if existing_sql and not sql_validation_failed:
                self.logger.info(f"SQL already exists, skipping generation: {existing_sql[:100]}...")
                state["confidence_scores"]["sql_generation"] = 1.0
                return state
            elif existing_sql and sql_validation_failed:
                self.logger.info(f"Previous SQL validation failed, generating new SQL...")
                # SQL ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œìš´ SQL ìƒì„±
                state["sql_query"] = None
                state["sql_validation_failed"] = False
            
            # SQL ìƒì„±
            if self.sql_generator:
                result = self.sql_generator.generate_sql(user_query)
                
                if result["success"]:
                    state["sql_query"] = result["sql"]
                    state["sql_generation_metadata"] = {
                        "model": result.get("model"),
                        "prompt_length": result.get("prompt_length"),
                        "response_length": result.get("response_length"),
                        "mock": result.get("mock", False)
                    }
                    
                    # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
                    confidence = self._calculate_sql_confidence(result, schema_mapping)
                    state["confidence_scores"]["sql_generation"] = confidence
            
                    self.logger.info(f"Generated SQL: {result['sql']}")
                else:
                    # SQL ìƒì„± ì‹¤íŒ¨ ì‹œ Fanding í…œí”Œë¦¿ ì‹œë„
                    self.logger.warning(f"SQL generation failed: {result.get('error', 'Unknown error')}")
                    self.logger.info("Attempting Fanding template fallback...")
                    
                    fanding_template = self.fanding_templates.match_query_to_template(user_query)
                    if fanding_template:
                        state["sql_query"] = fanding_template.sql_template
                        state["fanding_template"] = fanding_template
                        state["confidence_scores"]["sql_generation"] = 0.8  # í…œí”Œë¦¿ ì‚¬ìš© ì‹œ ì¤‘ê°„ ì‹ ë¢°ë„
                        self.logger.info(f"Fanding template fallback successful: {fanding_template.name}")
                    else:
                        # DATA_QUERYì¸ë° ìƒì„± ì‹¤íŒ¨ ì‹œ: ëª…í™•í™” ì§ˆë¬¸ ìš”ì²­
                        clarification = self._build_clarification_question(user_query)
                        state["clarification_question"] = clarification
                        state["conversation_response"] = clarification  # ë¬¸ìì—´ë¡œ ì„¤ì •
                        state["conversation_text"] = clarification
                        state["confidence_scores"]["sql_generation"] = 0.0
                        self.logger.info("Asking clarification instead of switching to generic conversation")
            else:
                # SQL ìƒì„±ê¸° ì—†ìŒ: ëª…í™•í™” ì§ˆë¬¸ ìš”ì²­
                clarification = self._build_clarification_question(user_query)
                state["clarification_question"] = clarification
                state["conversation_response"] = clarification  # ë¬¸ìì—´ë¡œ ì„¤ì •
                state["conversation_text"] = clarification
                state["confidence_scores"]["sql_generation"] = 0.0
                self.logger.warning("No SQL generator available, asking for clarification")
            
        except Exception as e:
            self.logger.error(f"Error in SQLGenerationNode: {str(e)}")
            state["error_message"] = f"SQL generation failed: {str(e)}"
            state["confidence_scores"]["sql_generation"] = 0.0
        
        return state
    
    def _build_clarification_question(self, user_query: str) -> str:
        """ê°„ë‹¨í•œ ëª…í™•í™” ì§ˆë¬¸ ìƒì„± (ê¸°ê°„/Top-K/ì§€í‘œ ìš°ì„ )"""
        return generate_clarification_question(user_query)
    
    def _calculate_sql_confidence(self, result: Dict[str, Any], schema_mapping) -> float:
        """SQL ìƒì„± ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.8
        
        # ëª¨ì˜ SQLì¸ ê²½ìš° ì‹ ë¢°ë„ ê°ì†Œ
        if result.get("mock", False):
            base_confidence *= 0.7
        
        # ìŠ¤í‚¤ë§ˆ ë§¤í•‘ì´ ìˆëŠ” ê²½ìš° ë³´ë„ˆìŠ¤
        if schema_mapping and schema_mapping.relevant_tables:
            base_confidence += 0.1
        
        # ì‘ë‹µ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        response_length = result.get("response_length", 0)
        if response_length > 100:
            base_confidence += 0.05
        
        return min(base_confidence, 1.0)
    
    def _generate_fallback_sql(self, query: str) -> str:
        """Fallback SQL ìƒì„± (API ì‚¬ìš© ë¶ˆê°€ ì‹œ)"""
        query_lower = query.lower()
        
        if "íšŒì›" in query_lower or "ì‚¬ìš©ì" in query_lower:
            if "ìˆ˜" in query_lower or "ê°œìˆ˜" in query_lower:
                return "SELECT COUNT(*) FROM t_member;"
            else:
                return "SELECT * FROM t_member LIMIT 100;"
        elif "í¬ë¦¬ì—ì´í„°" in query_lower:
            return "SELECT nickname, description FROM t_creator LIMIT 100;"
        elif "í€ë”©" in query_lower or "í”„ë¡œì íŠ¸" in query_lower:
            return "SELECT title, goal_amount, current_amount FROM t_funding LIMIT 100;"
        else:
            return "SELECT 1 as placeholder;"
    
    def _extract_simple_slots(self, query: str) -> Dict[str, Any]:
        """ê°„ë‹¨ ìŠ¬ë¡¯ ì¶”ì¶œ: month, top_k, intent(creator_topk_new_members)"""
        q = query.lower()
        from .date_utils import DateUtils
        month = DateUtils.get_analysis_month(query)
        # top-k
        top_k = 5
        m = re.search(r"top\s*(\d+)|ìƒìœ„\s*(\d+)", q)
        if m:
            top_k = int([g for g in m.groups() if g][0])
        # intent
        intent = None
        if ("í¬ë¦¬ì—ì´í„°" in q or "creator" in q) and ("top" in q or "ìƒìœ„" in q) and ("ì‹ ê·œ" in q or "íšŒì›" in q):
            intent = "creator_topk_new_members"
        return {"month": month, "top_k": top_k, "intent": intent}
    
    def _guess_creator_column(self) -> Optional[str]:
        """db_schemaì—ì„œ ê°€ëŠ¥í•œ í¬ë¦¬ì—ì´í„° ì‹ë³„ ì»¬ëŸ¼ ì¶”ì •"""
        # ë¡œê·¸ì¸ ë¡œê·¸ í…Œì´ë¸” ê¸°ì¤€ìœ¼ë¡œ íƒìƒ‰
        table = self.db_schema.get("t_member_login_log") or {}
        candidates = ["creator_id", "creator_no", "creator", "channel_id", "influencer_id"]
        for c in candidates:
            if c in table:
                return c
        return None


class SQLValidationNode(BaseNode):
    """SQL ê²€ì¦ ì—ì´ì „íŠ¸ ë…¸ë“œ - ìƒì„±ëœ SQLì˜ êµ¬ë¬¸ ë° ì˜ë¯¸ ê²€ì¦"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # ìºì‹±ëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
        self.db_schema = get_cached_db_schema()
    
        
    def process(self, state: GraphState) -> GraphState:
        """SQL ì¿¼ë¦¬ ê²€ì¦"""
        self._log_processing(state, "SQLValidationNode")
        
        try:
            # ëŒ€í™” ì‘ë‹µì´ ìˆëŠ” ê²½ìš° ê²€ì¦ ê±´ë„ˆë›°ê¸°
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            
            self.logger.info(f"SQLValidationNode - conversation_response: {conversation_response is not None}")
            self.logger.info(f"SQLValidationNode - intent: {intent}")
            
            if (conversation_response or 
                intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]):
                self.logger.info("Skipping SQL validation for conversation response")
                state["validation_result"] = {"is_valid": True, "message": "Conversation response - validation skipped"}
                state["is_valid"] = True
                return state
            
            sql_query = state.get("sql_query")
            if not sql_query:
                state["error_message"] = "No SQL query to validate"
                return state
            
            # ê¸°ë³¸ êµ¬ë¬¸ ê²€ì¦ (ìƒˆë¡œìš´ SQL íŒŒì„œ ì‚¬ìš©)
            syntax_validation = validate_sql_syntax(sql_query)
            
            # ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦ (ì‹¤ì œ ê²€ì¦ ìˆ˜í–‰)
            schema_validation = self._validate_schema_compatibility(sql_query)
            
            # ë³´ì•ˆ ê²€ì¦
            security_validation = self._validate_security(sql_query)
            
            # ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ ìë™ ìˆ˜ì • ì‹œë„
            corrected_sql = sql_query
            if not schema_validation["is_valid"] and "corrections" in schema_validation:
                corrected_sql = self._apply_schema_corrections(sql_query, schema_validation["corrections"])
                if corrected_sql != sql_query:
                    self.logger.info(f"SQL auto-corrected: {sql_query[:100]}... -> {corrected_sql[:100]}...")
                    state["sql_query"] = corrected_sql
                    state["sql_corrected"] = True
            
            # ì¢…í•© ê²€ì¦ ê²°ê³¼
            is_valid = all([
                syntax_validation["is_valid"],
                schema_validation["is_valid"],
                security_validation["is_valid"]
            ])
            
            validation_result = {
                "is_valid": is_valid,
                "syntax": syntax_validation,
                "schema": schema_validation,
                "security": security_validation,
                "suggestions": self._generate_suggestions(sql_query, [
                    syntax_validation,
                    schema_validation,
                    security_validation
                ])
            }
            
            state["sql_validation"] = validation_result
            state["validation_result"] = validation_result
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_validation_confidence(validation_result)
            state["confidence_scores"]["sql_validation"] = confidence
            
            if is_valid:
                state["validated_sql"] = sql_query
                state["sql_validation_failed"] = False
                self.logger.info("SQL validation passed")
            else:
                state["sql_validation_failed"] = True
                self.logger.warning(f"SQL validation failed: {validation_result['suggestions']}")
            
        except Exception as e:
            self.logger.error(f"Error in SQLValidationNode: {str(e)}")
            state["error_message"] = f"SQL validation failed: {str(e)}"
            state["confidence_scores"]["sql_validation"] = 0.0
        
        return state
    
    
    def _validate_schema_compatibility(self, sql_query: str) -> Dict[str, Any]:
        """ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦ ë° ìë™ ìˆ˜ì •"""
        try:
            issues = []
            corrections = []
            
            # ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ í™•ì¸ (ìºì‹±ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            actual_schema = self.db_schema
            
            # í…Œì´ë¸”ëª… ê²€ì¦ (ìƒˆë¡œìš´ SQL íŒŒì„œ ì‚¬ìš©)
            table_names = extract_table_names(sql_query)
            for table_name in table_names:
                if table_name not in actual_schema:
                    # ìœ ì‚¬í•œ í…Œì´ë¸”ëª… ì°¾ê¸°
                    similar_table = self._find_similar_table(table_name, actual_schema)
                    if similar_table:
                        issues.append(f"Table '{table_name}' not found, did you mean '{similar_table}'?")
                        corrections.append(f"Replace '{table_name}' with '{similar_table}'")
                    else:
                        issues.append(f"Table '{table_name}' not found in schema")
            
            # ì»¬ëŸ¼ëª… ê²€ì¦ (íŠ¹íˆ ins_datetime ë¬¸ì œ)
            if 'ins_datetime' in sql_query:
                # t_member_login_log í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ins_datetime ì»¬ëŸ¼ì´ ì˜¬ë°”ë¦„
                if 't_member_login_log' in sql_query:
                    # t_member_login_log í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    login_log_table = actual_schema.get('t_member_login_log', {})
                    login_log_columns = login_log_table.get('columns', {})
                    if 'ins_datetime' not in login_log_columns:
                        issues.append("Column 'ins_datetime' not found in t_member_login_log table")
                    # t_member_login_logë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ëŠ” ìœ íš¨í•¨
                elif 't_member_info' in sql_query:
                    # t_member_info í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    member_info_table = actual_schema.get('t_member_info', {})
                    member_info_columns = member_info_table.get('columns', {})
                    if 'ins_datetime' not in member_info_columns:
                        issues.append("Column 'ins_datetime' not found in t_member_info table")
                        corrections.append("Verify t_member_info table schema")
                elif 't_member' in sql_query:
                    # t_member í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (t_memberëŠ” ins_datetimeì´ ì—†ìŒ)
                    member_table = actual_schema.get('t_member', {})
                    member_columns = member_table.get('columns', {})
                    if 'ins_datetime' not in member_columns:
                        issues.append("Column 'ins_datetime' not found in t_member table")
                        corrections.append("Use t_member_info table instead of t_member for ins_datetime column")
            
            return {
                "is_valid": len(issues) == 0,
                "issues": issues,
                "corrections": corrections,
                "details": "Schema validation completed"
            }
            
        except Exception as e:
            return {
                "is_valid": False,
                "error": "Schema validation error",
                "details": str(e)
            }
    
    
    def _find_similar_table(self, table_name: str, schema: Dict[str, Any]) -> Optional[str]:
        """ìœ ì‚¬í•œ í…Œì´ë¸”ëª… ì°¾ê¸°"""
        table_name_lower = table_name.lower()
        
        # ì •í™•í•œ ë§¤ì¹­
        if table_name in schema:
            return table_name
        
        # ë¶€ë¶„ ë§¤ì¹­
        for actual_table in schema.keys():
            if table_name_lower in actual_table.lower() or actual_table.lower() in table_name_lower:
                return actual_table
        
        return None
    
    def _apply_schema_corrections(self, sql_query: str, corrections: List[str]) -> str:
        """ìŠ¤í‚¤ë§ˆ ìˆ˜ì •ì‚¬í•­ì„ SQLì— ì ìš©"""
        corrected_sql = sql_query
        
        for correction in corrections:
            if "Replace 'ins_datetime' with" in correction:
                # ins_datetimeì„ ëŒ€ì²´ ì»¬ëŸ¼ìœ¼ë¡œ êµì²´
                alt_col = correction.split("'")[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë”°ì˜´í‘œ ì•ˆì˜ ê°’
                corrected_sql = re.sub(r'\bins_datetime\b', alt_col, corrected_sql, flags=re.IGNORECASE)
                self.logger.info(f"Replaced 'ins_datetime' with '{alt_col}'")
        
        return corrected_sql
    
    def _validate_security(self, sql_query: str) -> Dict[str, Any]:
        """SQL ë³´ì•ˆ ê²€ì¦"""
        try:
            issues = []
            sql_upper = sql_query.upper()
            
            # ìœ„í—˜í•œ í‚¤ì›Œë“œ í™•ì¸ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤) - ìƒìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            for keyword in DANGEROUS_SQL_KEYWORDS:
                # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
                pattern = r'\b' + re.escape(keyword) + r'\b'
                if re.search(pattern, sql_upper):
                    issues.append(f"Dangerous keyword detected: {keyword}")
            
            # ì£¼ì„ í™•ì¸ (SQL ì¸ì ì…˜ ë°©ì§€) - í…œí”Œë¦¿ì˜ ì •ë‹¹í•œ ì£¼ì„ì€ í—ˆìš©
            # ë©€í‹°ë¼ì¸ ì£¼ì„ /* */ë§Œ ì°¨ë‹¨ (ë‹¨ì¼ ë¼ì¸ ì£¼ì„ -- ëŠ” í—ˆìš©)
            if '/*' in sql_query or '*/' in sql_query:
                issues.append("Suspicious multi-line comment detected")
            
            # ë‹¨ì¼ ë¼ì¸ ì£¼ì„(--)ì€ í—ˆìš© (í…œí”Œë¦¿ì—ì„œ ì •ë‹¹í•˜ê²Œ ì‚¬ìš©ë¨)
            
            return {
                "is_valid": len(issues) == 0,
                "issues": issues,
                "details": "Security validation completed"
            }
            
        except Exception as e:
            return {
                "is_valid": False,
                "error": "Security validation error",
                "details": str(e)
            }
    
    
    def _generate_suggestions(self, sql_query: str, validations: List[Dict[str, Any]]) -> List[str]:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        for validation in validations:
            if not validation["is_valid"]:
                if "issues" in validation:
                    suggestions.extend(validation["issues"])
                elif "error" in validation:
                    suggestions.append(validation["error"])
        
        return suggestions
    
    def _calculate_validation_confidence(self, validation_result: Dict[str, Any]) -> float:
        """ê²€ì¦ ì‹ ë¢°ë„ ê³„ì‚°"""
        if validation_result["is_valid"]:
            return 1.0
        
        # ê° ê²€ì¦ í•­ëª©ì˜ ê°€ì¤‘ì¹˜
        weights = {
            "syntax": 0.4,
            "schema": 0.4,
            "security": 0.2
        }
        
        confidence = 0.0
        for validation_type, weight in weights.items():
            if validation_result[validation_type]["is_valid"]:
                confidence += weight
        
        return confidence


class DataSummarizationNode(BaseNode):
    """ë°ì´í„° ìš”ì•½ ì—ì´ì „íŠ¸ ë…¸ë“œ - SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.llm = self._initialize_llm()
        # Removed: DataInsightAnalyzer (deleted module)
        # self.insight_analyzer = DataInsightAnalyzer(config)
        
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        try:
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            import os
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv("GOOGLE_API_KEY")
            
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-pro",
                google_api_key=api_key,
                temperature=0.3,
                max_output_tokens=1024,
                request_timeout=10.0  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            )
        except Exception as e:
            self.logger.warning(f"Failed to initialize LLM: {str(e)}")
            return None
    
    def process(self, state: GraphState) -> GraphState:
        """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½"""
        self._log_processing(state, "DataSummarizationNode")
        
        try:
            # ëŒ€í™” ì‘ë‹µì´ ìˆëŠ” ê²½ìš° ìš”ì•½ ê±´ë„ˆë›°ê¸°
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            fanding_template = state.get("fanding_template")
            
            self.logger.info(f"DataSummarizationNode - conversation_response: {conversation_response is not None}")
            self.logger.info(f"DataSummarizationNode - intent: {intent}")
            self.logger.info(f"DataSummarizationNode - fanding_template: {fanding_template is not None}")
            
            if (conversation_response or 
                intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]):
                self.logger.info("Skipping data summarization for conversation response")
                state["data_summary"] = conversation_response or "ëŒ€í™” ì‘ë‹µì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                state["success"] = True
                return state
            
            # Fanding í…œí”Œë¦¿ì´ ìˆëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if fanding_template:
                query_result = state.get("query_result")
                if query_result:
                    # Fanding í…œí”Œë¦¿ ê²°ê³¼ í¬ë§·íŒ…
                    from .fanding_sql_templates import FandingSQLTemplates
                    templates = FandingSQLTemplates()
                    formatted_result = templates.format_sql_result(fanding_template, query_result)
                    state["data_summary"] = formatted_result
                    state["success"] = True
                    self.logger.info(f"ğŸ¯ Fanding template result formatted: {fanding_template.name}")
                    return state
            
            query_result = state.get("query_result")
            user_query = state.get("user_query")
            
            if not query_result:
                state["error_message"] = "No query result to summarize"
                return state
            
            # ê²°ê³¼ ë°ì´í„° ë¶„ì„
            result_stats = self._analyze_results(query_result)
            
            # ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ìˆ˜í–‰ - DISABLED: DataInsightAnalyzer deleted
            # try:
            #     sql_query = state.get("sql_query", "")
            #     insight_report = self.insight_analyzer.analyze_data(user_query, query_result, sql_query)
            #     
            #     # ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ìƒíƒœì— ì €ì¥
            #     state["insight_report"] = insight_report
            #     state["business_insights"] = insight_report.insights
            #     state["insight_summary"] = insight_report.summary
            #     
            #     # ì¸ì‚¬ì´íŠ¸ê°€ ìˆëŠ” ê²½ìš° ìš”ì•½ì— í¬í•¨
            #     if insight_report.insights:
            #         insight_text = self.insight_analyzer.format_insight_report(insight_report)
            #         state["insight_report_formatted"] = insight_text
            #     self.logger.info(f"Generated {len(insight_report.insights)} business insights")
            #     
            # except Exception as e:
            #     self.logger.warning(f"Insight analysis failed: {e}")
            #     # ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ìš”ì•½ì€ ê³„ì† ì§„í–‰
            #     state["insight_report"] = None
            #     state["business_insights"] = []
            
            # Set default values since insight analyzer is disabled
            state["insight_report"] = None
            state["business_insights"] = []
            
            # ìš”ì•½ ìƒì„±
            if self.llm:
                summary = self._generate_ai_summary(user_query, query_result, result_stats)
            else:
                summary = self._generate_fallback_summary(query_result, result_stats)
            
            state["data_summary"] = summary
            state["result_statistics"] = result_stats
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_summary_confidence(summary, result_stats)
            state["confidence_scores"]["data_summarization"] = confidence
            
            self.logger.info(f"Generated summary: {summary[:100]}...")
            
        except Exception as e:
            self.logger.error(f"Error in DataSummarizationNode: {str(e)}")
            state["error_message"] = f"Data summarization failed: {str(e)}"
            state["confidence_scores"]["data_summarization"] = 0.0
        
        return state
    
    def _analyze_results(self, query_result: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ ë°ì´í„° í†µê³„ ë¶„ì„"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return {"row_count": 0, "columns": [], "data_types": {}}
        
        stats = {
            "row_count": len(query_result),
            "columns": list(query_result[0].keys()) if query_result else [],
            "data_types": {},
            "sample_values": {},
            "null_counts": {}
        }
        
        if query_result:
            # ë°ì´í„° íƒ€ì… ë¶„ì„
            for column in stats["columns"]:
                sample_values = [row.get(column) for row in query_result[:5]]
                stats["sample_values"][column] = sample_values
                
                # NULL ê°’ ê°œìˆ˜
                null_count = sum(1 for row in query_result if row.get(column) is None)
                stats["null_counts"][column] = null_count
                
                # ë°ì´í„° íƒ€ì… ì¶”ë¡ 
                non_null_values = [v for v in sample_values if v is not None]
                if non_null_values:
                    first_value = non_null_values[0]
                    if isinstance(first_value, int):
                        stats["data_types"][column] = "integer"
                    elif isinstance(first_value, float):
                        stats["data_types"][column] = "float"
                    elif isinstance(first_value, str):
                        stats["data_types"][column] = "string"
                    else:
                        stats["data_types"][column] = "unknown"
        
        return stats
    
    def _generate_ai_summary(self, user_query: str, query_result: List[Dict[str, Any]], stats: Dict[str, Any]) -> str:
        """AIë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±"""
        try:
            # ê²°ê³¼ ë°ì´í„° í¬ë§·íŒ…
            formatted_results = self._format_results(query_result)
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {user_query}

ì¿¼ë¦¬ ê²°ê³¼ í†µê³„:
- ì´ í–‰ ìˆ˜: {stats['row_count']}
- ì»¬ëŸ¼ ìˆ˜: {len(stats['columns'])}
- ì»¬ëŸ¼ëª…: {', '.join(stats['columns'])}

ìƒ˜í”Œ ë°ì´í„°:
{formatted_results[:500]}...

ìš”êµ¬ì‚¬í•­:
1. ê²°ê³¼ì˜ ì£¼ìš” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…
2. ë°ì´í„°ì˜ ê·œëª¨ì™€ íŠ¹ì§•ì„ ì–¸ê¸‰
3. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
4. 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
5. í•œêµ­ì–´ë¡œ ì‘ì„±

ìš”ì•½:
"""
            
            # ìµœì‹  LangChain ë°©ì‹: SystemMessage ëŒ€ì‹  HumanMessageì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
            messages = [
                HumanMessage(content=f"ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{summary_prompt}")
            ]
            
            response = self.llm.invoke(messages)
            return response.content.strip()
            
        except Exception as e:
            self.logger.error(f"AI summary generation failed: {e}")
            return self._generate_fallback_summary(query_result, stats)
    
    def _generate_fallback_summary(self, query_result: List[Dict[str, Any]], stats: Dict[str, Any]) -> str:
        """Fallback ìš”ì•½ ìƒì„±"""
        row_count = stats.get("row_count", 0)
        columns = stats.get("columns", [])
        
        if row_count == 0:
            return "ì¿¼ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        elif row_count == 1:
            return f"ì´ 1ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
        else:
            return f"ì´ {row_count}ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
    
    def _format_results(self, query_result: List[Dict[str, Any]], max_rows: int = 10) -> str:
        """ê²°ê³¼ ë°ì´í„°ë¥¼ í¬ë§·íŒ…"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return "ê²°ê³¼ ì—†ìŒ"
        
        formatted_rows = []
        for i, row in enumerate(query_result[:max_rows]):
            row_str = f"í–‰ {i+1}: {dict(row)}"
            formatted_rows.append(row_str)
        
        result = "\n".join(formatted_rows)
        
        if len(query_result) > max_rows:
            result += f"\n... ë° {len(query_result) - max_rows}ê°œ í–‰ ë”"
        
        return result
    
    def _calculate_summary_confidence(self, summary: str, stats: Dict[str, Any]) -> float:
        """ìš”ì•½ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.8
        
        # ìš”ì•½ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        if len(summary) > 50:
            base_confidence += 0.1
        
        # í†µê³„ ì •ë³´ í™œìš©ë„ì— ë”°ë¥¸ ì¡°ì •
        if stats.get("row_count", 0) > 0:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)

