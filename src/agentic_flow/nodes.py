"""
LangGraph Node Components for NL-to-SQL Pipeline

This module implements the individual nodes that make up the LangGraph pipeline.
"""

import re
import logging
import sqlparse
import random
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

from .prompts import GeminiSQLGenerator, SQLPromptTemplate
from .fanding_sql_templates import FandingSQLTemplates

# Removed unused imports: enhanced_rag_mapper, data_insight_analyzer, dynamic_schema_expander

from .state import (
    GraphState,
    Entity,
    SchemaMapping,
    SQLResult,
    QueryIntent,
    QueryComplexity,
    set_sql_result,
    set_rag_mapping_result,
    set_dynamic_pattern,
    set_fanding_template,
    set_conversation_response,
    clear_sql_generation,
    get_effective_sql,
    is_sql_generation_skipped,
)
from core.config import get_settings
from core.db import (
    get_db_session,
    get_cached_db_schema,
    extract_table_names,
    extract_column_names,
    validate_sql_syntax,
)
from core.logging import get_logger

logger = get_logger(__name__)


# Constants and Configuration for Agentic Flow
# Intent Classification Patterns
GREETING_PATTERNS = [
    "ì•ˆë…•",
    "ë°˜ê°€ì›Œ",
    "hello",
    "hi",
    "ì¢‹ì€ ì•„ì¹¨",
    "ì¢‹ì€ ì €ë…",
    "í™˜ì˜",
    "ì¸ì‚¬",
    "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "ì•ˆë…•í•˜ì„¸ìš”",
    "ì•ˆë…•í•˜ì„¸ìš”",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤",
    "ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "ì¢‹ì€ í•˜ë£¨",
    "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”",
    "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”",
]

HELP_REQUEST_PATTERNS = [
    "ë„ì›€",
    "ì‚¬ìš©ë²•",
    "ì–´ë–»ê²Œ",
    "help",
    "ëª…ë ¹ì–´",
    "ë„ì™€ì¤˜",
    "ì„¤ëª…",
    "ê°€ì´ë“œ",
    "ì‚¬ìš©ë²•",
    "ë„ì›€ë§",
    "ì‚¬ìš©ë²• ì•Œë ¤ì¤˜",
    "ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”",
    "ê¸°ëŠ¥",
    "ê¸°ëŠ¥ì´ ë­ì•¼",
    "ë­ê°€ ìžˆì–´",
    "ë­˜ í•  ìˆ˜ ìžˆì–´",
    "í•  ìˆ˜ ìžˆëŠ” ê²ƒ",
    "ê¸°ëŠ¥ ì„¤ëª…",
    "ë„ˆê°€ í•  ìˆ˜ ìžˆëŠ” ì¼",
    "ë­ì•¼",
    "ë­ì§€",
    "ë­”ê°€",
    "ë­”ë°",
]

GENERAL_CHAT_PATTERNS = [
    "ì–´ë•Œ",
    "ì–´ë– ",
    "ì¢‹ì•„",
    "ë‚˜ì˜",
    "ìž¬ë¯¸",
    "ìž¬ë¯¸ìžˆ",
    "ì§€ë£¨",
    "í”¼ê³¤",
    "ë‚ ì”¨",
    "ì˜¤ëŠ˜",
    "ì–´ì œ",
    "ë‚´ì¼",
    "ì£¼ë§",
    "íœ´ì¼",
    "ì¼",
    "ì¼ì •",
    "ê³ ë§ˆì›Œ",
    "ê°ì‚¬",
    "ë¯¸ì•ˆ",
    "ì£„ì†¡",
    "ê´œì°®",
    "ê´œì°®ì•„",
    "ê´œì°®ìŠµë‹ˆë‹¤",
    "ë­ì•¼",
    "ë­ì§€",
    "ë­”ê°€",
    "ë­”ë°",
    "ë­”ê°€ìš”",
    "ë­”ê°€ìš”?",
]

FANDING_DATA_KEYWORDS = [
    "ë©¤ë²„ì‹­",
    "ì„±ê³¼",
    "íšŒì›",
    "ë§¤ì¶œ",
    "ë°©ë¬¸ìž",
    "ë¦¬í…ì…˜",
    "í¬ìŠ¤íŠ¸",
    "ì¡°íšŒìˆ˜",
    "ì¸ê¸°",
    "ë¶„ì„",
    "í†µê³„",
    "ë¦¬í¬íŠ¸",
    "ì›”ê°„",
    "ì¼ê°„",
    "ì£¼ê°„",
    "ë…„ê°„",
    "í¬ë¦¬ì—ì´í„°",
    "íŽ€ë”©",
    "í”„ë¡œì íŠ¸",
    "8ì›”",
    "9ì›”",
    "10ì›”",
    "11ì›”",
    "12ì›”",
    "1ì›”",
    "2ì›”",
    "3ì›”",
    "4ì›”",
    "5ì›”",
    "6ì›”",
    "7ì›”",
    "ì˜¬í•´",
    "ìž‘ë…„",
    "ì§€ë‚œë‹¬",
    "ì´ë²ˆë‹¬",
    "ì‹ ê·œ",
    "ì´íƒˆ",
    "í™œì„±",
    "êµ¬ë…",
    "ê²°ì œ",
    "ìˆ˜ìµ",
    "ë§¤ì¶œì•¡",
    "í˜„í™©",
    "ìƒí™©",
    "ê²°ê³¼",
    "ì„±ê³¼ë¶„ì„",
    "ì„±ê³¼",
    "ë¶„ì„í•´ì¤˜",
    "ë³´ê³ ì„œ",
    "ìš”ì•½",
    "ì •ë¦¬",
    "í˜„ìž¬",
    "ìµœê·¼",
    "ì§€ê¸ˆ",
    "ì˜¤ëŠ˜",
    "ì–´ì œ",
    "ë‚´ì¼",
]

DATA_QUERY_PATTERNS = [
    "ì¡°íšŒ",
    "ê²€ìƒ‰",
    "ë³´ì—¬ì¤˜",
    "ì°¾ì•„",
    "í…Œì´ë¸”",
    "ì¿¼ë¦¬",
    "ê°œìˆ˜",
    "ìˆ˜",
    "í•©ê³„",
    "í‰ê· ",
    "ìµœëŒ€",
    "ìµœì†Œ",
    "í†µê³„",
    "ì•Œë ¤ì¤˜",
    "ë³´ì—¬ì¤˜",
    "ì°¾ì•„ì¤˜",
    "ê°€ì ¸ì™€",
    "ì–¼ë§ˆë‚˜",
    "ëª‡ ê°œ",
    "ëª‡ ëª…",
    "ì–¼ë§ˆ",
    "ì–´ëŠ ì •ë„",
]

QUESTION_PATTERNS = [
    "ë­",
    "ë¬´ì—‡",
    "ì–´ë–¤",
    "ì–´ë””",
    "ì–¸ì œ",
    "ì™œ",
    "ì–´ë–»ê²Œ",
    "ëˆ„êµ¬",
    "ë­”ê°€",
    "ë­”ì§€",
    "ë­”ë°",
    "ë­ì•¼",
    "ë­ì§€",
    "ë­”ê°€ìš”",
    "ë­”ê°€ìš”?",
]

GRATITUDE_PATTERNS = [
    "ê³ ë§ˆì›Œ",
    "ê°ì‚¬",
    "ê°ì‚¬í•©ë‹ˆë‹¤",
    "ê³ ë§ˆì›Œìš”",
    "ê³ ë§™ìŠµë‹ˆë‹¤",
    "ìˆ˜ê³ ",
    "ìˆ˜ê³ í•˜ì…¨",
    "ìˆ˜ê³ í•˜ì…¨ì–´ìš”",
    "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤",
]

# Response Templates
GREETING_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ðŸ‘‹ Fanding Data Report ë´‡ìž…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    "ì•ˆë…•í•˜ì„¸ìš”! ðŸ˜Š ë°ì´í„° ë¶„ì„ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ë°˜ê°‘ìŠµë‹ˆë‹¤! ðŸ¤– ë©¤ë²„ì‹­ ì„±ê³¼ë‚˜ íšŒì› ë°ì´í„°ë¥¼ ì¡°íšŒí•´ë“œë¦´ ìˆ˜ ìžˆì–´ìš”.",
    "ì•ˆë…•í•˜ì„¸ìš”! ðŸ“Š Fanding ë°ì´í„°ë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
]

GENERAL_CHAT_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! ðŸ˜Š ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìžˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!",
    "ë„¤, ë“£ê³  ìžˆì–´ìš”! ðŸ“Š Fanding ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.",
    "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ðŸ¤– ë©¤ë²„ì‹­ ì„±ê³¼ë‚˜ íšŒì› ë°ì´í„°ê°€ ê¶ê¸ˆí•˜ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.",
    "ê°ì‚¬í•©ë‹ˆë‹¤! ðŸ˜Š ë°ì´í„° ë¶„ì„ì„ ë„ì™€ë“œë¦´ ì¤€ë¹„ê°€ ë˜ì–´ìžˆì–´ìš”.",
]

# SQL Security Keywords
DANGEROUS_SQL_KEYWORDS = [
    "INSERT",
    "UPDATE",
    "DELETE",
    "DROP",
    "CREATE",
    "ALTER",
    "TRUNCATE",
    "EXEC",
    "EXECUTE",
    "UNION",
    "SCRIPT",
    "GRANT",
    "REVOKE",
    "COMMIT",
    "ROLLBACK",
]

# Table Name Mapping
TABLE_NAME_MAPPING = {
    # íšŒì› ê´€ë ¨
    "users": "t_member",
    "user": "t_member",
    "members": "t_member",
    "member": "t_member",
    "íšŒì›": "t_member",
    "ì‚¬ìš©ìž": "t_member",
    # íšŒì› ì •ë³´ ê´€ë ¨
    "user_info": "t_member_info",
    "member_info": "t_member_info",
    "íšŒì›ì •ë³´": "t_member_info",
    # íšŒì› í”„ë¡œí•„ ê´€ë ¨
    "user_profile": "t_member_profile",
    "member_profile": "t_member_profile",
    "profiles": "t_member_profile",
    "í”„ë¡œí•„": "t_member_profile",
    # í¬ë¦¬ì—ì´í„° ê´€ë ¨
    "creators": "t_creator",
    "creator": "t_creator",
    "í¬ë¦¬ì—ì´í„°": "t_creator",
    "ì°½ìž‘ìž": "t_creator",
    # íŽ€ë”© ê´€ë ¨
    "fundings": "t_funding",
    "funding": "t_funding",
    "íŽ€ë”©": "t_funding",
    "projects": "t_funding",
    "í”„ë¡œì íŠ¸": "t_funding",
    # íŽ€ë”© ì°¸ì—¬ìž
    "funding_members": "t_funding_member",
    "backers": "t_funding_member",
    "supporters": "t_funding_member",
    "í›„ì›ìž": "t_funding_member",
    # íŒ”ë¡œìš° ê´€ê³„
    "follows": "t_follow",
    "follow": "t_follow",
    "íŒ”ë¡œìš°": "t_follow",
    # ì£¼ë¬¸ ê´€ë ¨
    "orders": "t_order",
    "order": "t_order",
    "ì£¼ë¬¸": "t_order",
}

# Korean to English Mappings
KOREAN_MAPPINGS = {
    "ë³´ì—¬ì¤˜": "show",
    "ì°¾ì•„ì¤˜": "find",
    "ê°€ì ¸ì™€": "get",
    "ê°œìˆ˜": "count",
    "í•©ê³„": "sum",
    "í‰ê· ": "average",
    "ìµœëŒ€": "max",
    "ìµœì†Œ": "min",
}

# Entity Extraction Keywords
MEMBER_KEYWORDS = [
    "íšŒì›",
    "ë©¤ë²„",
    "ì‚¬ìš©ìž",
    "ìœ ì €",
    "member",
    "user",
    "íšŒì›ìˆ˜",
    "ë©¤ë²„ìˆ˜",
]
CREATOR_KEYWORDS = ["í¬ë¦¬ì—ì´í„°", "ì°½ìž‘ìž", "ìž‘ê°€", "ì•„í‹°ìŠ¤íŠ¸", "ì œìž‘ìž", "creator"]
DATE_KEYWORDS = ["ì‹ ê·œ", "í˜„í™©", "ì›”ê°„", "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„"]
LOGIN_KEYWORDS = ["ë¡œê·¸ì¸", "login", "ì ‘ì†"]
RANKING_KEYWORDS = ["top", "top5", "top10", "ìƒìœ„", "ìµœê³ ", "ë§Žì€", "ì ì€", "ìˆœìœ„"]
STATISTICS_KEYWORDS = ["ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„", "ë¶„ì„"]

# Confidence Thresholds
LLM_CONFIDENCE_THRESHOLD_HIGH = 0.8
LLM_CONFIDENCE_THRESHOLD_MEDIUM = 0.6
LLM_CONFIDENCE_THRESHOLD_LOW = 0.3
RAG_CONFIDENCE_THRESHOLD = 0.6
SQL_GENERATION_CONFIDENCE_THRESHOLD = 0.7


# Response Generator Functions


def generate_greeting_response(user_query: str) -> str:
    """Generate a random greeting response."""
    return random.choice(GREETING_RESPONSES)


def generate_help_response(user_query: str) -> str:
    """Generate a help response."""
    return """ðŸ¤– **Fanding Data Report ë´‡ ì‚¬ìš©ë²•**

**ðŸ“Š ë°ì´í„° ì¡°íšŒ ê¸°ëŠ¥:**
â€¢ "í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜" - í™œì„± íšŒì› ìˆ˜ í™•ì¸
â€¢ "8ì›” ë©¤ë²„ì‹­ ì„±ê³¼ ë¶„ì„í•´ì¤˜" - íŠ¹ì • ì›” ì„±ê³¼ ë¶„ì„
â€¢ "ì „ì²´ íšŒì› ìˆ˜ ë³´ì—¬ì¤˜" - ì „ì²´ íšŒì› ìˆ˜ í™•ì¸
â€¢ "ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜" - ì‹ ê·œ íšŒì› í˜„í™©

**ðŸ’¡ ì‚¬ìš© íŒ:**
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš” (ì˜ˆ: "8ì›” ì„±ê³¼", "í™œì„± íšŒì›")
â€¢ ë‚ ì§œë‚˜ ê¸°ê°„ì„ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì˜ˆ: "ì´ë²ˆ ë‹¬", "ì§€ë‚œ ì£¼")
â€¢ ë©¤ë²„ì‹­, íšŒì›, ì„±ê³¼ ë“± í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”

**â“ ê¶ê¸ˆí•œ ì ì´ ìžˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!**"""


def generate_general_chat_response(user_query: str) -> str:
    """Generate a random general chat response."""
    return random.choice(GENERAL_CHAT_RESPONSES)


def generate_error_response(error: Exception) -> str:
    """Generate user-friendly error response."""
    error_type = type(error).__name__

    # íŠ¹ì • ì—ëŸ¬ íƒ€ìž…ë³„ ë§žì¶¤í˜• ì‘ë‹µ
    if "UnicodeEncodeError" in error_type:
        return """ðŸ˜… **ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì£„ì†¡í•©ë‹ˆë‹¤. íŠ¹ìˆ˜ ë¬¸ìžë‚˜ ì´ëª¨ì§€ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.
ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ðŸ¤–"""

    elif "ConnectionError" in error_type or "TimeoutError" in error_type:
        return """ðŸŒ **ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ìžˆì–´ìš”.
ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ðŸ”„"""

    elif "ValueError" in error_type or "TypeError" in error_type:
        return """âš ï¸ **ìž…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ë° ë¬¸ì œê°€ ìžˆì—ˆì–´ìš”.
ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”! ðŸ’¡"""

    else:
        return """ðŸ˜” **ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**

ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.
ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ê¸°ìˆ íŒ€ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”! ðŸ› ï¸

**ðŸ’¡ ë„ì›€ë§:** "ì‚¬ìš©ë²• ì•Œë ¤ì¤˜"ë¼ê³  ë§ì”€í•´ì£¼ì‹œë©´ ì‚¬ìš©ë²•ì„ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”."""


def generate_clarification_question(user_query: str) -> str:
    """Generate a clarification question for ambiguous queries."""
    q = user_query.lower()
    needs_topk = "top" in q or "ìƒìœ„" in q or "top5" in q
    needs_period = any(
        k in q
        for k in [
            "ì´ë²ˆ",
            "ì§€ë‚œ",
            "ì´ë²ˆë‹¬",
            "ì§€ë‚œë‹¬",
            "ì›”",
            "ë¶„ê¸°",
            "ì£¼",
            "week",
            "month",
            "quarter",
        ]
    )
    needs_metric = any(
        k in q for k in ["íšŒì›ìˆ˜", "ì‹ ê·œ", "í™œì„±", "ë¡œê·¸ì¸", "ì¡°íšŒìˆ˜", "ë§¤ì¶œ", "íŒë§¤"]
    )

    parts = []
    if needs_period:
        parts.append("ê¸°ê°„(ì˜ˆ: 2025-08, ì§€ë‚œë‹¬)ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
    if needs_topk:
        parts.append("ìƒìœ„ K ê°œ(ì˜ˆ: Top5)ëŠ” ëª‡ ê°œë¥¼ ì›í•˜ì‹œë‚˜ìš”?")
    if needs_metric:
        parts.append("ì–´ë–¤ ì§€í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëž­í‚¹ì„ ì›í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ì‹ ê·œ íšŒì›ìˆ˜)")
    if not parts:
        parts.append("ê¸°ê°„/ì§€í‘œ/Top-K ì¤‘ í•„ìš”í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.")

    return "ì§ˆì˜ë¥¼ ì •í™•ížˆ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•´ ì£¼ì„¸ìš”: " + " ".join(parts)


class BaseNode(ABC):
    """Base class for all pipeline nodes."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(self.__class__.__name__)
        self._llm_service = None  # Lazy initialization

    @abstractmethod
    def process(self, state: GraphState) -> GraphState:
        """Process the current state and return updated state."""
        pass

    def _log_processing(self, state: GraphState, component: str):
        """Log processing information."""
        self.logger.info(
            f"Processing {component}",
            user_id=state.get("user_id"),
            channel_id=state.get("channel_id"),
            query=state.get("user_query", "")[:100],
        )

    def _get_llm_service(self):
        """Get LLM service instance (lazy initialization)."""
        if self._llm_service is None:
            from agentic_flow.llm_service import get_llm_service

            self._llm_service = get_llm_service()
        return self._llm_service

    def _get_intent_llm(self):
        """Get intent classification LLM (lightweight, fast response)."""
        return self._get_llm_service().get_intent_llm()

    def _get_sql_llm(self):
        """Get SQL generation LLM (high-performance model)."""
        return self._get_llm_service().get_sql_llm()


class NLProcessor(BaseNode):
    """Natural Language Processing node for query analysis."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # Use centralized LLM service for intent classification
        self.llm = self._get_intent_llm()
        self.fanding_templates = FandingSQLTemplates()
        # Removed: EnhancedRAGMapper and DynamicSchemaExpander (deleted modules)
        # self.rag_mapper = EnhancedRAGMapper(config)
        # self.schema_expander = DynamicSchemaExpander(config)

    def process(self, state: GraphState) -> GraphState:
        """Process natural language query and extract intent and entities."""
        self._log_processing(state, "NLProcessor")

        try:
            # ìž…ë ¥ ë°ì´í„° ê²€ì¦
            user_query = state.get("user_query")
            if not user_query:
                self.logger.error("user_query is None or empty")
                # ìž¬ìž…ë ¥ ìš”ì²­ ì„¤ì •
                state["conversation_response"] = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì§€ ëª»í–ˆì–´ìš”. ðŸ˜Š\n\n"
                    "ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
                    "ì˜ˆì‹œ: '9ì›” ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜', 'í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜'"
                )
                state["skip_sql_generation"] = True
                state["needs_clarification"] = True  # ìž¬ìž…ë ¥ í•„ìš” í”Œëž˜ê·¸ ì„¤ì •
                state["success"] = False
                return state

            # Normalize query
            normalized_query = self._normalize_query(user_query)

            # ì •ê·œí™”ëœ ì¿¼ë¦¬ ê²€ì¦
            if not normalized_query or len(normalized_query.strip()) == 0:
                self.logger.error("normalized_query is empty after processing")
                # ìž¬ìž…ë ¥ ìš”ì²­ ì„¤ì •
                state["conversation_response"] = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ðŸ¤”\n\n"
                    "ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
                    "ì˜ˆì‹œ: '9ì›” ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜', 'í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜'"
                )
                state["skip_sql_generation"] = True
                state["needs_clarification"] = True  # ìž¬ìž…ë ¥ í•„ìš” í”Œëž˜ê·¸ ì„¤ì •
                state["success"] = False
                return state

            # Extract intent and entities (LLM ê²°ê³¼ í¬í•¨)
            llm_intent_result = state.get("llm_intent_result")
            intent, entities = self._extract_intent_and_entities(
                normalized_query, llm_intent_result
            )  # NOTE: confidenceê°€ ë†’ë‹¤ë©´ LLMì˜ ê²ƒì„, ë‚®ë‹¤ë©´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ intentë¥¼ ë¶„ë¥˜í•˜ê³  í•„ìš”í•˜ë‹¤ë©´ entityë¥¼ ì¶”ì¶œí•¨

            # Update state
            state["normalized_query"] = normalized_query
            state["intent"] = intent
            state["entities"] = entities

            # ì¸ì‚¬ë§ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 1)
            if intent == QueryIntent.GREETING:
                response = self._handle_greeting(user_query)
                set_conversation_response(state, response, skip_sql=True)
                state["success"] = True
                self.logger.info(f"Greeting handled: {user_query}")
                return state

            # ë„ì›€ë§ ìš”ì²­ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 2)
            if intent == QueryIntent.HELP_REQUEST:
                response = self._handle_help_request(user_query)
                set_conversation_response(state, response, skip_sql=True)
                state["success"] = True
                self.logger.info(f"Help request handled: {user_query}")
                return state

            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ìš”ì²­ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ 3 - SHOW/DESCRIBE ëŒ€ì•ˆ)
            schema_info_response = self.fanding_templates.get_schema_info(user_query)
            if schema_info_response:
                state["conversation_response"] = schema_info_response
                state["intent"] = QueryIntent.HELP_REQUEST
                state["skip_sql_generation"] = True
                state["success"] = True
                self.logger.info(f"Schema information request handled: {user_query}")
                return state

            # ì¸í…íŠ¸ë³„ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
            if intent == QueryIntent.GENERAL_CHAT:
                # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
                response = self._handle_general_chat(user_query)
                set_conversation_response(state, response, skip_sql=True)
                clear_sql_generation(state)
                state["success"] = True
                self.logger.info(f"General chat handled: {intent}")

            elif intent == QueryIntent.DATA_QUERY:
                # ë°ì´í„° ì¡°íšŒ ì˜ë„ - Fanding í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„
                self.logger.info(f"Data query intent detected: {user_query}")
                self._handle_data_query(state, user_query)
                state["success"] = True

            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ì¸í…íŠ¸ - ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                self.logger.warning(
                    f"Unknown intent: {intent}, treating as general chat"
                )
                state["skip_sql_generation"] = True
                state["conversation_response"] = self._handle_general_chat(user_query)
                state["sql_query"] = None
                state["validated_sql"] = None
                state["success"] = True
                self.logger.info(f"Unknown intent handled as general chat: {intent}")

            # Log confidence
            confidence = self._calculate_confidence(normalized_query, intent, entities)
            state["confidence_scores"]["nl_processing"] = confidence

            self.logger.info(f"Processed query: {normalized_query}")
            self.logger.info(f"Intent: {intent}, Entities: {len(entities)}")

        except Exception as e:
            self.logger.error(f"Error in NLProcessor: {str(e)}", exc_info=True)
            # ì‚¬ìš©ìž ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            error_response = self._generate_error_response(e)
            state["conversation_response"] = error_response
            state["skip_sql_generation"] = True
            state["sql_query"] = None
            state["validated_sql"] = None
            state["success"] = False
            state["error_message"] = f"Natural language processing failed: {str(e)}"

        return state

    def _handle_greeting(self, user_query: str) -> str:
        """ì¸ì‚¬ë§ ì²˜ë¦¬ (ëžœë¤ ì‘ë‹µ)"""
        return generate_greeting_response(user_query)

    def _handle_help_request(self, user_query: str) -> str:
        """ë„ì›€ë§ ìš”ì²­ ì²˜ë¦¬"""
        return generate_help_response(user_query)

    def _handle_general_chat(self, user_query: str) -> str:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ëžœë¤ ì‘ë‹µ)"""
        return generate_general_chat_response(user_query)

    def _generate_error_response(self, error: Exception) -> str:
        """ì‚¬ìš©ìž ì¹œí™”ì ì¸ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return generate_error_response(error)

    def _handle_data_query(self, state: GraphState, user_query: str) -> None:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ ì²˜ë¦¬ (RAG + ë™ì  ìŠ¤í‚¤ë§ˆ í™•ìž¥ í†µí•©)"""
        # ì• ë§¤í•œ ì¿¼ë¦¬ì¸ì§€ ë¨¼ì € í™•ì¸
        if self.fanding_templates.is_ambiguous_query(user_query):
            self.logger.info(
                f"ðŸ” Ambiguous query detected - requesting clarification: {user_query}"
            )
            clarification_question = (
                self.fanding_templates.generate_clarification_question(user_query)
            )
            state["conversation_response"] = clarification_question
            state["skip_sql_generation"] = True
            state["needs_clarification"] = True  # ìž¬ìž…ë ¥ í•„ìš” í”Œëž˜ê·¸ ì„¤ì •
            state["success"] = True  # ëª…í™•í™” ì§ˆë¬¸ ìƒì„± ì„±ê³µ (ì •ìƒ ì²˜ë¦¬)
            self.logger.info(
                "âœ… Generated clarification question for ambiguous query (this is normal behavior)"
            )
            return

        # 1. RAG ë§¤í•‘ ì‹œë„ (ìš°ì„ ìˆœìœ„ ë†’ìŒ) - DISABLED: EnhancedRAGMapper deleted
        # try:
        #     rag_result = self.rag_mapper.map_query_to_schema(user_query, context={"prefer_detailed": True})
        #     if rag_result and rag_result.confidence > LLM_CONFIDENCE_THRESHOLD_HIGH:
        #         self.logger.info(f"RAG mapping successful: {rag_result.source.value} (confidence: {rag_result.confidence:.2f})")
        #         set_rag_mapping_result(state, rag_result)
        #         state["skip_sql_generation"] = False
        #         self.logger.info(f"RAG SQL applied: {rag_result.sql_template[:100]}...")
        #         return
        # except Exception as e:
        #     self.logger.warning(f"RAG mapping failed: {str(e)}")

        # Skip RAG mapping and go directly to Fanding templates

        # 2. Fanding í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„ (í´ë°±)
        fanding_template = self.fanding_templates.match_query_to_template(user_query)
        if fanding_template:
            self.logger.info(f"Fanding template matched: {fanding_template.name}")
            set_fanding_template(state, fanding_template)
            state["skip_sql_generation"] = False
            state["success"] = True  # í…œí”Œë¦¿ ë§¤ì¹­ ì„±ê³µ
            self.logger.info(f"SQL template applied: {fanding_template.sql_template}")
        else:
            # 3. ë™ì  ì›”ë³„ í…œí”Œë¦¿ ìƒì„± ì‹œë„ (ë©¤ë²„ì‹­ ì„±ê³¼ ê´€ë ¨)
            try:
                dynamic_template = (
                    self.fanding_templates.create_dynamic_monthly_template(user_query)
                )
                if dynamic_template:
                    self.logger.info(
                        f"Dynamic monthly template created: {dynamic_template.name}"
                    )
                    set_fanding_template(state, dynamic_template)
                    state["skip_sql_generation"] = False
                    state["success"] = True  # ë™ì  í…œí”Œë¦¿ ìƒì„± ì„±ê³µ
                    self.logger.info(
                        f"Dynamic SQL applied: {dynamic_template.sql_template[:100]}..."
                    )
                    return
            except Exception as e:
                self.logger.warning(
                    f"Dynamic monthly template creation failed: {str(e)}"
                )

            # 4. ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
            self.logger.info(
                "No template/pattern matched, proceeding with general SQL generation"
            )
            state["skip_sql_generation"] = False
            state["success"] = True  # ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰ (ì •ìƒ ì²˜ë¦¬)

    def _normalize_query(self, query: str) -> str:
        """Normalize the user query."""
        # Remove extra whitespace
        normalized = re.sub(r"\s+", " ", query.strip())

        # Convert to lowercase for consistency
        normalized = normalized.lower()

        # Handle common Korean database terms (ìƒìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        for korean, english in KOREAN_MAPPINGS.items():
            normalized = normalized.replace(korean, english)

        return normalized

    def _extract_intent_and_entities(
        self, query: str, llm_intent_result: Optional[Dict] = None
    ) -> Tuple[QueryIntent, List[Entity]]:
        """Extract intent and entities from the query."""

        # 1. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (MEDIUM ìž„ê³„ê°’ 0.6 ì‚¬ìš©)
        # MEDIUM ìž„ê³„ê°’: LLM ë¶„ë¥˜ê°€ ìƒë‹¹ížˆ í™•ì‹¤í•  ë•Œë§Œ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¶„ë¥˜ ë°©ì§€
        if (
            llm_intent_result
            and llm_intent_result.get("confidence", 0)
            >= LLM_CONFIDENCE_THRESHOLD_MEDIUM
        ):
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(
                    f"Using LLM intent classification: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})"
                )
                # ì—”í‹°í‹°ë„ ì¶”ì¶œ
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                self.logger.warning(
                    f"Invalid LLM intent: {llm_intent_result.get('intent')}"
                )

        # 2. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ ì°¸ê³  (LOW ìž„ê³„ê°’ 0.3 ì‚¬ìš©)
        # LOW ìž„ê³„ê°’: LLMì´ ë¶ˆí™•ì‹¤í•´ë„ ê·œì¹™ ê¸°ë°˜ë³´ë‹¤ëŠ” ë‚˜ì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ìµœì†Œí•œì˜ ì‹ ë¢°ë„ë¡œ ì°¸ê³ 
        if (
            llm_intent_result
            and llm_intent_result.get("confidence", 0) >= LLM_CONFIDENCE_THRESHOLD_LOW
        ):
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(
                    f"Using LLM intent as fallback: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})"
                )
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                pass

        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬ (LLM ì‹¤íŒ¨ ì‹œ fallback)
        if self._has_data_query_indicators(query):
            # ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìžˆìœ¼ë©´ DATA_QUERYë¡œ ë¶„ë¥˜ (LLM ì‹¤íŒ¨í•´ë„)
            self.logger.info(
                f"Data query indicators detected, classifying as DATA_QUERY: {query}"
            )
            entities = self._extract_entities_from_query(query)
            return QueryIntent.DATA_QUERY, entities

        # 4. ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹œë„ (ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        rule_based_intent = self._classify_intent_by_rules(query)

        if rule_based_intent != QueryIntent.UNKNOWN:
            # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ëœ ê²½ìš° (ì¸ì‚¬, ì¼ë°˜ ëŒ€í™” ë“±)
            return rule_based_intent, []

        # 5. ëª¨ë“  ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        return QueryIntent.GENERAL_CHAT, []

    def _extract_entities_from_query(self, query: str) -> List[Entity]:
        """ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
        query_lower = query.lower()

        # íšŒì› ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in MEMBER_KEYWORDS):
            entities.append(Entity(name="member", type="table", confidence=0.9))

        # í¬ë¦¬ì—ì´í„° ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in CREATOR_KEYWORDS):
            entities.append(Entity(name="creator", type="table", confidence=0.9))

        # ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in DATE_KEYWORDS):
            entities.append(Entity(name="date", type="column", confidence=0.8))

        # ë¡œê·¸ì¸ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in LOGIN_KEYWORDS):
            entities.append(Entity(name="login", type="table", confidence=0.8))

        # Top/ìˆœìœ„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in RANKING_KEYWORDS):
            entities.append(Entity(name="ranking", type="aggregation", confidence=0.8))

        # í†µê³„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in STATISTICS_KEYWORDS):
            entities.append(
                Entity(name="statistics", type="aggregation", confidence=0.8)
            )

        return entities

    def _classify_intent_by_rules(self, query: str) -> QueryIntent:
        """ê·œì¹™ ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜ (ê°œì„ ëœ ë²„ì „)"""
        query_lower = query.lower().strip()

        # 1. ë¨¼ì € ëª…í™•í•œ ë¹„ë°ì´í„° ì˜ë„ íŒ¨í„´ í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)

        # 1-1. ì¸ì‚¬ë§ íŒ¨í„´ (ìµœìš°ì„ )
        if any(pattern in query_lower for pattern in GREETING_PATTERNS):
            return QueryIntent.GREETING

        # 1-2. ë„ì›€ë§ ìš”ì²­ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in HELP_REQUEST_PATTERNS):
            return QueryIntent.HELP_REQUEST

        # 1-3. ì¼ë°˜ ëŒ€í™” íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in GENERAL_CHAT_PATTERNS):
            return QueryIntent.GENERAL_CHAT

        # 2. Fanding ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ í™•ì¸ (ëª…í™•í•œ ë°ì´í„° ì¡°íšŒ ì˜ë„ë§Œ)
        if any(keyword in query_lower for keyword in FANDING_DATA_KEYWORDS):
            return QueryIntent.DATA_QUERY

        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ í‚¤ì›Œë“œ (ëª…í™•í•œ ì¡°íšŒ ì˜ë„)
        if any(pattern in query_lower for pattern in DATA_QUERY_PATTERNS):
            return QueryIntent.DATA_QUERY

        # 4. ì§ˆë¬¸ íŒ¨í„´ (ì˜ë¬¸ì‚¬ ê¸°ë°˜) - ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        if any(pattern in query_lower for pattern in QUESTION_PATTERNS):
            # ì§ˆë¬¸ì´ì§€ë§Œ ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
            return QueryIntent.GENERAL_CHAT

        # 5. ê°ì‚¬/ì¸ì‚¬ í‘œí˜„
        if any(pattern in query_lower for pattern in GRATITUDE_PATTERNS):
            return QueryIntent.GENERAL_CHAT

        # 6. ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜ (UNKNOWN ëŒ€ì‹ )
        return QueryIntent.GENERAL_CHAT

    def _has_data_query_indicators(self, query: str) -> bool:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìžˆëŠ”ì§€ í™•ì¸"""
        query_lower = query.lower()

        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ (ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •)
        data_keywords = [
            "ì¡°íšŒ",
            "ê²€ìƒ‰",
            "ë°ì´í„°",
            "í…Œì´ë¸”",
            "ì¿¼ë¦¬",
            "ì‚¬ìš©ìž",
            "íšŒì›",
            "í¬ë¦¬ì—ì´í„°",
            "íŽ€ë”©",
            "í”„ë¡œì íŠ¸",
            "ì£¼ë¬¸",
            "ê°œìˆ˜",
            "ìˆ˜",
            "í•©ê³„",
            "í‰ê· ",
            "ìµœëŒ€",
            "ìµœì†Œ",
            "í†µê³„",
            "ë©¤ë²„ì‹­",
            "ì„±ê³¼",
            "ë§¤ì¶œ",
            "ë°©ë¬¸ìž",
            "ë¦¬í…ì…˜",
            "í¬ìŠ¤íŠ¸",
            "ì¡°íšŒìˆ˜",
            "ì¸ê¸°",
            "ë¶„ì„",
            "ë¦¬í¬íŠ¸",
            "ì›”ê°„",
            "ì¼ê°„",
            "ì£¼ê°„",
            "ë…„ê°„",
            # ì¶”ê°€ í‚¤ì›Œë“œ
            "ë½‘ì•„ì¤˜",
            "ë½‘ì•„",
            "ì¶”ì¶œ",
            "ì„ íƒ",
            "ê³ ë¥´",
            "ì •ë ¬",
            "ìˆœìœ„",
            "top",
            "top5",
            "top10",
            "ìƒìœ„",
            "ìµœê³ ",
            "ë§Žì€",
            "ì ì€",
            "íšŒì›ìˆ˜",
            "ë©¤ë²„ìˆ˜",
            "ì‚¬ìš©ìžìˆ˜",
            "ê°€ìž…ìž",
            "í™œì„±",
            "ì‹ ê·œ",
            "í¬ë¦¬ì—ì´í„°",
            "ì°½ìž‘ìž",
            "ìž‘ê°€",
            "ì•„í‹°ìŠ¤íŠ¸",
            "ì œìž‘ìž",
        ]

        # ë°ì´í„° ì¡°íšŒì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´
        data_question_patterns = [
            "ì–¼ë§ˆë‚˜",
            "ëª‡ ê°œ",
            "ëª‡ ëª…",
            "ëª‡ ê±´",
            "ëª‡ ê°œì˜",
            "ëª‡ ëª…ì˜",
            "ê°€ì ¸ì™€",
            "ì°¾ì•„ì¤˜",
            "ë³´ì—¬ì¤˜",
            "ì•Œë ¤ì¤˜",  # ë°ì´í„° ê´€ë ¨ ë§¥ë½ì—ì„œë§Œ
        ]

        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ì œì™¸
        general_question_patterns = [
            "ë­ì•¼",
            "ë­”ê°€",
            "ë­”ì§€",
            "ì–´ë–»ê²Œ",
            "ì™œ",
            "ì–¸ì œ",
            "ì–´ë””ì„œ",
            "í•  ìˆ˜ ìžˆëŠ”",
            "í•  ìˆ˜ ìžˆëŠ”ì§€",
            "í•  ìˆ˜ ìžˆëŠ”ê²Œ",
            "í•  ìˆ˜ ìžˆëŠ”ê²ƒ",
        ]

        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìžˆìœ¼ë©´ ë°ì´í„° ì¡°íšŒê°€ ì•„ë‹˜
        if any(pattern in query_lower for pattern in general_question_patterns):
            return False

        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œë‚˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ ìžˆëŠ” ê²½ìš°ë§Œ
        return any(keyword in query_lower for keyword in data_keywords) or any(
            pattern in query_lower for pattern in data_question_patterns
        )

    def _extract_with_llm(self, query: str) -> Tuple[QueryIntent, List[Entity]]:
        """LLMì„ ì‚¬ìš©í•œ ì¸í…íŠ¸ ë° ì—”í‹°í‹° ì¶”ì¶œ"""
        system_prompt = """
        You are a database query analyzer. Analyze the given natural language query and extract:
        1. Query intent (SELECT, COUNT, AGGREGATE, FILTER, JOIN, UNKNOWN)
        2. Entities (tables, columns, values, conditions)
        
        Return your analysis in JSON format:
        {
            "intent": "SELECT",
            "entities": [
                {"name": "users", "type": "table", "confidence": 0.9},
                {"name": "email", "type": "column", "confidence": 0.8}
            ]
        }
        """

        try:
            if self.llm is None:
                # LLMì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° ì¡°íšŒë¡œ ë¶„ë¥˜
                return QueryIntent.SELECT, []

            # ìµœì‹  LangChain ë°©ì‹: SystemMessage ëŒ€ì‹  HumanMessageì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
            messages = [
                HumanMessage(content=f"{system_prompt}\n\nAnalyze this query: {query}")
            ]

            response = self.llm.invoke(messages)
            response_content = response.content
            # Handle different response types
            if isinstance(response_content, str):
                response_text = response_content
            elif isinstance(response_content, list):
                # Extract text from list of content blocks
                response_text = " ".join(str(item) for item in response_content)
            else:
                response_text = str(response_content)
            result = self._parse_llm_response(response_text)

            intent = QueryIntent(result.get("intent", "UNKNOWN"))
            entities = [
                Entity(
                    name=entity["name"],
                    type=entity["type"],
                    confidence=entity["confidence"],
                    context=entity.get("context"),
                )
                for entity in result.get("entities", [])
            ]

            return intent, entities

        except Exception as e:
            self.logger.error(f"Error extracting intent and entities: {str(e)}")
            return QueryIntent.UNKNOWN, []

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response and extract JSON."""
        try:
            import json

            # Extract JSON from response
            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {"intent": "UNKNOWN", "entities": []}
        except Exception as e:
            self.logger.error(f"Error parsing LLM response: {str(e)}")
            return {"intent": "UNKNOWN", "entities": []}

    def _calculate_confidence(
        self, query: str, intent: QueryIntent, entities: List[Entity]
    ) -> float:
        """Calculate confidence score for the processing."""
        base_confidence = 0.8

        # Adjust based on intent clarity
        if intent != QueryIntent.UNKNOWN:
            base_confidence += 0.1

        # Adjust based on entity extraction
        if entities:
            avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
            base_confidence = (base_confidence + avg_entity_confidence) / 2

        return min(base_confidence, 1.0)


class SchemaMapper(BaseNode):
    """Database schema mapping node."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

    def process(self, state: GraphState) -> GraphState:
        """Map entities to database schema."""
        self._log_processing(state, "SchemaMapper")

        # TODO: Implement schema mapping logic
        pass

        return state


class SQLGenerationNode(BaseNode):
    """SQL ìƒì„± ì—ì´ì „íŠ¸ ë…¸ë“œ - ìžì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.db_schema = config.get("db_schema", {})

        # LLM ì„œë¹„ìŠ¤ì—ì„œ SQL LLM ê°€ì ¸ì˜¤ê¸°
        self.llm = self._get_sql_llm()

    def process(self, state: GraphState) -> GraphState:
        """ìžì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜"""
        self._log_processing(state, "SQLGenerationNode")

        # TODO: Implement SQL generation logic using LLM and templates
        pass

        return state


class SQLValidationNode(BaseNode):
    """SQL ê²€ì¦ ì—ì´ì „íŠ¸ ë…¸ë“œ - ìƒì„±ëœ SQLì˜ êµ¬ë¬¸ ë° ì˜ë¯¸ ê²€ì¦"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # ìºì‹±ëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
        # self.db_schema = get_cached_db_schema()

    def process(self, state: GraphState) -> GraphState:
        """SQL ì¿¼ë¦¬ ê²€ì¦"""
        self._log_processing(state, "SQLValidationNode")

        # TODO: Implement SQL validation logic
        pass

        return state


class DataSummarizationNode(BaseNode):
    """ë°ì´í„° ìš”ì•½ ì—ì´ì „íŠ¸ ë…¸ë“œ - SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìžì—°ì–´ë¡œ ìš”ì•½"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        # LLM ì„œë¹„ìŠ¤ì—ì„œ SQL LLM ê°€ì ¸ì˜¤ê¸° (ìš”ì•½ìš©ìœ¼ë¡œë„ ì‚¬ìš©)
        self.llm = self._get_sql_llm()

        # Removed: DataInsightAnalyzer (deleted module)
        # self.insight_analyzer = DataInsightAnalyzer(config)

    def process(self, state: GraphState) -> GraphState:
        """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìžì—°ì–´ë¡œ ìš”ì•½"""
        self._log_processing(state, "DataSummarizationNode")

        try:
            # ëŒ€í™” ì‘ë‹µì´ ìžˆëŠ” ê²½ìš° ìš”ì•½ ê±´ë„ˆë›°ê¸°
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            fanding_template = state.get("fanding_template")

            self.logger.info(
                f"DataSummarizationNode - conversation_response: {conversation_response is not None}"
            )
            self.logger.info(f"DataSummarizationNode - intent: {intent}")
            self.logger.info(
                f"DataSummarizationNode - fanding_template: {fanding_template is not None}"
            )

            if conversation_response or intent in [
                "GREETING",
                "GENERAL_CHAT",
                "HELP_REQUEST",
            ]:
                self.logger.info(
                    "Skipping data summarization for conversation response"
                )
                state["data_summary"] = (
                    conversation_response or "ëŒ€í™” ì‘ë‹µì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                )
                state["success"] = True
                return state

            # Fanding í…œí”Œë¦¿ì´ ìžˆëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if fanding_template:
                query_result = state.get("query_result")
                if query_result:
                    # Fanding í…œí”Œë¦¿ ê²°ê³¼ í¬ë§·íŒ…
                    from .fanding_sql_templates import FandingSQLTemplates

                    templates = FandingSQLTemplates()
                    formatted_result = templates.format_sql_result(
                        fanding_template, query_result
                    )
                    state["data_summary"] = formatted_result
                    state["success"] = True
                    self.logger.info(
                        f"ðŸŽ¯ Fanding template result formatted: {fanding_template.name}"
                    )
                    return state

            query_result = state.get("query_result")
            user_query = state.get("user_query")

            if not query_result:
                state["error_message"] = "No query result to summarize"
                return state

            # ê²°ê³¼ ë°ì´í„° ë¶„ì„
            result_stats = self._analyze_results(query_result)

            # ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ìˆ˜í–‰ - DISABLED: DataInsightAnalyzer deleted
            # try:
            #     sql_query = state.get("sql_query", "")
            #     insight_report = self.insight_analyzer.analyze_data(user_query, query_result, sql_query)
            #
            #     # ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ìƒíƒœì— ì €ìž¥
            #     state["insight_report"] = insight_report
            #     state["business_insights"] = insight_report.insights
            #     state["insight_summary"] = insight_report.summary
            #
            #     # ì¸ì‚¬ì´íŠ¸ê°€ ìžˆëŠ” ê²½ìš° ìš”ì•½ì— í¬í•¨
            #     if insight_report.insights:
            #         insight_text = self.insight_analyzer.format_insight_report(insight_report)
            #         state["insight_report_formatted"] = insight_text
            #     self.logger.info(f"Generated {len(insight_report.insights)} business insights")
            #
            # except Exception as e:
            #     self.logger.warning(f"Insight analysis failed: {e}")
            #     # ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ìš”ì•½ì€ ê³„ì† ì§„í–‰
            #     state["insight_report"] = None
            #     state["business_insights"] = []

            # Set default values since insight analyzer is disabled
            state["insight_report"] = None
            state["business_insights"] = None

            # ìš”ì•½ ìƒì„±
            if self.llm:
                summary = self._generate_ai_summary(
                    user_query, query_result, result_stats
                )
            else:
                summary = self._generate_fallback_summary(query_result, result_stats)

            state["data_summary"] = summary
            # Ensure result_stats is a dict for result_statistics
            if isinstance(result_stats, dict):
                state["result_statistics"] = result_stats
            else:
                state["result_statistics"] = None

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_summary_confidence(summary, result_stats)
            state["confidence_scores"]["data_summarization"] = confidence

            self.logger.info(f"Generated summary: {summary[:100]}...")

        except Exception as e:
            self.logger.error(f"Error in DataSummarizationNode: {str(e)}")
            state["error_message"] = f"Data summarization failed: {str(e)}"
            state["confidence_scores"]["data_summarization"] = 0.0

        return state

    def _analyze_results(self, query_result: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ ë°ì´í„° í†µê³„ ë¶„ì„"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return {"row_count": 0, "columns": [], "data_types": {}}

        stats = {
            "row_count": len(query_result),
            "columns": list(query_result[0].keys()) if query_result else [],
            "data_types": {},
            "sample_values": {},
            "null_counts": {},
        }

        if query_result:
            # ë°ì´í„° íƒ€ìž… ë¶„ì„
            for column in stats["columns"]:
                sample_values = [row.get(column) for row in query_result[:5]]
                stats["sample_values"][column] = sample_values

                # NULL ê°’ ê°œìˆ˜
                null_count = sum(1 for row in query_result if row.get(column) is None)
                stats["null_counts"][column] = null_count

                # ë°ì´í„° íƒ€ìž… ì¶”ë¡ 
                non_null_values = [v for v in sample_values if v is not None]
                if non_null_values:
                    first_value = non_null_values[0]
                    if isinstance(first_value, int):
                        stats["data_types"][column] = "integer"
                    elif isinstance(first_value, float):
                        stats["data_types"][column] = "float"
                    elif isinstance(first_value, str):
                        stats["data_types"][column] = "string"
                    else:
                        stats["data_types"][column] = "unknown"

        return stats

    def _generate_ai_summary(
        self, user_query: str, query_result: List[Dict[str, Any]], stats: Dict[str, Any]
    ) -> str:
        """AIë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±"""
        try:
            # ê²°ê³¼ ë°ì´í„° í¬ë§·íŒ…
            formatted_results = self._format_results(query_result)

            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìž ì¹œí™”ì ì¸ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {user_query}

ì¿¼ë¦¬ ê²°ê³¼ í†µê³„:
- ì´ í–‰ ìˆ˜: {stats['row_count']}
- ì»¬ëŸ¼ ìˆ˜: {len(stats['columns'])}
- ì»¬ëŸ¼ëª…: {', '.join(stats['columns'])}

ìƒ˜í”Œ ë°ì´í„°:
{formatted_results[:500]}...

ìš”êµ¬ì‚¬í•­:
1. ê²°ê³¼ì˜ ì£¼ìš” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…
2. ë°ì´í„°ì˜ ê·œëª¨ì™€ íŠ¹ì§•ì„ ì–¸ê¸‰
3. ì‚¬ìš©ìžê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
4. 3-5ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½
5. í•œêµ­ì–´ë¡œ ìž‘ì„±

ìš”ì•½:
"""

            # ìµœì‹  LangChain ë°©ì‹: SystemMessage ëŒ€ì‹  HumanMessageì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
            messages = [
                HumanMessage(
                    content=f"ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì‚¬ìš©ìž ì¹œí™”ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{summary_prompt}"
                )
            ]

            if not self.llm:
                self.logger.warning("LLM not initialized, returning default summary")
                return "ë°ì´í„° ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            response = self.llm.invoke(messages)
            response_content = response.content
            # Handle different response types
            if isinstance(response_content, str):
                return response_content.strip()
            elif isinstance(response_content, list):
                # Extract text from list of content blocks
                return " ".join(str(item) for item in response_content).strip()
            else:
                return str(response_content).strip()

        except Exception as e:
            self.logger.error(f"AI summary generation failed: {e}")
            return self._generate_fallback_summary(query_result, stats)

    def _generate_fallback_summary(
        self, query_result: List[Dict[str, Any]], stats: Dict[str, Any]
    ) -> str:
        """Fallback ìš”ì•½ ìƒì„±"""
        row_count = stats.get("row_count", 0)
        columns = stats.get("columns", [])

        if row_count == 0:
            return "ì¿¼ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        elif row_count == 1:
            return f"ì´ 1ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
        else:
            return (
                f"ì´ {row_count}ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
            )

    def _format_results(
        self, query_result: List[Dict[str, Any]], max_rows: int = 10
    ) -> str:
        """ê²°ê³¼ ë°ì´í„°ë¥¼ í¬ë§·íŒ…"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return "ê²°ê³¼ ì—†ìŒ"

        formatted_rows = []
        for i, row in enumerate(query_result[:max_rows]):
            row_str = f"í–‰ {i+1}: {dict(row)}"
            formatted_rows.append(row_str)

        result = "\n".join(formatted_rows)

        if len(query_result) > max_rows:
            result += f"\n... ë° {len(query_result) - max_rows}ê°œ í–‰ ë”"

        return result

    def _calculate_summary_confidence(
        self, summary: str, stats: Dict[str, Any]
    ) -> float:
        """ìš”ì•½ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.8

        # ìš”ì•½ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        if len(summary) > 50:
            base_confidence += 0.1

        # í†µê³„ ì •ë³´ í™œìš©ë„ì— ë”°ë¥¸ ì¡°ì •
        if stats.get("row_count", 0) > 0:
            base_confidence += 0.1

        return min(base_confidence, 1.0)
