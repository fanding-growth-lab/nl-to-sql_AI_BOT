"""
LangGraph Node Components for NL-to-SQL Pipeline

This module implements the individual nodes that make up the LangGraph pipeline.
"""

import re
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from langchain_core.messages import HumanMessage, SystemMessage
from langchain.output_parsers.json import SimpleJsonOutputParser

from .prompts import (
    SQLPromptTemplate,
    GREETING_PATTERNS, HELP_REQUEST_PATTERNS, GENERAL_CHAT_PATTERNS, GRATITUDE_PATTERNS,
    generate_greeting_response, generate_help_response, 
    generate_general_chat_response, generate_error_response,
    generate_clarification_question
)
from agentic_flow.llm_output_parser import parse_json_response
from agentic_flow.llm_service import get_llm_service
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
from .fanding_sql_templates import FandingSQLTemplates
from .date_utils import DateUtils
# Note: PythonCodeGeneratorNode is imported in state_machine.py to avoid circular import

from .state import (
    GraphState, Entity, SchemaMapping, SQLResult, 
    QueryIntent, QueryComplexity,
    set_sql_result, set_rag_mapping_result, set_dynamic_pattern, 
    set_fanding_template, set_conversation_response, clear_sql_generation,
    get_effective_sql, is_sql_generation_skipped
)
from core.config import get_settings
from core.db import get_db_session, get_cached_db_schema, extract_table_names, extract_column_names, validate_sql_syntax
from core.logging import get_logger

logger = get_logger(__name__)


# Constants and Configuration for Agentic Flow
# Note: Intent patterns and response templates moved to prompts.py
# Only business logic constants remain here

FANDING_DATA_KEYWORDS = [
    "ë©¤ë²„ì‹­", "ì„±ê³¼", "íšŒì›", "ë§¤ì¶œ", "ë°©ë¬¸ì", "ë¦¬í…ì…˜", "í¬ìŠ¤íŠ¸", 
    "ì¡°íšŒìˆ˜", "ì¸ê¸°", "ë¶„ì„", "í†µê³„", "ë¦¬í¬íŠ¸", "ì›”ê°„", 
    "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„", "í¬ë¦¬ì—ì´í„°", "í€ë”©", "í”„ë¡œì íŠ¸",
    "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”", "1ì›”", "2ì›”", "3ì›”", 
    "4ì›”", "5ì›”", "6ì›”", "7ì›”", "ì˜¬í•´", "ì‘ë…„", "ì§€ë‚œë‹¬", "ì´ë²ˆë‹¬",
    "ì‹ ê·œ", "ì´íƒˆ", "í™œì„±", "êµ¬ë…", "ê²°ì œ", "ìˆ˜ìµ", "ë§¤ì¶œì•¡",
    "í˜„í™©", "ìƒí™©", "ê²°ê³¼", "ì„±ê³¼ë¶„ì„", "ì„±ê³¼", "ë¶„ì„í•´ì¤˜", "ë³´ê³ ì„œ",
    "ìš”ì•½", "ì •ë¦¬", "í˜„ì¬", "ìµœê·¼", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼"
]

DATA_QUERY_PATTERNS = [
    "ì¡°íšŒ", "ê²€ìƒ‰", "ë³´ì—¬ì¤˜", "ì°¾ì•„", "í…Œì´ë¸”", "ì¿¼ë¦¬",
    "ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„",
    "ì•Œë ¤ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ê°€ì ¸ì™€", "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ",
    "ëª‡ ëª…", "ì–¼ë§ˆ", "ì–´ëŠ ì •ë„"
]

QUESTION_PATTERNS = [
    "ë­", "ë¬´ì—‡", "ì–´ë–¤", "ì–´ë””", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ëˆ„êµ¬",
    "ë­”ê°€", "ë­”ì§€", "ë­”ë°", "ë­ì•¼", "ë­ì§€", "ë­”ê°€ìš”", "ë­”ê°€ìš”?"
]

# Note: GRATITUDE_PATTERNS moved to prompts.py
# SQL Security Keywords
DANGEROUS_SQL_KEYWORDS = [
    'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 
    'TRUNCATE', 'EXEC', 'EXECUTE', 'UNION', 'SCRIPT',
    'GRANT', 'REVOKE', 'COMMIT', 'ROLLBACK'
]

# Table Name Mapping
TABLE_NAME_MAPPING = {
    # íšŒì› ê´€ë ¨
    "users": "t_member",
    "user": "t_member",
    "members": "t_member",
    "member": "t_member",
    "íšŒì›": "t_member",
    "ì‚¬ìš©ì": "t_member",
    
    # íšŒì› ì •ë³´ ê´€ë ¨
    "user_info": "t_member_info",
    "member_info": "t_member_info",
    "íšŒì›ì •ë³´": "t_member_info",
    
    # íšŒì› í”„ë¡œí•„ ê´€ë ¨
    "user_profile": "t_member_profile",
    "member_profile": "t_member_profile",
    "profiles": "t_member_profile",
    "í”„ë¡œí•„": "t_member_profile",
    
    # í¬ë¦¬ì—ì´í„° ê´€ë ¨
    "creators": "t_creator",
    "creator": "t_creator",
    "í¬ë¦¬ì—ì´í„°": "t_creator",
    "ì°½ì‘ì": "t_creator",
    
    # í€ë”© ê´€ë ¨
    "fundings": "t_funding",
    "funding": "t_funding",
    "í€ë”©": "t_funding",
    "projects": "t_funding",
    "í”„ë¡œì íŠ¸": "t_funding",
    
    # í€ë”© ì°¸ì—¬ì
    "funding_members": "t_funding_member",
    "backers": "t_funding_member",
    "supporters": "t_funding_member",
    "í›„ì›ì": "t_funding_member",
    
    # íŒ”ë¡œìš° ê´€ê³„
    "follows": "t_follow",
    "follow": "t_follow",
    "íŒ”ë¡œìš°": "t_follow",
    
    # ì£¼ë¬¸ ê´€ë ¨
    "orders": "t_order",
    "order": "t_order",
    "ì£¼ë¬¸": "t_order",
}

# Korean to English Mappings
KOREAN_MAPPINGS = {
    'ë³´ì—¬ì¤˜': 'show',
    'ì°¾ì•„ì¤˜': 'find',
    'ê°€ì ¸ì™€': 'get',
    'ê°œìˆ˜': 'count',
    'í•©ê³„': 'sum',
    'í‰ê· ': 'average',
    'ìµœëŒ€': 'max',
    'ìµœì†Œ': 'min'
}

# Entity Extraction Keywords
MEMBER_KEYWORDS = ["íšŒì›", "ë©¤ë²„", "ì‚¬ìš©ì", "ìœ ì €", "member", "user", "íšŒì›ìˆ˜", "ë©¤ë²„ìˆ˜"]
CREATOR_KEYWORDS = ["í¬ë¦¬ì—ì´í„°", "ì°½ì‘ì", "ì‘ê°€", "ì•„í‹°ìŠ¤íŠ¸", "ì œì‘ì", "creator"]
DATE_KEYWORDS = ["ì‹ ê·œ", "í˜„í™©", "ì›”ê°„", "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„"]
LOGIN_KEYWORDS = ["ë¡œê·¸ì¸", "login", "ì ‘ì†"]
RANKING_KEYWORDS = ["top", "top5", "top10", "ìƒìœ„", "ìµœê³ ", "ë§ì€", "ì ì€", "ìˆœìœ„"]
STATISTICS_KEYWORDS = ["ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„", "ë¶„ì„"]

# Confidence Thresholds
LLM_CONFIDENCE_THRESHOLD_HIGH = 0.8
LLM_CONFIDENCE_THRESHOLD_MEDIUM = 0.6
LLM_CONFIDENCE_THRESHOLD_LOW = 0.3
RAG_CONFIDENCE_THRESHOLD = 0.6
SQL_GENERATION_CONFIDENCE_THRESHOLD = 0.7


# Note: Response generator functions moved to prompts.py
# Import them from prompts module instead


class BaseNode(ABC):
    """Base class for all pipeline nodes."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(self.__class__.__name__)
        self._llm_service = None  # Lazy initialization
    
    @abstractmethod
    def process(self, state: GraphState) -> GraphState:
        """Process the current state and return updated state."""
        pass
    
    def _log_processing(self, state: GraphState, component: str):
        """Log processing information."""
        self.logger.info(
            f"Processing {component}",
            user_id=state.get("user_id"),
            channel_id=state.get("channel_id"),
            query=state.get("user_query", "")[:100]
        )
    
    def _get_llm_service(self):
        """Get LLM service instance (lazy initialization)."""
        if self._llm_service is None:
            self._llm_service = get_llm_service()
        return self._llm_service
    
    def _get_intent_llm(self):
        """Get intent classification LLM (lightweight, fast response)."""
        return self._get_llm_service().get_intent_llm()
    
    def _get_sql_llm(self):
        """Get SQL generation LLM (high-performance model)."""
        return self._get_llm_service().get_sql_llm()
    
    def _extract_creator_name(self, query: str) -> Optional[str]:
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ í¬ë¦¬ì—ì´í„° ì´ë¦„ ì¶”ì¶œ (ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ)
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            ì¶”ì¶œëœ í¬ë¦¬ì—ì´í„° ì´ë¦„ ë˜ëŠ” None
        
        ì˜ˆì‹œ:
            "ì„¸ìƒí•™ ê°œë¡ ì˜ íšŒì› ìˆ˜" -> "ì„¸ìƒí•™ ê°œë¡ "
            "ì„¸ìƒí•™ê°œë¡  í¬ë¦¬ì—ì´í„°ì˜ 10ì›” ì‹ ê·œ íšŒì›ìˆ˜" -> "ì„¸ìƒí•™ê°œë¡ "
            "í¬ë¦¬ì—ì´í„° ì„¸ìƒí•™ ê°œë¡  ì‹ ê·œ íšŒì›" -> "ì„¸ìƒí•™ ê°œë¡ "
            "ìƒìœ„ 5ê°œ í¬ë¦¬ì—ì´í„°" -> None (êµ¬ì²´ì ì¸ ì´ë¦„ ì—†ìŒ)
        """
        import re
        
        # íŒ¨í„´ 1: "í¬ë¦¬ì—ì´í„°ëª… í¬ë¦¬ì—ì´í„°" í˜•ì‹ (ì˜ˆ: "ì„¸ìƒí•™ê°œë¡  í¬ë¦¬ì—ì´í„°ì˜")
        pattern1 = r'([ê°€-í£a-zA-Z0-9\s]{2,30}?)\s+í¬ë¦¬ì—ì´í„°\s*(?:ì˜|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ)?'
        match1 = re.search(pattern1, query, re.IGNORECASE)
        if match1:
            creator_name = match1.group(1).strip()
            # "í¬ë¦¬ì—ì´í„°" í‚¤ì›Œë“œ ìì²´ëŠ” ì œì™¸
            if creator_name and creator_name.lower() not in ['í¬ë¦¬ì—ì´í„°', 'creator']:
                # ìˆ«ìë‚˜ ë‚ ì§œ í‘œí˜„ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸ (ì˜ˆ: "10ì›”", "2024ë…„")
                if not re.search(r'\d+\s*(?:ì›”|ë…„|ì¼|ê°œ|ëª…|ìœ„)', creator_name):
                    self.logger.debug(f"Extracted creator name (pattern 1): '{creator_name}'")
                    return creator_name
        
        # íŒ¨í„´ 2: "í¬ë¦¬ì—ì´í„°ëª…ì˜" í˜•ì‹ (ì˜ˆ: "ì„¸ìƒí•™ ê°œë¡ ì˜ íšŒì› ìˆ˜")
        pattern2 = r'([ê°€-í£a-zA-Z0-9\s]{2,30}?)\s*(?:ì˜|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ)\s+(?:ì‹ ê·œ|í™œì„±|íšŒì›|ë©¤ë²„|êµ¬ë…ì|íŒ”ë¡œì›Œ|ìˆ˜|ê°œìˆ˜|í†µê³„|ë¶„ì„|ì¡°íšŒ|ë³´ì—¬|ì•Œë ¤|ì°¾ì•„|ê°€ì ¸)'
        match2 = re.search(pattern2, query, re.IGNORECASE)
        if match2:
            creator_name = match2.group(1).strip()
            # í‚¤ì›Œë“œ ì œì™¸ ë¦¬ìŠ¤íŠ¸
            exclude_keywords = ['í¬ë¦¬ì—ì´í„°', 'creator', 'ìƒìœ„', 'top', 'ì¸ê¸°', 'popular', 'ìµœê³ ', 'ë§ì€', 'ì ì€', 'ì „ì²´', 'ëª¨ë“ ']
            if creator_name and len(creator_name) >= 2 and creator_name.lower() not in [kw.lower() for kw in exclude_keywords]:
                # ìˆ«ìë‚˜ ë‚ ì§œ í‘œí˜„ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸
                if not re.search(r'\d+\s*(?:ì›”|ë…„|ì¼|ê°œ|ëª…|ìœ„)', creator_name):
                    # "ìƒìœ„ 5ê°œ" ê°™ì€ íŒ¨í„´ ì œì™¸
                    if not re.search(r'(ìƒìœ„|top|ì¸ê¸°|ìµœê³ )\s*\d+', creator_name, re.IGNORECASE):
                        self.logger.debug(f"Extracted creator name (pattern 2): '{creator_name}'")
                        return creator_name
        
        # íŒ¨í„´ 3: "í¬ë¦¬ì—ì´í„° í¬ë¦¬ì—ì´í„°ëª…" í˜•ì‹ (ì˜ˆ: "í¬ë¦¬ì—ì´í„° ì„¸ìƒí•™ ê°œë¡ ")
        pattern3 = r'í¬ë¦¬ì—ì´í„°\s+([ê°€-í£a-zA-Z0-9\s]{2,30}?)(?:\s+(?:ì˜|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ)|ì‹ ê·œ|í™œì„±|íšŒì›|ë©¤ë²„|êµ¬ë…ì|íŒ”ë¡œì›Œ|ìˆ˜|ê°œìˆ˜|í†µê³„|ë¶„ì„|ì¡°íšŒ|ë³´ì—¬|ì•Œë ¤|ì°¾ì•„|ê°€ì ¸|\s+\d+)'
        match3 = re.search(pattern3, query, re.IGNORECASE)
        if match3:
            creator_name = match3.group(1).strip()
            if creator_name:
                # ìˆ«ìë‚˜ ë‚ ì§œ í‘œí˜„ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸
                if not re.search(r'\d+\s*(?:ì›”|ë…„|ì¼|ê°œ|ëª…|ìœ„)', creator_name):
                    self.logger.debug(f"Extracted creator name (pattern 3): '{creator_name}'")
                    return creator_name
        
        return None


class NLProcessor(BaseNode):
    """Natural Language Processing node for query analysis."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # Use centralized LLM service for intent classification
        self.llm = self._get_intent_llm()
        # FandingSQLTemplatesë¥¼ configì—ì„œ ê³µìœ í•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„± (ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€)
        if "fanding_templates" in config:
            self.fanding_templates = config["fanding_templates"]
            self.logger.debug("Using shared FandingSQLTemplates from config")
        else:
            # db_schemaëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì´ë¯¸ ë¡œë“œë¨ (ì—†ìœ¼ë©´ ìë™ ë¡œë“œ)
            self.fanding_templates = FandingSQLTemplates(db_schema=getattr(self, 'db_schema', None))
            config["fanding_templates"] = self.fanding_templates  # ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ configì— ì €ì¥
    
    def process(self, state: GraphState) -> GraphState:
        """Process natural language query and extract intent and entities."""
        self._log_processing(state, "NLProcessor")
        
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            user_query = state.get("user_query")
            if not user_query:
                self.logger.error("user_query is None or empty")
                # ì¬ì…ë ¥ ìš”ì²­ ì„¤ì •
                state["conversation_response"] = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì§€ ëª»í–ˆì–´ìš”. ğŸ˜Š\n\n"
                    "ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
                    "ì˜ˆì‹œ: '9ì›” ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜', 'í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜'"
                )
                state["skip_sql_generation"] = True
                state["needs_clarification"] = True  # ì¬ì…ë ¥ í•„ìš” í”Œë˜ê·¸ ì„¤ì •
                state["success"] = False
                return state
            
            # Normalize query
            normalized_query = self._normalize_query(user_query)
            
            # ì •ê·œí™”ëœ ì¿¼ë¦¬ ê²€ì¦
            if not normalized_query or len(normalized_query.strip()) == 0:
                self.logger.error("normalized_query is empty after processing")
                # ì¬ì…ë ¥ ìš”ì²­ ì„¤ì •
                state["conversation_response"] = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ğŸ¤”\n\n"
                    "ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
                    "ì˜ˆì‹œ: '9ì›” ì‹ ê·œ íšŒì› í˜„í™© ì•Œë ¤ì¤˜', 'í™œì„± íšŒì› ìˆ˜ ì¡°íšŒí•´ì¤˜'"
                )
                state["skip_sql_generation"] = True
                state["needs_clarification"] = True  # ì¬ì…ë ¥ í•„ìš” í”Œë˜ê·¸ ì„¤ì •
                state["success"] = False
                return state
            
            # needs_clarificationì´ ì´ë¯¸ í•´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸ (LLMIntentClassifierì—ì„œ ë³´ì™„ ì¿¼ë¦¬ ê²°í•©)
            if state.get("needs_clarification", False) and not state.get("conversation_response"):
                # needs_clarificationì´ Trueì˜€ì§€ë§Œ conversation_responseê°€ ì—†ë‹¤ë©´
                # LLMIntentClassifierì—ì„œ ë³´ì™„ ì¿¼ë¦¬ë¥¼ ê²°í•©í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                state["needs_clarification"] = False
                self.logger.info("Cleared needs_clarification flag - clarification followup detected")
            
            # Extract intent and entities (LLM ê²°ê³¼ í¬í•¨)
            llm_intent_result = state.get("llm_intent_result")
            intent, entities = self._extract_intent_and_entities(normalized_query, llm_intent_result)
            
            # Update state
            state["normalized_query"] = normalized_query
            state["intent"] = intent
            state["entities"] = entities
            
            # ì£¼ì˜: CHAT_PATH (GREETING, HELP_REQUEST, GENERAL_CHAT)ëŠ” route_after_intent_classificationì—ì„œ
            # ì´ë¯¸ data_summarizationìœ¼ë¡œ ë¼ìš°íŒ…ë˜ë¯€ë¡œ nl_processing ë…¸ë“œì—ëŠ” ë„ë‹¬í•˜ì§€ ì•ŠìŒ
            # ë”°ë¼ì„œ ì—¬ê¸°ì„œ ë„ë‹¬í•˜ëŠ” intentëŠ” ë°ì´í„° ì˜ë„(SIMPLE_AGGREGATION, COMPLEX_ANALYSIS)ë§Œ ì²˜ë¦¬
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ìš”ì²­ ì²˜ë¦¬ (SHOW/DESCRIBE ëŒ€ì•ˆ)
            # ì´ëŠ” ë°ì´í„° ì˜ë„ì´ì§€ë§Œ íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
            schema_info_response = self.fanding_templates.get_schema_info(user_query)
            if schema_info_response:
                state["conversation_response"] = schema_info_response
                state["intent"] = QueryIntent.HELP_REQUEST
                state["skip_sql_generation"] = True
                state["success"] = True
                self.logger.info(f"Schema information request handled: {user_query}")
                return state
            
            # ë°ì´í„° ì˜ë„ ì²˜ë¦¬ (SIMPLE_AGGREGATION, COMPLEX_ANALYSIS)
            if intent == QueryIntent.SIMPLE_AGGREGATION:
                # ê°„ë‹¨í•œ ì§‘ê³„ ì¿¼ë¦¬ - SQL ê²½ë¡œë¡œ ì²˜ë¦¬
                self.logger.info(f"Simple aggregation intent detected: {user_query}")
                # SQL ê²½ë¡œ: fanding_template ë§¤ì¹­ ì‹œë„
                self._handle_data_query(state, user_query)
                state["success"] = True
            
            elif intent == QueryIntent.COMPLEX_ANALYSIS:
                # ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ - Python ê²½ë¡œë¡œ ì²˜ë¦¬
                self.logger.info(f"Complex analysis intent detected: {user_query}")
                # Python ê²½ë¡œ ìµœì í™”: SQL í…œí”Œë¦¿ ë§¤ì¹­ ê±´ë„ˆë›°ê¸° (SQL ìƒì„±ì´ ì—†ìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”)
                # entitiesëŠ” ì´ë¯¸ ì¶”ì¶œë˜ì—ˆìœ¼ë¯€ë¡œ rag_schema_retrieverì—ì„œ ì‚¬ìš© ê°€ëŠ¥
                # _handle_data_query()ëŠ” SQL ê²½ë¡œ ì „ìš©ì´ë¯€ë¡œ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
                state["success"] = True
            
            else:
                # ì˜ˆì™¸ ìƒí™©: ë°ì´í„° ì˜ë„ê°€ ì•„ë‹Œ intentê°€ ë„ë‹¬í•œ ê²½ìš°
                # ì´ëŠ” ë¼ìš°íŒ… ë¡œì§ ì˜¤ë¥˜ì´ê±°ë‚˜ intent ë¶„ë¥˜ ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŒ
                self.logger.error(
                    f"Unexpected intent '{intent}' reached nlp_processing. "
                    f"This should only process SIMPLE_AGGREGATION or COMPLEX_ANALYSIS. "
                    f"Routing may be incorrect."
                )
                # ì•ˆì „í•˜ê²Œ ì²˜ë¦¬: ë°ì´í„° ì¡°íšŒë¡œ ê°„ì£¼í•˜ê³  ì§„í–‰
                self.logger.warning(f"Treating unexpected intent '{intent}' as SIMPLE_AGGREGATION for safety")
                self._handle_data_query(state, user_query)
                state["success"] = True
            
            # Log confidence
            confidence = self._calculate_confidence(normalized_query, intent, entities)
            state["confidence_scores"]["nl_processing"] = confidence
            
            self.logger.info(f"Processed query: {normalized_query}")
            self.logger.info(f"Intent: {intent}, Entities: {len(entities)}")
            
        except Exception as e:
            self.logger.error(f"Error in NLProcessor: {str(e)}", exc_info=True)
            # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            error_response = self._generate_error_response(e)
            state["conversation_response"] = error_response
            state["skip_sql_generation"] = True
            state["sql_query"] = None
            state["validated_sql"] = None
            state["success"] = False
            state["error_message"] = f"Natural language processing failed: {str(e)}"
        
        return state
    
    def _handle_greeting(self, user_query: str) -> str:
        """ì¸ì‚¬ë§ ì²˜ë¦¬ (ëœë¤ ì‘ë‹µ)"""
        return generate_greeting_response(user_query)
    
    def _handle_help_request(self, user_query: str) -> str:
        """ë„ì›€ë§ ìš”ì²­ ì²˜ë¦¬"""
        return generate_help_response(user_query)
    
    def _handle_general_chat(self, user_query: str) -> str:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ëœë¤ ì‘ë‹µ)"""
        return generate_general_chat_response(user_query)
    
    def _generate_error_response(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return generate_error_response(error)
    
    def _handle_data_query(self, state: GraphState, user_query: str) -> None:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ ì²˜ë¦¬ (RAG + ë™ì  ìŠ¤í‚¤ë§ˆ í™•ì¥ í†µí•©)"""
        # í¬ë¦¬ì—ì´í„° ì •ë³´ê°€ í¬í•¨ëœ ì¿¼ë¦¬ì¸ì§€ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì²´í¬ë§Œ ìˆ˜í–‰
        # (ì •í™•í•œ íŒŒì‹±ì€ SQLGenerationNodeì—ì„œ í•„ìš”í•  ë•Œë§Œ ìˆ˜í–‰)
        query_lower = user_query.lower()
        has_creator_keyword = (
            'í¬ë¦¬ì—ì´í„°' in query_lower or 
            'creator' in query_lower or
            any(keyword in query_lower for keyword in ['ì‘ê°€', 'ì•„í‹°ìŠ¤íŠ¸', 'ì œì‘ì'])
        )
        
        # Fanding í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„
        fanding_template = self.fanding_templates.match_query_to_template(user_query)
        if fanding_template:
            # í¬ë¦¬ì—ì´í„° ì •ë³´ê°€ í•„ìš”í•œ ì¿¼ë¦¬ì¸ë° í…œí”Œë¦¿ì— í¬ë¦¬ì—ì´í„° í•„í„°ë§ì´ ì—†ëŠ” ê²½ìš°
            # í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
            sql_template = fanding_template.sql_template if hasattr(fanding_template, 'sql_template') else str(fanding_template)
            if has_creator_keyword and 'creator' not in sql_template.lower() and 'creator_no' not in sql_template:
                self.logger.info(f"Fanding template matched but missing creator filter: {fanding_template.name}. Skipping template, will generate SQL with creator filter.")
                # í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
                fanding_template = None
            else:
                self.logger.info(f"Fanding template matched: {fanding_template.name}")
                set_fanding_template(state, fanding_template)
                state["skip_sql_generation"] = False
                state["success"] = True  # í…œí”Œë¦¿ ë§¤ì¹­ ì„±ê³µ
                self.logger.info(f"SQL template applied: {sql_template[:100]}...")
                return  # í…œí”Œë¦¿ ì‚¬ìš© ì„±ê³µ ì‹œ ì—¬ê¸°ì„œ ì¢…ë£Œ
        
        if not fanding_template:
            # 3. ë™ì  ì›”ë³„ í…œí”Œë¦¿ ìƒì„± ì‹œë„ (ë©¤ë²„ì‹­ ì„±ê³¼ ê´€ë ¨)
            try:
                dynamic_template = self.fanding_templates.create_dynamic_monthly_template(user_query)
                if dynamic_template:
                    self.logger.info(f"Dynamic monthly template created: {dynamic_template.name}")
                    set_fanding_template(state, dynamic_template)
                    state["skip_sql_generation"] = False
                    state["success"] = True  # ë™ì  í…œí”Œë¦¿ ìƒì„± ì„±ê³µ
                    self.logger.info(f"Dynamic SQL applied: {dynamic_template.sql_template[:100]}...")
                    return
            except Exception as e:
                self.logger.warning(f"Dynamic monthly template creation failed: {str(e)}")
            
            # 4. ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
            self.logger.info("No template/pattern matched, proceeding with general SQL generation")
            state["skip_sql_generation"] = False
            state["success"] = True  # ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰ (ì •ìƒ ì²˜ë¦¬)

    def _normalize_query(self, query: str) -> str:
        """Normalize the user query."""
        # Remove extra whitespace
        normalized = re.sub(r'\s+', ' ', query.strip())
        
        # Convert to lowercase for consistency
        normalized = normalized.lower()
        
        # Handle common Korean database terms (ìƒìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        for korean, english in KOREAN_MAPPINGS.items():
            normalized = normalized.replace(korean, english)
        
        return normalized
    
    def _extract_intent_and_entities(self, query: str, llm_intent_result: Optional[Dict] = None) -> Tuple[QueryIntent, List[Entity]]:
        """Extract intent and entities from the query."""
        
        # 1. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (MEDIUM ì„ê³„ê°’ 0.6 ì‚¬ìš©)
        # MEDIUM ì„ê³„ê°’: LLM ë¶„ë¥˜ê°€ ìƒë‹¹íˆ í™•ì‹¤í•  ë•Œë§Œ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¶„ë¥˜ ë°©ì§€
        if llm_intent_result and llm_intent_result.get("confidence", 0) >= LLM_CONFIDENCE_THRESHOLD_MEDIUM:
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(f"Using LLM intent classification: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})")
                # ì—”í‹°í‹°ë„ ì¶”ì¶œ
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                self.logger.warning(f"Invalid LLM intent: {llm_intent_result.get('intent')}")
        
        # 2. LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì°¸ê³  (LOW ì„ê³„ê°’ 0.3 ì‚¬ìš©)
        # LOW ì„ê³„ê°’: LLMì´ ë¶ˆí™•ì‹¤í•´ë„ ê·œì¹™ ê¸°ë°˜ë³´ë‹¤ëŠ” ë‚˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œí•œì˜ ì‹ ë¢°ë„ë¡œ ì°¸ê³ 
        if llm_intent_result and llm_intent_result.get("confidence", 0) >= LLM_CONFIDENCE_THRESHOLD_LOW:
            try:
                llm_intent = QueryIntent(llm_intent_result["intent"])
                self.logger.info(f"Using LLM intent as fallback: {llm_intent.value} (confidence: {llm_intent_result['confidence']:.2f})")
                entities = self._extract_entities_from_query(query)
                return llm_intent, entities
            except ValueError:
                pass
        
        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (LLM ì‹¤íŒ¨ ì‹œ fallback)
        if self._has_data_query_indicators(query):
            # ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆìœ¼ë©´ SIMPLE_AGGREGATIONìœ¼ë¡œ ë¶„ë¥˜ (LLM ì‹¤íŒ¨í•´ë„)
            # ë¶ˆëª…í™•í•œ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ SIMPLE_AGGREGATION ì‚¬ìš© (SQLì´ ë” ì•ˆì „)
            self.logger.info(f"Data query indicators detected, classifying as SIMPLE_AGGREGATION: {query}")
            entities = self._extract_entities_from_query(query)
            return QueryIntent.SIMPLE_AGGREGATION, entities
        
        # 4. ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹œë„ (ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        rule_based_intent = self._classify_intent_by_rules(query)
        
        if rule_based_intent != QueryIntent.UNKNOWN:
            # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ëœ ê²½ìš° (ì¸ì‚¬, ì¼ë°˜ ëŒ€í™” ë“±)
            return rule_based_intent, []
        
        # 5. ëª¨ë“  ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        return QueryIntent.GENERAL_CHAT, []
    
    def _extract_entities_from_query(self, query: str) -> List[Entity]:
        """ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
        query_lower = query.lower()
        
        # íšŒì› ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in MEMBER_KEYWORDS):
            entities.append(Entity(name="member", type="table", confidence=0.9))
        
        # í¬ë¦¬ì—ì´í„° ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in CREATOR_KEYWORDS):
            entities.append(Entity(name="creator", type="table", confidence=0.9))
        
        # ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in DATE_KEYWORDS):
            entities.append(Entity(name="date", type="column", confidence=0.8))
        
        # ë¡œê·¸ì¸ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in LOGIN_KEYWORDS):
            entities.append(Entity(name="login", type="table", confidence=0.8))
        
        # Top/ìˆœìœ„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in RANKING_KEYWORDS):
            entities.append(Entity(name="ranking", type="aggregation", confidence=0.8))
        
        # í†µê³„ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(keyword in query_lower for keyword in STATISTICS_KEYWORDS):
            entities.append(Entity(name="statistics", type="aggregation", confidence=0.8))
        
        return entities
    
    def _classify_intent_by_rules(self, query: str) -> QueryIntent:
        """ê·œì¹™ ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜ (ê°œì„ ëœ ë²„ì „)"""
        query_lower = query.lower().strip()
        
        # 1. ë¨¼ì € ëª…í™•í•œ ë¹„ë°ì´í„° ì˜ë„ íŒ¨í„´ í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        
        # 1-1. ì¸ì‚¬ë§ íŒ¨í„´ (ìµœìš°ì„ )
        if any(pattern in query_lower for pattern in GREETING_PATTERNS):
            return QueryIntent.GREETING
        
        # 1-2. ë„ì›€ë§ ìš”ì²­ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in HELP_REQUEST_PATTERNS):
            return QueryIntent.HELP_REQUEST
        
        # 1-3. ì¼ë°˜ ëŒ€í™” íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if any(pattern in query_lower for pattern in GENERAL_CHAT_PATTERNS):
            return QueryIntent.GENERAL_CHAT
        
        # 2. Fanding ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ í™•ì¸ (ëª…í™•í•œ ë°ì´í„° ì¡°íšŒ ì˜ë„ë§Œ)
        if any(keyword in query_lower for keyword in FANDING_DATA_KEYWORDS):
            return QueryIntent.SIMPLE_AGGREGATION
        
        # 3. ë°ì´í„° ì¡°íšŒ ì˜ë„ í‚¤ì›Œë“œ (ëª…í™•í•œ ì¡°íšŒ ì˜ë„)
        if any(pattern in query_lower for pattern in DATA_QUERY_PATTERNS):
            return QueryIntent.SIMPLE_AGGREGATION
        
        # 4. ì§ˆë¬¸ íŒ¨í„´ (ì˜ë¬¸ì‚¬ ê¸°ë°˜) - ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
        if any(pattern in query_lower for pattern in QUESTION_PATTERNS):
            # ì§ˆë¬¸ì´ì§€ë§Œ ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜
            return QueryIntent.GENERAL_CHAT
        
        # 5. ê°ì‚¬/ì¸ì‚¬ í‘œí˜„
        if any(pattern in query_lower for pattern in GRATITUDE_PATTERNS):
            return QueryIntent.GENERAL_CHAT
        
        # 6. ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜ (UNKNOWN ëŒ€ì‹ )
        return QueryIntent.GENERAL_CHAT
    
    def _has_data_query_indicators(self, query: str) -> bool:
        """ë°ì´í„° ì¡°íšŒ ì˜ë„ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        query_lower = query.lower()
        
        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œ (ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •)
        data_keywords = [
            "ì¡°íšŒ", "ê²€ìƒ‰", "ë°ì´í„°", "í…Œì´ë¸”", "ì¿¼ë¦¬",
            "ì‚¬ìš©ì", "íšŒì›", "í¬ë¦¬ì—ì´í„°", "í€ë”©", "í”„ë¡œì íŠ¸", "ì£¼ë¬¸",
            "ê°œìˆ˜", "ìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í†µê³„",
            "ë©¤ë²„ì‹­", "ì„±ê³¼", "ë§¤ì¶œ", "ë°©ë¬¸ì", "ë¦¬í…ì…˜", "í¬ìŠ¤íŠ¸",
            "ì¡°íšŒìˆ˜", "ì¸ê¸°", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì›”ê°„", "ì¼ê°„", "ì£¼ê°„", "ë…„ê°„",
            # ì¶”ê°€ í‚¤ì›Œë“œ
            "ë½‘ì•„ì¤˜", "ë½‘ì•„", "ì¶”ì¶œ", "ì„ íƒ", "ê³ ë¥´", "ì •ë ¬", "ìˆœìœ„",
            "top", "top5", "top10", "ìƒìœ„", "ìµœê³ ", "ë§ì€", "ì ì€",
            "íšŒì›ìˆ˜", "ë©¤ë²„ìˆ˜", "ì‚¬ìš©ììˆ˜", "ê°€ì…ì", "í™œì„±", "ì‹ ê·œ",
            "í¬ë¦¬ì—ì´í„°", "ì°½ì‘ì", "ì‘ê°€", "ì•„í‹°ìŠ¤íŠ¸", "ì œì‘ì"
        ]
        
        # ë°ì´í„° ì¡°íšŒì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´
        data_question_patterns = [
            "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ", "ëª‡ ëª…", "ëª‡ ê±´", "ëª‡ ê°œì˜", "ëª‡ ëª…ì˜",
            "ê°€ì ¸ì™€", "ì°¾ì•„ì¤˜", "ë³´ì—¬ì¤˜", "ì•Œë ¤ì¤˜"  # ë°ì´í„° ê´€ë ¨ ë§¥ë½ì—ì„œë§Œ
        ]
        
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ì œì™¸
        general_question_patterns = [
            "ë­ì•¼", "ë­”ê°€", "ë­”ì§€", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ",
            "í•  ìˆ˜ ìˆëŠ”", "í•  ìˆ˜ ìˆëŠ”ì§€", "í•  ìˆ˜ ìˆëŠ”ê²Œ", "í•  ìˆ˜ ìˆëŠ”ê²ƒ"
        ]
        
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë°ì´í„° ì¡°íšŒê°€ ì•„ë‹˜
        if any(pattern in query_lower for pattern in general_question_patterns):
            return False
            
        # ë°ì´í„° ì¡°íšŒ í‚¤ì›Œë“œë‚˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš°ë§Œ
        return (any(keyword in query_lower for keyword in data_keywords) or
                any(pattern in query_lower for pattern in data_question_patterns))
    
    def _calculate_confidence(self, query: str, intent: QueryIntent, entities: List[Entity]) -> float:
        """Calculate confidence score for the processing."""
        base_confidence = 0.8
        
        # Adjust based on intent clarity
        if intent != QueryIntent.UNKNOWN:
            base_confidence += 0.1
        
        # Adjust based on entity extraction
        if entities:
            avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
            base_confidence = (base_confidence + avg_entity_confidence) / 2
        
        return min(base_confidence, 1.0)


class SchemaMapper(BaseNode):
    """Database schema mapping node."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # db_schemaê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™” ì‹œì ì— í•œ ë²ˆë§Œ ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
        self.db_schema = config.get("db_schema") or {}
        if not self.db_schema or len(self.db_schema) == 0:
            from core.db import get_cached_db_schema
            self.db_schema = get_cached_db_schema()
            self.logger.debug("db_schema was empty in config, loaded from cache during initialization")
        
        # í…Œì´ë¸” ì´ë¦„ ë§¤í•‘: ì¼ë°˜ì ì¸ ì´ë¦„ -> ì‹¤ì œ DB í…Œì´ë¸”ëª…
        self.table_name_mapping = TABLE_NAME_MAPPING
    
    def process(self, state: GraphState) -> GraphState:
        """Map entities to database schema."""
        self._log_processing(state, "SchemaMapper")
        
        try:
            entities = state.get("entities", [])
            intent = state.get("intent")
            
            # Map entities to schema
            relevant_tables = self._find_relevant_tables(entities)
            relevant_columns = self._find_relevant_columns(entities, relevant_tables)
            relationships = self._find_relationships(relevant_tables)
            
            # Calculate confidence
            confidence = self._calculate_mapping_confidence(
                entities, relevant_tables, relevant_columns
            )
            
            # Create schema mapping
            schema_mapping = SchemaMapping(
                relevant_tables=relevant_tables,
                relevant_columns=relevant_columns,
                relationships=relationships,
                confidence=confidence
            )
            
            state["schema_mapping"] = schema_mapping
            state["confidence_scores"]["schema_mapping"] = confidence
            
            self.logger.info(f"Mapped to {len(relevant_tables)} tables, {len(relevant_columns)} columns")
            
        except Exception as e:
            self.logger.error(f"Error in SchemaMapper: {str(e)}")
            state["error_message"] = f"Schema mapping failed: {str(e)}"
        
        return state
    
    def _find_relevant_tables(self, entities: List[Entity]) -> List[str]:
        """Find relevant tables based on entities."""
        relevant_tables = []
        
        for entity in entities:
            if entity.type == "table":
                # Direct table mention
                table_name = self._normalize_table_name(entity.name)
                if table_name in self.db_schema:
                    relevant_tables.append(table_name)
            elif entity.type == "column":
                # Find tables containing this column
                for table_name, table_info in self.db_schema.items():
                    if entity.name in table_info.get("columns", {}):
                        relevant_tables.append(table_name)
        
        return list(set(relevant_tables))
    
    def _find_relevant_columns(self, entities: List[Entity], tables: List[str]) -> List[str]:
        """Find relevant columns based on entities and tables."""
        relevant_columns = []
        
        for entity in entities:
            if entity.type == "column":
                relevant_columns.append(entity.name)
        
        # Add columns from relevant tables
        for table in tables:
            if table in self.db_schema:
                table_columns = list(self.db_schema[table].get("columns", {}).keys())
                relevant_columns.extend(table_columns)
        
        return list(set(relevant_columns))
    
    def _find_relationships(self, tables: List[str]) -> List[Dict[str, str]]:
        """Find relationships between tables."""
        relationships = []
        
        # Simple relationship detection based on common patterns
        for table1 in tables:
            for table2 in tables:
                if table1 != table2:
                    # Check for foreign key relationships
                    if self._has_foreign_key_relationship(table1, table2):
                        relationships.append({
                            "from_table": table1,
                            "to_table": table2,
                            "type": "foreign_key"
                        })
        
        return relationships
    
    def _has_foreign_key_relationship(self, table1: str, table2: str) -> bool:
        """Check if there's a foreign key relationship between tables."""
        if table1 not in self.db_schema or table2 not in self.db_schema:
            return False
        
        # Simple heuristic: check if table1 has a column that references table2
        table1_columns = self.db_schema[table1].get("columns", {})
        for column_name, column_info in table1_columns.items():
            if column_info.get("type") == "foreign_key" and table2 in str(column_info):
                return True
        
        return False
    
    def _normalize_table_name(self, name: str) -> str:
        """Normalize table name to match schema."""
        # Remove common prefixes/suffixes
        name = name.lower().strip()
        
        # ë¨¼ì € ë§¤í•‘ í…Œì´ë¸”ì—ì„œ í™•ì¸
        if name in self.table_name_mapping:
            return self.table_name_mapping[name]
        
        # Handle common variations
        if name.endswith('s'):
            singular_name = name[:-1]
            if singular_name in self.table_name_mapping:
                return self.table_name_mapping[singular_name]
        
        # Check if it matches any table in the schema
        for table_name in self.db_schema.keys():
            if name in table_name.lower() or table_name.lower() in name:
                return table_name
        
        return name
    
    
    def _calculate_mapping_confidence(self, entities: List[Entity], tables: List[str], columns: List[str]) -> float:
        """Calculate confidence for schema mapping."""
        from agentic_flow.utils import calculate_mapping_confidence
        return calculate_mapping_confidence(entities, tables, columns)


class SQLGenerationNode(BaseNode):
    """
    SQL ìƒì„± ì—ì´ì „íŠ¸ ë…¸ë“œ - ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜
    
    - ê°„ë‹¨í•œ ì¿¼ë¦¬: Few-shot ì˜ˆì œ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ
    - ë³µì¡í•œ ì¿¼ë¦¬: RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ê²½ë¡œ
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # db_schemaê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™” ì‹œì ì— í•œ ë²ˆë§Œ ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
        self.db_schema = config.get("db_schema") or {}
        if not self.db_schema or len(self.db_schema) == 0:
            self.db_schema = get_cached_db_schema()
            self.logger.debug("db_schema was empty in config, loaded from cache during initialization")
        
        # LLM ì„œë¹„ìŠ¤ì—ì„œ SQL LLM ê°€ì ¸ì˜¤ê¸°
        self.llm = self._get_sql_llm()
        
        # SQLPromptTemplate ì´ˆê¸°í™” (RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³µì¡í•œ ì¿¼ë¦¬ìš©)
        self.prompt_template = SQLPromptTemplate(db_schema=self.db_schema)
        
        # DynamicSQLGenerator í†µí•©: Few-shot ì˜ˆì œ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œìš© í”„ë¡¬í”„íŠ¸
        self.simple_prompt = ChatPromptTemplate.from_messages([
            ("system", """ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•˜ì„¸ìš”.

í…Œì´ë¸”: t_member, t_creator, t_member_info, t_member_login_log

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
    "sql_query": "SELECT ...",
    "confidence": 0.9,
    "reasoning": "ì´ìœ "
}}"""),
            ("human", "ì¿¼ë¦¬: {query}")
        ])
        
        # JSON íŒŒì„œ ì´ˆê¸°í™” (Few-shot ê²½ë¡œìš©)
        self.json_parser = SimpleJsonOutputParser()
        
        # FandingSQLTemplatesë¥¼ configì—ì„œ ê³µìœ í•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„± (ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€)
        if "fanding_templates" in config:
            self.fanding_templates = config["fanding_templates"]
            self.logger.debug("Using shared FandingSQLTemplates from config")
        else:
            # db_schemaëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì´ë¯¸ ë¡œë“œë¨ (ì—†ìœ¼ë©´ ìë™ ë¡œë“œ)
            self.fanding_templates = FandingSQLTemplates(db_schema=getattr(self, 'db_schema', None))
            config["fanding_templates"] = self.fanding_templates  # ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ configì— ì €ì¥
    
    def process(self, state: GraphState) -> GraphState:
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜"""
        self._log_processing(state, "SQLGenerationNode")
        
        try:
            # ì¼ë°˜ ëŒ€í™”ì¸ ê²½ìš° SQL ìƒì„± ê±´ë„ˆë›°ê¸°
            skip_flag = state.get("skip_sql_generation", False)
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            
            self.logger.info(f"SQLGenerationNode - skip_sql_generation: {skip_flag}")
            self.logger.info(f"SQLGenerationNode - conversation_response: {conversation_response is not None}")
            self.logger.info(f"SQLGenerationNode - intent: {intent}")
            
            # # ëŒ€í™” ì¸í…íŠ¸ì¸ ê²½ìš° SQL ìƒì„± ê±´ë„ˆë›°ê¸°
            # if (skip_flag or conversation_response or 
            #     intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]):
            #     # ëª…í™•í™” ì§ˆë¬¸ ë˜ëŠ” ëŒ€í™” ì˜ë„ì¸ì§€ í™•ì¸
            #     if state.get("conversation_response") and "ì–´ë–¤" in str(state.get("conversation_response", "")):
            #         self.logger.info("Skipping SQL generation - clarification question detected")
            #     else:
            #         self.logger.info("Skipping SQL generation for conversation intent")
            #     state["sql_query"] = None
            #     state["validated_sql"] = None
            #     state["confidence_scores"]["sql_generation"] = 1.0
            #     return state
            
            user_query = state["user_query"]
            schema_mapping = state.get("schema_mapping")
            entities = state.get("entities", [])
            rag_schema_context = state.get("rag_schema_context", "")
            
            # ëˆ„ì  ìŠ¬ë¡¯ ë³‘í•© (ì´ì „ state + í˜„ì¬ ì§ˆì˜)
            prior_slots = state.get("slots") or {}
            new_slots = self._extract_simple_slots(user_query)
            slots = {**prior_slots, **{k: v for k, v in new_slots.items() if v}}
            state["slots"] = slots
            
            # ì¿¼ë¦¬ ë³µì¡ì„± í‰ê°€ ë° ì¡°ê±´ë¶€ ì²˜ë¦¬
            query_complexity = self._assess_query_complexity(user_query, entities, rag_schema_context)
            self.logger.info(f"Query complexity assessed as: {query_complexity}")
            
            # 1. NLProcessorì—ì„œ ì´ë¯¸ ë§¤ì¹­ëœ fanding_template í™•ì¸ (ìµœìš°ì„ )
            # NLProcessorì—ì„œ í…œí”Œë¦¿ ë§¤ì¹­ì„ ì´ë¯¸ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ì´ë¥¼ ìš°ì„  í™œìš©
            # ë‹¨, í¬ë¦¬ì—ì´í„° ì •ë³´ê°€ í•„ìš”í•œ ì¿¼ë¦¬ì¸ë° í…œí”Œë¦¿ì— í¬ë¦¬ì—ì´í„° í•„í„°ë§ì´ ì—†ëŠ” ê²½ìš°ëŠ” ì¼ë°˜ SQL ìƒì„±ìœ¼ë¡œ ì§„í–‰
            fanding_template = state.get("fanding_template")
            if fanding_template:
                # í…œí”Œë¦¿ ê°ì²´ì—ì„œ SQL ì¶”ì¶œ
                sql_template = None
                if hasattr(fanding_template, 'sql_template'):
                    sql_template = fanding_template.sql_template
                    template_name = fanding_template.name
                elif isinstance(fanding_template, dict):
                    sql_template = fanding_template.get("sql_template")
                    template_name = fanding_template.get("name", "unknown")
                
                if sql_template:
                    # í¬ë¦¬ì—ì´í„° ì •ë³´ê°€ í•„ìš”í•œ ì¿¼ë¦¬ì¸ì§€ í™•ì¸
                    creator_name = self._extract_creator_name(user_query)
                    if creator_name:
                        # í¬ë¦¬ì—ì´í„° ì •ë³´ê°€ í•„ìš”í•œë° í…œí”Œë¦¿ì— í¬ë¦¬ì—ì´í„° í•„í„°ë§ì´ ì—†ëŠ” ê²½ìš°
                        if 'creator' not in sql_template.lower() and 'creator_no' not in sql_template:
                            self.logger.info(f"Fanding template '{template_name}' matched but missing creator filter. Extracting creator info and adding to template.")
                            # í¬ë¦¬ì—ì´í„° ì •ë³´ ì¶”ì¶œ ë° ì¶”ê°€
                            creator_info = self._find_creator_by_name(creator_name)
                            if creator_info:
                                # db_schemaë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì°¾ê¸° (í•˜ë“œì½”ë”© ì œê±°)
                                creator_col = self._find_creator_column_in_sql_template(sql_template, state)
                                
                                if creator_col:
                                    # í…œí”Œë¦¿ SQLì— í¬ë¦¬ì—ì´í„° í•„í„° ì¶”ê°€
                                    # WHERE ì ˆì´ ìˆìœ¼ë©´ ANDë¡œ ì¶”ê°€, ì—†ìœ¼ë©´ WHERE ì¶”ê°€
                                    if 'WHERE' in sql_template.upper():
                                        # ê¸°ì¡´ WHERE ì ˆì— AND ì¶”ê°€
                                        sql_template = sql_template.rstrip(';').rstrip() + f" AND {creator_col} = :creator_no"
                                    else:
                                        # WHERE ì ˆ ì¶”ê°€
                                        sql_template = sql_template.rstrip(';').rstrip() + f" WHERE {creator_col} = :creator_no"
                                    
                                    # SQL íŒŒë¼ë¯¸í„° ì„¤ì •
                                    if "sql_params" not in state:
                                        state["sql_params"] = {}
                                    state["sql_params"]["creator_no"] = creator_info["creator_no"]
                                    
                                    self.logger.info(f"Added creator filter to template: {creator_col} = {creator_info['creator_no']} (creator: '{creator_name}')")
                                else:
                                    self.logger.warning(f"Could not find creator column in SQL template using db_schema, using template as-is")
                            else:
                                self.logger.warning(f"Creator '{creator_name}' not found in database, using template as-is")
                        else:
                            # í…œí”Œë¦¿ì— ì´ë¯¸ í¬ë¦¬ì—ì´í„° í•„í„°ë§ì´ ìˆëŠ” ê²½ìš°
                            self.logger.info(f"Fanding template '{template_name}' already includes creator filter")
                    
                    self.logger.info(f"Using fanding_template matched by NLProcessor: {template_name}")
                    state["sql_query"] = sql_template
                    state["confidence_scores"]["sql_generation"] = 0.9  # í…œí”Œë¦¿ ë§¤ì¹­ì€ ë†’ì€ ì‹ ë¢°ë„
                    self.logger.info(f"SQL from NLProcessor-matched template: {sql_template[:100]}...")
                    return state
            
            # 2. ê¸°ì¡´ dynamic_sql_resultê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            dynamic_sql_result = state.get("dynamic_sql_result")
            if dynamic_sql_result and isinstance(dynamic_sql_result, dict) and dynamic_sql_result.get("sql_query"):
                dynamic_confidence = dynamic_sql_result.get("confidence", 0.0)
                if dynamic_confidence >= 0.85:
                    self.logger.info(f"Using existing dynamic_sql_result (confidence: {dynamic_confidence:.2f})")
                    state["sql_query"] = dynamic_sql_result["sql_query"]
                    state["confidence_scores"]["sql_generation"] = dynamic_confidence
                return state
            
            # 3. RAG ë§¤í•‘ ê²°ê³¼ í™•ì¸ - ì‹ ë¢°ë„ ì„ê³„ê°’ ë‚®ì¶¤
            rag_result = state.get("rag_mapping_result")
            # RAG ì„ê³„ê°’(0.6) ì‚¬ìš©: RAG ë§¤í•‘ ê²°ê³¼ëŠ” ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì‹ ë¢°ë„ë¡œë„ ì‚¬ìš© ê°€ëŠ¥
            if rag_result and rag_result.confidence > RAG_CONFIDENCE_THRESHOLD:
                self.logger.info(f"Using RAG mapping result: {rag_result.source.value} (confidence: {rag_result.confidence:.2f})")
                set_sql_result(state, rag_result.sql_template, rag_result.confidence)
                return state
            
            # 2.5 ìŠ¬ë¡¯ ê¸°ë°˜ ê²°ì •ì  ë¹Œë“œ (ëˆ„ì  ìŠ¬ë¡¯ ì‚¬ìš©)
            slots = state.get("slots") or {}
            # intent ì¶”ë¡  ë³´ê°•: metricì´ active_membersì´ê³  creator/top_k/ì›”ì´ ì¡´ì¬í•˜ë©´ active_membersìš© intent
            if (slots.get("group_by") == "creator" or ("í¬ë¦¬ì—ì´í„°" in user_query)) and slots.get("top_k") and slots.get("month"):
                metric = slots.get("metric") or ("active_members" if ("í™œì„±" in user_query) else "new_members")
                # ëˆ„ì  ë°˜ì˜
                slots["metric"] = metric
                state["slots"] = slots
                month = slots.get("month")
                k = int(slots.get("top_k", 5))
                
                # í¬ë¦¬ì—ì´í„° ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "ì„¸ìƒí•™ ê°œë¡ ")
                creator_name = self._extract_creator_name(user_query)
                
                # í¬ë¦¬ì—ì´í„°ëª…ì´ ìˆìœ¼ë©´ DBì—ì„œ creator_no ê²€ìƒ‰ (SQL Injection ë°©ì§€ + ìœ ì‚¬ë„ ê²€ìƒ‰)
                creator_info = None
                if creator_name:
                    creator_info = self._find_creator_by_name(creator_name)
                    if creator_info:
                        self.logger.info(f"Creator found: '{creator_name}' -> creator_no={creator_info['creator_no']}, match_type={creator_info['match_type']}, similarity={creator_info['similarity']:.2f}")
                    else:
                        self.logger.warning(f"Creator not found for name: '{creator_name}'")
                
                creator_col = self._guess_creator_column(state)
                if creator_col:
                    if metric == "active_members":
                        # t_memberì—ëŠ” creator_noê°€ ì—†ìœ¼ë¯€ë¡œ t_fandingì„ JOINí•´ì•¼ í•¨
                        # data_dictionary.md ê¸°ì¤€: t_fanding.fanding_status = 'T' (í™œì„± ë©¤ë²„ì‹­)
                        if creator_info:
                            # êµ¬ì²´ì ì¸ í¬ë¦¬ì—ì´í„°ëª…ì´ ìˆê³  ë§¤ì¹­ëœ ê²½ìš°: creator_noë¥¼ ì§ì ‘ ì‚¬ìš© (SQL Injection ë°©ì§€)
                            # íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ì„ ìœ„í•´ SQL ì¿¼ë¦¬ì— íŒŒë¼ë¯¸í„° í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
                            sql = (
                                    "SELECT f.{creator_col}, cm.nickname AS creator_name, COUNT(DISTINCT f.member_no) AS active_members "
                                    "FROM t_fanding f "
                                    "INNER JOIN t_member m ON f.member_no = m.no "
                                    "INNER JOIN t_creator c ON f.{creator_col} = c.no "
                                    "INNER JOIN t_member cm ON c.member_no = cm.no "
                                    "WHERE f.fanding_status = 'T' AND m.status = 'A' AND f.{creator_col} = :creator_no "
                                    "GROUP BY f.{creator_col}, cm.nickname ORDER BY active_members DESC LIMIT :limit_k"
                            ).format(creator_col=creator_col)
                            # SQL íŒŒë¼ë¯¸í„°ë¥¼ stateì— ì €ì¥ (sql_execution ë…¸ë“œì—ì„œ ì‚¬ìš©)
                            if "sql_params" not in state:
                                state["sql_params"] = {}
                            state["sql_params"] = {
                                "creator_no": creator_info["creator_no"],
                                "limit_k": k
                            }
                        else:
                            # í¬ë¦¬ì—ì´í„°ëª…ì´ ì—†ëŠ” ê²½ìš°: ì „ì²´ í¬ë¦¬ì—ì´í„°ë³„ ì§‘ê³„
                            sql = (
                                "SELECT f.{creator_col}, COUNT(DISTINCT f.member_no) AS active_members "
                                "FROM t_fanding f "
                                "INNER JOIN t_member m ON f.member_no = m.no "
                                "WHERE f.fanding_status = 'T' AND m.status = 'A' "
                                "GROUP BY f.{creator_col} ORDER BY active_members DESC LIMIT :limit_k"
                            ).format(creator_col=creator_col)
                            if "sql_params" not in state:
                                state["sql_params"] = {}
                            state["sql_params"] = {"limit_k": k}
                    else:
                        # ì‹ ê·œ íšŒì›: t_fandingì˜ ins_datetime ì‚¬ìš© (ìµœì´ˆ íŒ¬ë”© ì‹œì‘ì¼)
                        if creator_info:
                            # êµ¬ì²´ì ì¸ í¬ë¦¬ì—ì´í„°ëª…ì´ ìˆê³  ë§¤ì¹­ëœ ê²½ìš°: creator_noë¥¼ ì§ì ‘ ì‚¬ìš©
                            sql = (
                                "SELECT f.{creator_col}, cm.nickname AS creator_name, COUNT(DISTINCT f.member_no) AS new_members "
                                "FROM t_fanding f "
                                "INNER JOIN t_creator c ON f.{creator_col} = c.no "
                                "INNER JOIN t_member cm ON c.member_no = cm.no "
                                "WHERE DATE_FORMAT(f.ins_datetime, '%Y-%m') = :month AND f.{creator_col} = :creator_no "
                                "GROUP BY f.{creator_col}, cm.nickname ORDER BY new_members DESC LIMIT :limit_k"
                            ).format(creator_col=creator_col)
                            if "sql_params" not in state:
                                state["sql_params"] = {}
                            state["sql_params"] = {
                                "creator_no": creator_info["creator_no"],
                                "month": month,
                                "limit_k": k
                            }
                        else:
                            # í¬ë¦¬ì—ì´í„°ëª…ì´ ì—†ëŠ” ê²½ìš°
                            sql = (
                                "SELECT f.{creator_col}, COUNT(DISTINCT f.member_no) AS new_members "
                                "FROM t_fanding f "
                                "WHERE DATE_FORMAT(f.ins_datetime, '%Y-%m') = :month "
                                "GROUP BY f.{creator_col} ORDER BY new_members DESC LIMIT :limit_k"
                            ).format(creator_col=creator_col)
                            if "sql_params" not in state:
                                state["sql_params"] = {}
                            state["sql_params"] = {
                                "month": month,
                                "limit_k": k
                            }
                    state["sql_query"] = sql
                    state["confidence_scores"]["sql_generation"] = 0.8
                    if creator_info:
                        self.logger.info(f"Built deterministic SQL using accumulated slots (creator_name: '{creator_name}' -> creator_no={creator_info['creator_no']}, match_type={creator_info['match_type']})")
                    else:
                        self.logger.info(f"Built deterministic SQL using accumulated slots (no specific creator name)")
                    return state
                else:
                    clarification = (
                        "í¬ë¦¬ì—ì´í„° ì‹ë³„ ì»¬ëŸ¼ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ì»¬ëŸ¼ìœ¼ë¡œ ê·¸ë£¹í•‘í• ê¹Œìš”? ì˜ˆ: creator_id/creator_no"
                    )
                    state["clarification_question"] = clarification
                    state["conversation_response"] = clarification
                    state["skip_sql_generation"] = True
                    state["needs_clarification"] = True  # ì¬ì…ë ¥ í•„ìš” í”Œë˜ê·¸ ì„¤ì •
                    state["confidence_scores"]["sql_generation"] = 0.0
                    return state
            
            # ë³µì¡ì„± ê¸°ë°˜ ì¡°ê±´ë¶€ SQL ìƒì„±
            existing_sql = state.get("sql_query")
            sql_validation_failed = state.get("sql_validation_failed", False)
            
            # ê¸°ì¡´ SQLì´ ìˆê³  ê²€ì¦ ì‹¤íŒ¨í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (Fanding í…œí”Œë¦¿ ë“±)
            if existing_sql and not sql_validation_failed and not rag_schema_context:
                self.logger.info(f"SQL already exists, skipping generation: {existing_sql[:100]}...")
                state["confidence_scores"]["sql_generation"] = state.get("confidence_scores", {}).get("sql_generation", 0.8)
                return state
            
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ SQL ë¬´íš¨í™”
            if sql_validation_failed:
                self.logger.info("Previous SQL validation failed, generating new SQL...")
                state["sql_query"] = None
                state["sql_validation_failed"] = False
                existing_sql = None
            
            # ë³µì¡ì„±ì— ë”°ë¼ SQL ìƒì„± ê²½ë¡œ ì„ íƒ
            if query_complexity == "simple":
                # ê°„ë‹¨í•œ ì¿¼ë¦¬: Few-shot ì˜ˆì œ ê¸°ë°˜ ë¹ ë¥¸ ê²½ë¡œ
                self.logger.info("Using simple SQL generation path (Few-shot based)")
                simple_result = self._generate_sql_simple(user_query)
                
                if simple_result and simple_result.get("sql_query"):
                    sql_query = simple_result["sql_query"]
                    confidence = simple_result.get("confidence", 0.7)
                    
                    state["sql_query"] = sql_query
                    state["confidence_scores"]["sql_generation"] = confidence
                    state["dynamic_sql_result"] = {
                        "sql_query": sql_query,
                        "confidence": confidence,
                        "reasoning": simple_result.get("reasoning", "Few-shot ì˜ˆì œ ê¸°ë°˜ ìƒì„±")
                    }
                    self.logger.info(f"Simple SQL generated successfully (confidence: {confidence:.2f})")
                else:
                    # ê°„ë‹¨í•œ ê²½ë¡œ ì‹¤íŒ¨ ì‹œ ë³µì¡í•œ ê²½ë¡œë¡œ í´ë°±
                    self.logger.warning("Simple SQL generation failed, falling back to complex path")
                    query_complexity = "complex"  # í´ë°±
            else:
                # ë³µì¡í•œ ì¿¼ë¦¬: RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ ê²½ë¡œ
                self.logger.info("Using complex SQL generation path (RAG context based)")
                complex_sql = self._generate_sql_complex(user_query, rag_schema_context, schema_mapping)
                
                if complex_sql:
                    state["sql_query"] = complex_sql
                    state["sql_generation_metadata"] = {
                        "model": self.llm.model if self.llm else "unknown",
                        "prompt_length": len(str(rag_schema_context)) if rag_schema_context else 0,
                        "response_length": len(complex_sql),
                        "mock": False
                    }
                    confidence = self._calculate_sql_confidence(
                        {"sql": complex_sql, "success": True, "response_length": len(complex_sql)},
                        schema_mapping
                    )
                    state["confidence_scores"]["sql_generation"] = confidence
                    self.logger.info(f"Complex SQL generated successfully (confidence: {confidence:.2f})")
            
            # SQL ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ì²˜ë¦¬
            if not state.get("sql_query"):
                self.logger.warning("Both SQL generation paths failed, attempting fallback...")
                
                # NLProcessorì—ì„œ ì´ë¯¸ ë§¤ì¹­ì„ ì‹œë„í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•ŠìŒ
                # ëŒ€ì‹  ìµœì¢… í´ë°±: ëª…í™•í™” ì§ˆë¬¸ ìš”ì²­
                clarification = self._build_clarification_question(user_query)
                state["clarification_question"] = clarification
                state["conversation_response"] = clarification
                state["skip_sql_generation"] = True
                state["needs_clarification"] = True
                state["confidence_scores"]["sql_generation"] = 0.0
                self.logger.info("All SQL generation methods failed, asking for clarification")
            
        except Exception as e:
            self.logger.error(f"Error in SQLGenerationNode: {str(e)}")
            state["error_message"] = f"SQL generation failed: {str(e)}"
            state["confidence_scores"]["sql_generation"] = 0.0
        
        return state
    
    def _build_clarification_question(self, user_query: str) -> str:
        """ê°„ë‹¨í•œ ëª…í™•í™” ì§ˆë¬¸ ìƒì„± (ê¸°ê°„/Top-K/ì§€í‘œ ìš°ì„ )"""
        return generate_clarification_question(user_query)
    
    def _calculate_sql_confidence(self, result: Dict[str, Any], schema_mapping) -> float:
        """SQL ìƒì„± ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
        from agentic_flow.utils import calculate_sql_confidence
        return calculate_sql_confidence(result, schema_mapping)
    
    def _extract_simple_slots(self, query: str) -> Dict[str, Any]:
        """ê°„ë‹¨ ìŠ¬ë¡¯ ì¶”ì¶œ: month, top_k, intent(creator_topk_new_members)"""
        q = query.lower()
        month = DateUtils.get_analysis_month(query)
        # top-k
        top_k = 5
        m = re.search(r"top\s*(\d+)|ìƒìœ„\s*(\d+)", q)
        if m:
            top_k = int([g for g in m.groups() if g][0])
        # intent
        intent = None
        if ("í¬ë¦¬ì—ì´í„°" in q or "creator" in q) and ("top" in q or "ìƒìœ„" in q) and ("ì‹ ê·œ" in q or "íšŒì›" in q):
            intent = "creator_topk_new_members"
        return {"month": month, "top_k": top_k, "intent": intent}
    
    # _extract_creator_nameì€ BaseNodeë¡œ ì´ë™ë˜ì—ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°ë¨
    
    def _find_creator_by_name(self, creator_name: str) -> Optional[Dict[str, Any]]:
        """
        í¬ë¦¬ì—ì´í„° ì´ë¦„ìœ¼ë¡œ DBì—ì„œ í¬ë¦¬ì—ì´í„° ê²€ìƒ‰ (SQL Injection ë°©ì§€ + ìœ ì‚¬ë„ ê²€ìƒ‰)
        
        Args:
            creator_name: í¬ë¦¬ì—ì´í„° ì´ë¦„ (ì˜ˆ: "ì„¸ìƒí•™ ê°œë¡ ")
            
        Returns:
            {
                "creator_no": int,
                "nickname": str,
                "match_type": "exact" | "partial" | "similar",
                "similarity": float
            } ë˜ëŠ” None
        """
        from core.db import execute_query
        from difflib import SequenceMatcher
        
        if not creator_name or len(creator_name.strip()) < 2:
            return None
        
        creator_name = creator_name.strip()
        
        try:
            # 1. ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            exact_query = """
                SELECT c.no AS creator_no, m.nickname
                FROM t_creator c
                INNER JOIN t_member m ON c.member_no = m.no
                WHERE m.nickname = :creator_name
                LIMIT 1
            """
            exact_results = execute_query(exact_query, {"creator_name": creator_name}, readonly=True)
            
            if exact_results and len(exact_results) > 0:
                self.logger.debug(f"Exact match found for creator name: '{creator_name}' -> creator_no: {exact_results[0]['creator_no']}")
                return {
                    "creator_no": exact_results[0]["creator_no"],
                    "nickname": exact_results[0]["nickname"],
                    "match_type": "exact",
                    "similarity": 1.0
                }
            
            # 2. ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (LIKE ì‚¬ìš©, íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ìœ¼ë¡œ SQL Injection ë°©ì§€)
            # ë„ì–´ì“°ê¸° ë¬¸ì œ í•´ê²°: ë„ì–´ì“°ê¸° ìˆëŠ” ë²„ì „ê³¼ ì—†ëŠ” ë²„ì „ ëª¨ë‘ ê²€ìƒ‰
            partial_query = """
                SELECT c.no AS creator_no, m.nickname
                FROM t_creator c
                INNER JOIN t_member m ON c.member_no = m.no
                WHERE m.nickname LIKE :creator_pattern OR m.nickname LIKE :creator_pattern_no_space
                LIMIT 20
            """
            # ë¶€ë¶„ ë§¤ì¹­: ì›ë³¸ê³¼ ë„ì–´ì“°ê¸° ì œê±° ë²„ì „ ëª¨ë‘ ê²€ìƒ‰
            creator_pattern = f"%{creator_name}%"
            creator_pattern_no_space = f"%{creator_name.replace(' ', '')}%"  # ë„ì–´ì“°ê¸° ì œê±°
            partial_results = execute_query(
                partial_query, 
                {
                    "creator_pattern": creator_pattern,
                    "creator_pattern_no_space": creator_pattern_no_space
                }, 
                readonly=True
            )
            
            if partial_results and len(partial_results) > 0:
                # ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ì •ê·œí™” í•¨ìˆ˜ (ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                def normalize_for_similarity(text: str) -> str:
                    """ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ê·œí™” (ë„ì–´ì“°ê¸°, í•˜ì´í”ˆ, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
                    import re
                    # ë„ì–´ì“°ê¸°, í•˜ì´í”ˆ, íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì†Œë¬¸ì ë³€í™˜
                    normalized = re.sub(r'[\s\-_\-]', '', text.lower())
                    return normalized
                
                # ì—¬ëŸ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìœ ì‚¬ë„ë¡œ ì •ë ¬
                if len(partial_results) > 1:
                    # ìœ ì‚¬ë„ ê³„ì‚° ë° ì •ë ¬ (ì •ê·œí™”ëœ ë²„ì „ìœ¼ë¡œ ë¹„êµ)
                    scored_results = []
                    normalized_creator_name = normalize_for_similarity(creator_name)
                    
                    for result in partial_results:
                        normalized_nickname = normalize_for_similarity(result["nickname"])
                        # ì •ê·œí™”ëœ ë²„ì „ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = SequenceMatcher(None, normalized_creator_name, normalized_nickname).ratio()
                        
                        # ì¶”ê°€ ì ìˆ˜: ì›ë³¸ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                        if creator_name.lower() in result["nickname"].lower() or creator_name.replace(' ', '').lower() in result["nickname"].lower():
                            similarity = max(similarity, 0.7)  # ìµœì†Œ 0.7 ë³´ì¥
                        
                        scored_results.append({
                            **result,
                            "similarity": similarity
                        })
                    scored_results.sort(key=lambda x: x["similarity"], reverse=True)
                    best_match = scored_results[0]
                    
                    # ìœ ì‚¬ë„ê°€ 0.5 ì´ìƒì¸ ê²½ìš°ë§Œ ë°˜í™˜ (ì„ê³„ê°’ ë‚®ì¶¤)
                    if best_match["similarity"] >= 0.5:
                        self.logger.debug(f"Similarity match found for creator name: '{creator_name}' -> '{best_match['nickname']}' (similarity: {best_match['similarity']:.2f})")
                        return {
                            "creator_no": best_match["creator_no"],
                            "nickname": best_match["nickname"],
                            "match_type": "similar" if best_match["similarity"] < 0.9 else "partial",
                            "similarity": best_match["similarity"]
                        }
                else:
                    # ë‹¨ì¼ ê²°ê³¼
                    result = partial_results[0]
                    normalized_creator_name = normalize_for_similarity(creator_name)
                    normalized_nickname = normalize_for_similarity(result["nickname"])
                    similarity = SequenceMatcher(None, normalized_creator_name, normalized_nickname).ratio()
                    
                    # ì¶”ê°€ ì ìˆ˜: ì›ë³¸ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                    if creator_name.lower() in result["nickname"].lower() or creator_name.replace(' ', '').lower() in result["nickname"].lower():
                        similarity = max(similarity, 0.7)  # ìµœì†Œ 0.7 ë³´ì¥
                    
                    # ìœ ì‚¬ë„ ì„ê³„ê°’ì„ 0.5ë¡œ ë‚®ì¶¤
                    if similarity >= 0.5:
                        self.logger.debug(f"Partial match found for creator name: '{creator_name}' -> '{result['nickname']}' (similarity: {similarity:.2f})")
                        return {
                            "creator_no": result["creator_no"],
                            "nickname": result["nickname"],
                            "match_type": "partial",
                            "similarity": similarity
                        }
            
            # 3. ìœ ì‚¬ë„ ê²€ìƒ‰ (ëª¨ë“  í¬ë¦¬ì—ì´í„°ì™€ ë¹„êµ, ì„±ëŠ¥ìƒ ì œí•œì )
            # ì‹¤ì œë¡œëŠ” ë¶€ë¶„ ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
            self.logger.warning(f"No creator found for name: '{creator_name}'")
            return None
            
        except Exception as e:
            self.logger.error(f"Error finding creator by name '{creator_name}': {e}")
            return None
    
    def _guess_creator_column(self, state: Optional[GraphState] = None) -> Optional[str]:
        """
        db_schemaì—ì„œ ê°€ëŠ¥í•œ í¬ë¦¬ì—ì´í„° ì‹ë³„ ì»¬ëŸ¼ ì¶”ì •
        
        Args:
            state: í˜„ì¬ ìƒíƒœ (ì„ íƒì , ê´€ë ¨ í…Œì´ë¸” ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©)
            
        Returns:
            í¬ë¦¬ì—ì´í„° ì»¬ëŸ¼ëª… (ì˜ˆ: "creator_no", "seller_creator_no") ë˜ëŠ” None
        
        Note:
            db_schemaëŠ” __init__ì—ì„œ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¬ë¡œë“œí•˜ì§€ ì•ŠìŒ
        """
        # ìš°ì„ ìˆœìœ„: creator_no > seller_creator_no > creator_id > creator
        candidates = ["creator_no", "seller_creator_no", "creator_id", "creator"]
        
        # 1. stateì—ì„œ ê´€ë ¨ í…Œì´ë¸” ì •ë³´ í™•ì¸ (ê°€ì¥ ì •í™•)
        if state:
            schema_mapping = state.get("agent_schema_mapping") or state.get("schema_mapping")
            if schema_mapping:
                relevant_tables = []
                if isinstance(schema_mapping, dict):
                    relevant_tables = schema_mapping.get("relevant_tables", [])
                elif hasattr(schema_mapping, "relevant_tables"):
                    relevant_tables = schema_mapping.relevant_tables
                
                # ê´€ë ¨ í…Œì´ë¸”ë“¤ì—ì„œ í¬ë¦¬ì—ì´í„° ì»¬ëŸ¼ ì°¾ê¸°
                for table_name in relevant_tables:
                    if table_name in self.db_schema:
                        table_cols = self.db_schema[table_name].get("columns", {})
                        for candidate in candidates:
                            if candidate in table_cols:
                                self.logger.debug(f"Found creator column '{candidate}' in table '{table_name}' from schema_mapping")
                                return candidate
        
        # 2. stateì—ì„œ ì‚¬ìš©ëœ SQL ì¿¼ë¦¬ì˜ í…Œì´ë¸” í™•ì¸
        if state:
            sql_query = state.get("sql_query")
            if sql_query:
                from core.db import extract_table_names
                used_tables = extract_table_names(sql_query)
                for table_name in used_tables:
                    if table_name in self.db_schema:
                        table_cols = self.db_schema[table_name].get("columns", {})
                        for candidate in candidates:
                            if candidate in table_cols:
                                self.logger.debug(f"Found creator column '{candidate}' in table '{table_name}' from SQL query")
                                return candidate
        
        # 3. ì¼ë°˜ì ìœ¼ë¡œ í¬ë¦¬ì—ì´í„° ê´€ë ¨ í…Œì´ë¸”ë“¤ì—ì„œ ì°¾ê¸°
        # data_dictionary.md ê¸°ì¤€: t_fanding, t_tier, t_creator, t_payment ë“±
        creator_related_tables = [
            "t_fanding", "t_tier", "t_creator", "t_payment", 
            "t_event", "t_follow", "t_creator_coupon"
        ]
        
        for table_name in creator_related_tables:
            if table_name in self.db_schema:
                table_cols = self.db_schema[table_name].get("columns", {})
                for candidate in candidates:
                    if candidate in table_cols:
                        self.logger.debug(f"Found creator column '{candidate}' in common creator-related table '{table_name}'")
                        return candidate
        
        # 4. db_schemaê°€ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ì‹œ ë¡œë“œ ì‹œë„
        if not self.db_schema or len(self.db_schema) == 0:
            from core.db import get_cached_db_schema
            self.db_schema = get_cached_db_schema()
            self.logger.debug("db_schema was empty, reloaded from cache in _guess_creator_column")
        
        # 5. ëª¨ë“  í…Œì´ë¸”ì—ì„œ í¬ë¦¬ì—ì´í„° ì»¬ëŸ¼ ê²€ìƒ‰ (fallback)
        for table_name, table_info in self.db_schema.items():
            table_cols = table_info.get("columns", {})
            for candidate in candidates:
                if candidate in table_cols:
                    self.logger.debug(f"Found creator column '{candidate}' in table '{table_name}' (fallback search)")
                    return candidate
        
        self.logger.warning("No creator column found in database schema")
        return None
    
    def _find_creator_column_in_sql_template(self, sql_template: str, state: Optional[GraphState] = None) -> Optional[str]:
        """
        SQL í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•˜ëŠ” í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ db_schemaì—ì„œ creator ì»¬ëŸ¼ ì°¾ê¸°
        
        Args:
            sql_template: SQL í…œí”Œë¦¿ ë¬¸ìì—´
            state: GraphState (ì„ íƒì )
            
        Returns:
            í…Œì´ë¸” aliasë¥¼ í¬í•¨í•œ creator ì»¬ëŸ¼ëª… (ì˜ˆ: 'f.creator_no') ë˜ëŠ” None
        """
        if not self.db_schema or len(self.db_schema) == 0:
            from core.db import get_cached_db_schema
            self.db_schema = get_cached_db_schema()
            self.logger.debug("db_schema was empty, reloaded from cache in _find_creator_column_in_sql_template")
        
        # SQL í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•˜ëŠ” í…Œì´ë¸”ê³¼ alias ì¶”ì¶œ
        import re
        
        # FROM ì ˆì—ì„œ í…Œì´ë¸”ëª…ê³¼ alias ì¶”ì¶œ (ê°œì„ : aliasê°€ ì—†ëŠ” ê²½ìš°ë„ ì²˜ë¦¬)
        # íŒ¨í„´ 1: "FROM t_member_info m" (alias ìˆìŒ)
        from_pattern_with_alias = r'FROM\s+(\w+)\s+(\w+)'
        # íŒ¨í„´ 2: "FROM t_member_info" (alias ì—†ìŒ)
        from_pattern_no_alias = r'FROM\s+(\w+)(?:\s+WHERE|\s+$)'
        from_matches = re.findall(from_pattern_with_alias, sql_template, re.IGNORECASE)
        from_matches_no_alias = re.findall(from_pattern_no_alias, sql_template, re.IGNORECASE)
        
        # JOIN ì ˆì—ì„œë„ í…Œì´ë¸”ëª…ê³¼ alias ì¶”ì¶œ
        join_pattern = r'JOIN\s+(\w+)\s+(\w+)'
        join_matches = re.findall(join_pattern, sql_template, re.IGNORECASE)
        
        all_tables = {}
        for table_name, alias in from_matches + join_matches:
            all_tables[alias] = table_name
        
        # aliasê°€ ì—†ëŠ” ê²½ìš°: í…Œì´ë¸”ëª…ì„ aliasë¡œ ì‚¬ìš©
        for table_name in from_matches_no_alias:
            if table_name not in all_tables.values():
                all_tables[table_name] = table_name  # í…Œì´ë¸”ëª…ì„ aliasë¡œ ì‚¬ìš©
        
        # creator ì»¬ëŸ¼ í›„ë³´ë“¤
        creator_candidates = ['creator_no', 'creator_id', 'seller_creator_no']
        
        # ê° í…Œì´ë¸”ì—ì„œ creator ì»¬ëŸ¼ ì°¾ê¸°
        for alias, table_name in all_tables.items():
            if table_name in self.db_schema:
                table_cols = self.db_schema[table_name].get("columns", {})
                for candidate in creator_candidates:
                    if candidate in table_cols:
                        self.logger.debug(f"Found creator column '{candidate}' in table '{table_name}' (alias: '{alias}')")
                        return f"{alias}.{candidate}"
        
        # t_fandingì´ ì‚¬ìš©ë˜ëŠ” ê²½ìš° (ê°€ì¥ ì¼ë°˜ì )
        if 't_fanding' in [t for t in all_tables.values()]:
            fanding_alias = [alias for alias, table in all_tables.items() if table == 't_fanding'][0]
            if 't_fanding' in self.db_schema:
                fanding_cols = self.db_schema['t_fanding'].get("columns", {})
                for candidate in creator_candidates:
                    if candidate in fanding_cols:
                        self.logger.debug(f"Found creator column '{candidate}' in t_fanding (alias: '{fanding_alias}')")
                        return f"{fanding_alias}.{candidate}"
        
        self.logger.warning(f"Could not find creator column in SQL template tables: {list(all_tables.values())}")
        return None
    
    def _assess_query_complexity(self, query: str, entities: List[Entity], rag_context: Optional[str] = None) -> str:
        """
        ì¿¼ë¦¬ ë³µì¡ì„± í‰ê°€
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            rag_context: RAG ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
            
        Returns:
            "simple" ë˜ëŠ” "complex"
        """
        query_lower = query.lower()
        
        # ë³µì¡í•œ ì¿¼ë¦¬ ì§€í‘œ
        complexity_indicators = [
            "join", "union", "subquery", "ì„œë¸Œì¿¼ë¦¬", "êµì§‘í•©", "í•©ì§‘í•©",
            "group by", "having", "order by", "window", "over",
            "ë¶„ì„", "í†µê³„", "íŠ¸ë Œë“œ", "íŒ¨í„´", "ë¹„êµ", "ìƒê´€ê´€ê³„",
            "case when", "if", "nullif", "coalesce"
        ]
        
        # ê°„ë‹¨í•œ ì¿¼ë¦¬ ì§€í‘œ
        simplicity_indicators = [
            "count", "sum", "avg", "max", "min",
            "ê°œìˆ˜", "í•©ê³„", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ",
            "ëª‡ ëª…", "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ"
        ]
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity_score = 0
        simplicity_score = 0
        
        # ë³µì¡ë„ ì§€í‘œ í™•ì¸
        for indicator in complexity_indicators:
            if indicator in query_lower:
                complexity_score += 2
        
        # ê°„ë‹¨ë„ ì§€í‘œ í™•ì¸
        for indicator in simplicity_indicators:
            if indicator in query_lower:
                simplicity_score += 1
        
        # ì—”í‹°í‹° ìˆ˜ í™•ì¸ (ì—”í‹°í‹°ê°€ ë§ì„ìˆ˜ë¡ ë³µì¡í•  ê°€ëŠ¥ì„±)
        if len(entities) > 3:
            complexity_score += 1
        
        # RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë³µì¡í•œ ì¿¼ë¦¬ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        if rag_context and len(rag_context) > 500:
            complexity_score += 1
        
        # ìµœì¢… ê²°ì •
        if complexity_score >= 2:
            return "complex"
        elif simplicity_score >= 2 and complexity_score == 0:
            return "simple"
        else:
            # ì• ë§¤í•œ ê²½ìš°: ê¸°ë³¸ì ìœ¼ë¡œ simpleë¡œ ì²˜ë¦¬ (ë¹ ë¥¸ ê²½ë¡œ ìš°ì„ )
            return "simple"
    
    def _generate_sql_simple(self, query: str) -> Optional[Dict[str, Any]]:
        """
        Few-shot ì˜ˆì œ ê¸°ë°˜ ë¹ ë¥¸ SQL ìƒì„± (DynamicSQLGenerator ë°©ì‹ í†µí•©)
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            {"sql_query": str, "confidence": float, "reasoning": str} ë˜ëŠ” None
        """
        try:
            if not self.llm:
                self.logger.warning("LLM not available for simple SQL generation")
                return None
            
            # Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±
            formatted_prompt = self.simple_prompt.format(query=query)
            
            # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            messages = [HumanMessage(content=formatted_prompt)]
            
            # LLM í˜¸ì¶œ
            self.logger.debug(f"Calling LLM for simple SQL generation: {query[:50]}...")
            response = self.llm.invoke(messages)
            
            if not response:
                self.logger.warning("LLM returned None response for simple SQL generation")
                return None
            
            # JSON íŒŒì‹±
            result_data = parse_json_response(response, parser=self.json_parser, fallback_extract=True)
            
            if not result_data:
                self.logger.warning("Failed to parse JSON from simple SQL generation response")
                return None
            
            # ê²°ê³¼ ì¶”ì¶œ
            sql_query = str(result_data.get("sql_query", "")).strip()
            confidence = float(result_data.get("confidence", 0.7))
            reasoning = str(result_data.get("reasoning", "Few-shot ì˜ˆì œ ê¸°ë°˜ ìƒì„±"))
            
            if not sql_query:
                self.logger.warning("Empty SQL query from simple SQL generation")
                return None
            
            self.logger.info(f"Simple SQL generated (confidence: {confidence:.2f}): {sql_query[:100]}...")
            
            return {
                "sql_query": sql_query,
                "confidence": confidence,
                "reasoning": reasoning
            }
            
        except Exception as e:
            self.logger.error(f"Error in simple SQL generation: {str(e)}", exc_info=True)
            return None
    
    def _generate_sql_complex(self, query: str, rag_context: Optional[str], schema_mapping: Optional[SchemaMapping]) -> Optional[str]:
        """
        RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™•í•œ SQL ìƒì„± (ê¸°ì¡´ SQLGeneration ë°©ì‹)
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            rag_context: RAG ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            schema_mapping: ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ë³´
            
        Returns:
            ìƒì„±ëœ SQL ì¿¼ë¦¬ ë¬¸ìì—´ ë˜ëŠ” None
        """
        try:
            if not self.llm:
                self.logger.warning("LLM not available for complex SQL generation")
                return None
            
            # ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì„¤ì •
            if schema_mapping:
                relevant_schema = {}
                for table_name in schema_mapping.relevant_tables:
                    if table_name in self.db_schema:
                        relevant_schema[table_name] = self.db_schema[table_name]
                self.prompt_template.set_schema(relevant_schema)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í†µí•©)
            prompt = self.prompt_template.create_prompt(
                user_query=query,
                include_relevant_examples=True,
                rag_context=rag_context if rag_context else None,
                max_context_length=4000
            )
            
            if rag_context:
                self.logger.debug(
                    f"Generated complex prompt with RAG context (length: {len(prompt)}, "
                    f"RAG context length: {len(rag_context)})"
                )
            
            # LLM í˜¸ì¶œ
            response = self.llm.invoke(prompt)
            response_content = response.content
            
            # ì‘ë‹µ ì²˜ë¦¬
            if isinstance(response_content, str):
                sql_query = response_content.strip()
            elif isinstance(response_content, list):
                sql_query = " ".join(str(item) for item in response_content).strip()
            else:
                sql_query = str(response_content).strip()
            
            # SQL ì¶”ì¶œ (```sql ... ``` í˜•íƒœì—ì„œ ì¶”ì¶œ)
            if "```sql" in sql_query:
                sql_query = sql_query.split("```sql")[1].split("```")[0].strip()
            elif "```" in sql_query:
                sql_query = sql_query.split("```")[1].split("```")[0].strip()
            
            if sql_query:
                self.logger.info(f"Complex SQL generated: {sql_query[:100]}...")
                return sql_query
            else:
                self.logger.warning("Empty SQL response from complex SQL generation")
                return None
                
        except Exception as e:
            self.logger.error(f"Error in complex SQL generation: {str(e)}", exc_info=True)
        return None


class SQLValidationNode(BaseNode):
    """SQL ê²€ì¦ ì—ì´ì „íŠ¸ ë…¸ë“œ - ìƒì„±ëœ SQLì˜ êµ¬ë¬¸ ë° ì˜ë¯¸ ê²€ì¦"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # db_schemaê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™” ì‹œì ì— í•œ ë²ˆë§Œ ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
        self.db_schema = config.get("db_schema") or {}
        if not self.db_schema or len(self.db_schema) == 0:
            from core.db import get_cached_db_schema
            self.db_schema = get_cached_db_schema()
            self.logger.debug("db_schema was empty in config, loaded from cache during initialization")
        
    def process(self, state: GraphState) -> GraphState:
        """SQL ì¿¼ë¦¬ ê²€ì¦"""
        self._log_processing(state, "SQLValidationNode")
        
        try:
            # ëŒ€í™” ì‘ë‹µì´ ìˆëŠ” ê²½ìš° ê²€ì¦ ê±´ë„ˆë›°ê¸°
            conversation_response = state.get("conversation_response")
            intent = state.get("intent")
            
            self.logger.info(f"SQLValidationNode - conversation_response: {conversation_response is not None}")
            self.logger.info(f"SQLValidationNode - intent: {intent}")
            
            if (conversation_response or 
                intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]):
                self.logger.info("Skipping SQL validation for conversation response")
                state["validation_result"] = {"is_valid": True, "message": "Conversation response - validation skipped"}
                state["is_valid"] = True
                return state
            
            sql_query = state.get("sql_query")
            if not sql_query:
                state["error_message"] = "No SQL query to validate"
                return state
            
            # SIMPLE_AGGREGATION ì¿¼ë¦¬ëŠ” ê°„ì†Œí™”ëœ ê²€ì¦ ìˆ˜í–‰ (í•˜ì§€ë§Œ ì¤‘ìš”í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì€ ìˆ˜í–‰)
            if intent == QueryIntent.SIMPLE_AGGREGATION:
                self.logger.info("SIMPLE_AGGREGATION: Performing simplified validation (syntax + security + critical schema checks)")
                
                # ê¸°ë³¸ êµ¬ë¬¸ ê²€ì¦
                syntax_validation = validate_sql_syntax(sql_query)
                
                # ë³´ì•ˆ ê²€ì¦ (í•„ìˆ˜)
                security_validation = self._validate_security(sql_query)
                
                # ì¤‘ìš”í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ë§Œ ìˆ˜í–‰ (t_member + ins_datetime ê°™ì€ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°©ì§€)
                critical_schema_validation = self._validate_critical_schema_issues(sql_query)
                
                # ì¢…í•© ê²€ì¦ ê²°ê³¼
                is_valid = (syntax_validation["is_valid"] and 
                           security_validation["is_valid"] and 
                           critical_schema_validation["is_valid"])
                
                validation_result = {
                    "is_valid": is_valid,
                    "confidence": syntax_validation.get("confidence", 0.8) if is_valid else 0.5,
                    "syntax_valid": syntax_validation["is_valid"],
                    "schema_valid": critical_schema_validation["is_valid"],
                    "security_valid": security_validation["is_valid"],
                    "message": "Simplified validation for SIMPLE_AGGREGATION (with critical schema checks)",
                    "simplified": True,
                    "corrections": critical_schema_validation.get("corrections", [])
                }
                
                # ìŠ¤í‚¤ë§ˆ ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš° ìë™ ì ìš©
                if not critical_schema_validation["is_valid"] and "corrections" in critical_schema_validation:
                    corrected_sql = self._apply_schema_corrections(sql_query, critical_schema_validation["corrections"])
                    if corrected_sql != sql_query:
                        self.logger.info(f"SQL auto-corrected for SIMPLE_AGGREGATION: {sql_query[:100]}... -> {corrected_sql[:100]}...")
                        state["sql_query"] = corrected_sql
                        state["sql_corrected"] = corrected_sql
                        # ìˆ˜ì • í›„ ë‹¤ì‹œ ê²€ì¦
                        validation_result["is_valid"] = True
                        validation_result["schema_valid"] = True
                        is_valid = True
                
                state["validation_result"] = validation_result
                state["is_valid"] = is_valid
                
                if not is_valid:
                    errors = []
                    if not syntax_validation["is_valid"]:
                        errors.extend(syntax_validation.get("errors", []))
                    if not security_validation["is_valid"]:
                        errors.extend(security_validation.get("errors", []))
                    if not critical_schema_validation["is_valid"]:
                        errors.extend(critical_schema_validation.get("issues", []))
                    state["error_message"] = "; ".join(errors)
                
                return state
            
            # COMPLEX_ANALYSIS ë° ê¸°íƒ€ ì¿¼ë¦¬ëŠ” ì „ì²´ ê²€ì¦ ìˆ˜í–‰
            # ê¸°ë³¸ êµ¬ë¬¸ ê²€ì¦ (ìƒˆë¡œìš´ SQL íŒŒì„œ ì‚¬ìš©)
            syntax_validation = validate_sql_syntax(sql_query)
            
            # ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦ (ì‹¤ì œ ê²€ì¦ ìˆ˜í–‰)
            schema_validation = self._validate_schema_compatibility(sql_query)
            
            # ë³´ì•ˆ ê²€ì¦
            security_validation = self._validate_security(sql_query)
            
            # ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ ìë™ ìˆ˜ì • ì‹œë„
            corrected_sql = sql_query
            if not schema_validation["is_valid"] and "corrections" in schema_validation:
                corrected_sql = self._apply_schema_corrections(sql_query, schema_validation["corrections"])
                if corrected_sql != sql_query:
                    self.logger.info(f"SQL auto-corrected: {sql_query[:100]}... -> {corrected_sql[:100]}...")
                    state["sql_query"] = corrected_sql
                    state["sql_corrected"] = corrected_sql
            
            # ì¢…í•© ê²€ì¦ ê²°ê³¼
            is_valid = all([
                syntax_validation["is_valid"],
                schema_validation["is_valid"],
                security_validation["is_valid"]
            ])
            
            validation_result = {
                "is_valid": is_valid,
                "syntax": syntax_validation,
                "schema": schema_validation,
                "security": security_validation,
                "suggestions": self._generate_suggestions(sql_query, [
                    syntax_validation,
                    schema_validation,
                    security_validation
                ])
            }
            
            state["sql_validation"] = validation_result
            state["validation_result"] = validation_result
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_validation_confidence(validation_result)
            state["confidence_scores"]["sql_validation"] = confidence
            
            if is_valid:
                state["validated_sql"] = sql_query
                state["sql_validation_failed"] = False
                self.logger.info("SQL validation passed")
            else:
                state["sql_validation_failed"] = True
                self.logger.warning(f"SQL validation failed: {validation_result['suggestions']}")
            
        except Exception as e:
            self.logger.error(f"Error in SQLValidationNode: {str(e)}")
            state["error_message"] = f"SQL validation failed: {str(e)}"
            state["confidence_scores"]["sql_validation"] = 0.0
        
        return state
    
    
    def _validate_schema_compatibility(self, sql_query: str) -> Dict[str, Any]:
        """ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦ ë° ìë™ ìˆ˜ì •"""
        try:
            issues = []
            corrections = []
            
            # ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ í™•ì¸ (ìºì‹±ëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            actual_schema = self.db_schema
            
            # í…Œì´ë¸”ëª… ê²€ì¦ (ìƒˆë¡œìš´ SQL íŒŒì„œ ì‚¬ìš©)
            table_names = extract_table_names(sql_query)
            for table_name in table_names:
                if table_name not in actual_schema:
                    # ìœ ì‚¬í•œ í…Œì´ë¸”ëª… ì°¾ê¸°
                    similar_table = self._find_similar_table(table_name, actual_schema)
                    if similar_table:
                        issues.append(f"Table '{table_name}' not found, did you mean '{similar_table}'?")
                        corrections.append(f"Replace '{table_name}' with '{similar_table}'")
                    else:
                        issues.append(f"Table '{table_name}' not found in schema")
            
            # ì»¬ëŸ¼ëª… ê²€ì¦ (íŠ¹íˆ ins_datetime ë¬¸ì œ)
            if 'ins_datetime' in sql_query:
                # t_member_login_log í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ins_datetime ì»¬ëŸ¼ì´ ì˜¬ë°”ë¦„
                if 't_member_login_log' in sql_query:
                    # t_member_login_log í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    login_log_table = actual_schema.get('t_member_login_log', {})
                    login_log_columns = login_log_table.get('columns', {})
                    if 'ins_datetime' not in login_log_columns:
                        issues.append("Column 'ins_datetime' not found in t_member_login_log table")
                    # t_member_login_logë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ëŠ” ìœ íš¨í•¨
                elif 't_member_info' in sql_query:
                    # t_member_info í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    member_info_table = actual_schema.get('t_member_info', {})
                    member_info_columns = member_info_table.get('columns', {})
                    if 'ins_datetime' not in member_info_columns:
                        issues.append("Column 'ins_datetime' not found in t_member_info table")
                        corrections.append("Verify t_member_info table schema")
                elif 't_member' in sql_query:
                    # t_member í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (t_memberëŠ” ins_datetimeì´ ì—†ìŒ)
                    member_table = actual_schema.get('t_member', {})
                    member_columns = member_table.get('columns', {})
                    if 'ins_datetime' not in member_columns:
                        issues.append("Column 'ins_datetime' not found in t_member table")
                        corrections.append("Use t_member_info table instead of t_member for ins_datetime column")
            
            return {
                "is_valid": len(issues) == 0,
                "issues": issues,
                "corrections": corrections,
                "details": "Schema validation completed"
            }
            
        except Exception as e:
            return {
                "is_valid": False,
                "error": "Schema validation error",
                "details": str(e)
            }
    
    
    def _find_similar_table(self, table_name: str, schema: Dict[str, Any]) -> Optional[str]:
        """ìœ ì‚¬í•œ í…Œì´ë¸”ëª… ì°¾ê¸°"""
        table_name_lower = table_name.lower()
        
        # ì •í™•í•œ ë§¤ì¹­
        if table_name in schema:
            return table_name
        
        # ë¶€ë¶„ ë§¤ì¹­
        for actual_table in schema.keys():
            if table_name_lower in actual_table.lower() or actual_table.lower() in table_name_lower:
                return actual_table
        
        return None
    
    def _validate_critical_schema_issues(self, sql_query: str) -> Dict[str, Any]:
        """
        SIMPLE_AGGREGATIONì„ ìœ„í•œ ì¤‘ìš”í•œ ìŠ¤í‚¤ë§ˆ ë¬¸ì œë§Œ ê²€ì¦
        (t_member + ins_datetime ê°™ì€ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°©ì§€)
        """
        issues = []
        corrections = []
        
        # t_member í…Œì´ë¸”ì— ins_datetime ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê²€ì¦
        if 't_member' in sql_query and 'ins_datetime' in sql_query:
            # t_member í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
            member_table = self.db_schema.get('t_member', {})
            member_columns = member_table.get('columns', {})
            
            if 'ins_datetime' not in member_columns:
                issues.append("Column 'ins_datetime' not found in t_member table")
                corrections.append("Use t_member_info table instead of t_member for ins_datetime column")
        
        return {
            "is_valid": len(issues) == 0,
            "issues": issues,
            "corrections": corrections,
            "message": "Critical schema validation for SIMPLE_AGGREGATION"
        }
    
    def _apply_schema_corrections(self, sql_query: str, corrections: List[str]) -> str:
        """ìŠ¤í‚¤ë§ˆ ìˆ˜ì •ì‚¬í•­ì„ SQLì— ì ìš©"""
        corrected_sql = sql_query
        
        for correction in corrections:
            # t_member í…Œì´ë¸”ì— ins_datetimeì´ ì—†ëŠ” ê²½ìš° t_member_infoë¡œ í…Œì´ë¸” ë³€ê²½
            # ë‹¨, t_fandingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ (t_fandingì— ins_datetimeê³¼ creator_noê°€ ëª¨ë‘ ìˆìŒ)
            if "Use t_member_info table instead of t_member for ins_datetime column" in correction:
                # t_fandingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë³€ê²½í•˜ì§€ ì•ŠìŒ
                if 't_fanding' not in corrected_sql.upper():
                    # t_memberë¥¼ t_member_infoë¡œ êµì²´ (ins_datetime ì»¬ëŸ¼ ì‚¬ìš© ì‹œ)
                    if 'ins_datetime' in sql_query:
                        corrected_sql = re.sub(r'\bt_member\b', 't_member_info', corrected_sql, flags=re.IGNORECASE)
                        self.logger.info("Replaced 't_member' with 't_member_info' for ins_datetime column")
                else:
                    self.logger.debug("Skipping t_member -> t_member_info replacement (t_fanding table is being used)")
            elif "Replace 'ins_datetime' with" in correction:
                # ins_datetimeì„ ëŒ€ì²´ ì»¬ëŸ¼ìœ¼ë¡œ êµì²´
                alt_col = correction.split("'")[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë”°ì˜´í‘œ ì•ˆì˜ ê°’
                corrected_sql = re.sub(r'\bins_datetime\b', alt_col, corrected_sql, flags=re.IGNORECASE)
                self.logger.info(f"Replaced 'ins_datetime' with '{alt_col}'")
        
        return corrected_sql
    
    def _validate_security(self, sql_query: str) -> Dict[str, Any]:
        """SQL ë³´ì•ˆ ê²€ì¦"""
        try:
            issues = []
            sql_upper = sql_query.upper()
            
            # ìœ„í—˜í•œ í‚¤ì›Œë“œ í™•ì¸ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤) - ìƒìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            for keyword in DANGEROUS_SQL_KEYWORDS:
                # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
                pattern = r'\b' + re.escape(keyword) + r'\b'
                if re.search(pattern, sql_upper):
                    issues.append(f"Dangerous keyword detected: {keyword}")
            
            # ì£¼ì„ í™•ì¸ (SQL ì¸ì ì…˜ ë°©ì§€) - í…œí”Œë¦¿ì˜ ì •ë‹¹í•œ ì£¼ì„ì€ í—ˆìš©
            # ë©€í‹°ë¼ì¸ ì£¼ì„ /* */ë§Œ ì°¨ë‹¨ (ë‹¨ì¼ ë¼ì¸ ì£¼ì„ -- ëŠ” í—ˆìš©)
            if '/*' in sql_query or '*/' in sql_query:
                issues.append("Suspicious multi-line comment detected")
            
            # ë‹¨ì¼ ë¼ì¸ ì£¼ì„(--)ì€ í—ˆìš© (í…œí”Œë¦¿ì—ì„œ ì •ë‹¹í•˜ê²Œ ì‚¬ìš©ë¨)
            
            return {
                "is_valid": len(issues) == 0,
                "issues": issues,
                "details": "Security validation completed"
            }
            
        except Exception as e:
            return {
                "is_valid": False,
                "error": "Security validation error",
                "details": str(e)
            }
    
    
    def _generate_suggestions(self, sql_query: str, validations: List[Dict[str, Any]]) -> List[str]:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        for validation in validations:
            if not validation["is_valid"]:
                if "issues" in validation:
                    suggestions.extend(validation["issues"])
                elif "error" in validation:
                    suggestions.append(validation["error"])
        
        return suggestions
    
    def _calculate_validation_confidence(self, validation_result: Dict[str, Any]) -> float:
        """ê²€ì¦ ì‹ ë¢°ë„ ê³„ì‚°"""
        if validation_result["is_valid"]:
            return 1.0
        
        # ê° ê²€ì¦ í•­ëª©ì˜ ê°€ì¤‘ì¹˜
        weights = {
            "syntax": 0.4,
            "schema": 0.4,
            "security": 0.2
        }
        
        confidence = 0.0
        for validation_type, weight in weights.items():
            if validation_result[validation_type]["is_valid"]:
                confidence += weight
        
        return confidence


class DataSummarizationNode(BaseNode):
    """ë°ì´í„° ìš”ì•½ ì—ì´ì „íŠ¸ ë…¸ë“œ - SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # LLM ì„œë¹„ìŠ¤ì—ì„œ SQL LLM ê°€ì ¸ì˜¤ê¸° (ìš”ì•½ìš©ìœ¼ë¡œë„ ì‚¬ìš©)
        self.llm = self._get_sql_llm()
        
    
    def process(self, state: GraphState) -> GraphState:
        """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½"""
        self._log_processing(state, "DataSummarizationNode")
        
        try:
            # ë¹„ë°ì´í„° ì˜ë„ (CHAT_PATH) ì²˜ë¦¬: ë´‡ ê¸°ëŠ¥ì— ë§ì¶˜ ì‘ë‹µ ìƒì„±
            intent = state.get("intent")
            user_query = state.get("user_query", "")
            conversation_response = state.get("conversation_response")
            
            self.logger.info(f"DataSummarizationNode - intent: {intent}")
            self.logger.info(f"DataSummarizationNode - conversation_response: {conversation_response is not None}")
            
            # ì´ë¯¸ conversation_responseê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ëª…í™•í™” ì§ˆë¬¸ ë“±)
            if conversation_response:
                self.logger.info("Using existing conversation_response")
                state["data_summary"] = conversation_response
                state["success"] = True
                return state
            
            # ë¹„ë°ì´í„° ì˜ë„ì— ëŒ€í•œ ë§¥ë½ì„ ê³ ë ¤í•œ ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±
            if intent in ["GREETING", "GENERAL_CHAT", "HELP_REQUEST"]:
                try:
                    intent_enum = QueryIntent(intent) if isinstance(intent, str) else intent
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
                    conversation_history = state.get("conversation_history", [])
                    
                    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì„ ê³ ë ¤í•œ ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±
                    # íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ë§¥ë½ì„ í™œìš©í•˜ê³ , ì—†ì–´ë„ LLMìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
                    if self.llm:
                        response = self._generate_contextual_response(
                            intent_enum, user_query, conversation_history
                        )
                    else:
                        # LLMì´ ì—†ëŠ” ê²½ìš° í…œí”Œë¦¿ ì‘ë‹µ ì‚¬ìš©
                        response = self._generate_template_response(intent_enum, user_query)
                    
                    self.logger.info(f"Generated conversation response for {intent}: {response[:50]}...")
                    state["data_summary"] = response
                    state["conversation_response"] = response
                    
                    # Update conversation history in state so LangGraph saves it to checkpointer
                    self._update_conversation_history_in_state(state, user_query, response)
                    
                    state["success"] = True
                    return state
                    
                except Exception as e:
                    self.logger.warning(f"Error generating conversation response: {e}, using fallback")
                    state["data_summary"] = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ“Š"
                    state["success"] = True
                    return state
            
            # ë°ì´í„° ì˜ë„ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ê³„ì† ìˆ˜í–‰
            fanding_template = state.get("fanding_template")
            
            # Fanding í…œí”Œë¦¿ì´ ìˆëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if fanding_template:
                query_result = state.get("query_result")
                if query_result:
                    # Fanding í…œí”Œë¦¿ ê²°ê³¼ í¬ë§·íŒ…
                    # db_schemaëŠ” configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì—†ìœ¼ë©´ ìë™ ë¡œë“œ
                    db_schema = state.get("db_schema") or getattr(self, 'db_schema', None)
                    templates = FandingSQLTemplates(db_schema=db_schema)
                    formatted_result = templates.format_sql_result(fanding_template, query_result)
                    state["data_summary"] = formatted_result
                    state["success"] = True
                    self.logger.info(f"ğŸ¯ Fanding template result formatted: {fanding_template.name}")
                    return state
            
            query_result = state.get("query_result")
            user_query = state.get("user_query")
            
            if not query_result:
                state["error_message"] = "No query result to summarize"
                return state
            
            # ê²°ê³¼ ë°ì´í„° ë¶„ì„
            result_stats = self._analyze_results(query_result)
            
            # Set default values (insight analyzer is disabled)
            state["insight_report"] = None
            state["business_insights"] = None
            
            # ìš”ì•½ ìƒì„±
            if self.llm:
                summary = self._generate_ai_summary(user_query, query_result, result_stats)
            else:
                summary = self._generate_fallback_summary(query_result, result_stats)
            
            state["data_summary"] = summary
            
            # Update conversation history in state so LangGraph saves it to checkpointer
            self._update_conversation_history_in_state(state, user_query, summary)
            
            # Ensure result_stats is a dict for result_statistics
            if isinstance(result_stats, dict):
                state["result_statistics"] = result_stats
            else:
                state["result_statistics"] = None
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_summary_confidence(summary, result_stats)
            state["confidence_scores"]["data_summarization"] = confidence
            
            self.logger.info(f"Generated summary: {summary[:100]}...")
            
        except Exception as e:
            self.logger.error(f"Error in DataSummarizationNode: {str(e)}")
            state["error_message"] = f"Data summarization failed: {str(e)}"
            state["confidence_scores"]["data_summarization"] = 0.0
        
        return state
    
    def _analyze_results(self, query_result: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ ë°ì´í„° í†µê³„ ë¶„ì„"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return {"row_count": 0, "columns": [], "data_types": {}}
        
        stats = {
            "row_count": len(query_result),
            "columns": list(query_result[0].keys()) if query_result else [],
            "data_types": {},
            "sample_values": {},
            "null_counts": {}
        }
        
        if query_result:
            # ë°ì´í„° íƒ€ì… ë¶„ì„
            for column in stats["columns"]:
                sample_values = [row.get(column) for row in query_result[:5]]
                stats["sample_values"][column] = sample_values
                
                # NULL ê°’ ê°œìˆ˜
                null_count = sum(1 for row in query_result if row.get(column) is None)
                stats["null_counts"][column] = null_count
                
                # ë°ì´í„° íƒ€ì… ì¶”ë¡ 
                non_null_values = [v for v in sample_values if v is not None]
                if non_null_values:
                    first_value = non_null_values[0]
                    if isinstance(first_value, int):
                        stats["data_types"][column] = "integer"
                    elif isinstance(first_value, float):
                        stats["data_types"][column] = "float"
                    elif isinstance(first_value, str):
                        stats["data_types"][column] = "string"
                    else:
                        stats["data_types"][column] = "unknown"
        
        return stats
    
    def _generate_ai_summary(self, user_query: str, query_result: List[Dict[str, Any]], stats: Dict[str, Any]) -> str:
        """AIë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±"""
        try:
            # ê²°ê³¼ ë°ì´í„° í¬ë§·íŒ…
            formatted_results = self._format_results(query_result)
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {user_query}

ì¿¼ë¦¬ ê²°ê³¼ í†µê³„:
- ì´ í–‰ ìˆ˜: {stats['row_count']}
- ì»¬ëŸ¼ ìˆ˜: {len(stats['columns'])}
- ì»¬ëŸ¼ëª…: {', '.join(stats['columns'])}

ìƒ˜í”Œ ë°ì´í„°:
{formatted_results[:500]}...

ìš”êµ¬ì‚¬í•­:
1. ê²°ê³¼ì˜ ì£¼ìš” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…
2. ë°ì´í„°ì˜ ê·œëª¨ì™€ íŠ¹ì§•ì„ ì–¸ê¸‰
3. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
4. 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
5. í•œêµ­ì–´ë¡œ ì‘ì„±

ìš”ì•½:
"""
            
            # ìµœì‹  LangChain ë°©ì‹: SystemMessage ëŒ€ì‹  HumanMessageì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
            messages = [
                HumanMessage(content=f"ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{summary_prompt}")
            ]
            
            if not self.llm:
                self.logger.warning("LLM not initialized, returning default summary")
                return "ë°ì´í„° ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            response = self.llm.invoke(messages)
            response_content = response.content
            # Handle different response types
            if isinstance(response_content, str):
                return response_content.strip()
            elif isinstance(response_content, list):
                # Extract text from list of content blocks
                return " ".join(str(item) for item in response_content).strip()
            else:
                return str(response_content).strip()
            
        except Exception as e:
            self.logger.error(f"AI summary generation failed: {e}")
            return self._generate_fallback_summary(query_result, stats)
    
    def _generate_contextual_response(
        self, 
        intent: QueryIntent, 
        user_query: str, 
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ë§¥ë½ì„ ê³ ë ¤í•œ ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±
        
        Args:
            intent: ì¸í…íŠ¸ íƒ€ì…
            user_query: í˜„ì¬ ì‚¬ìš©ì ì¿¼ë¦¬
            conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user|assistant", "content": "..."}]
            
        Returns:
            ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µ ë¬¸ìì—´
        """
        try:
            # íˆìŠ¤í† ë¦¬ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ (ì´ë¦„ ë“±)
            user_name = self._extract_user_name(conversation_history)
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            history_context = ""
            if conversation_history:
                history_context = "\n\n[ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬]\n"
                for msg in conversation_history[-5:]:  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role == "user":
                        history_context += f"ì‚¬ìš©ì: {content}\n"
                    elif role == "assistant":
                        history_context += f"ë´‡: {content}\n"
            
            # ì¸í…íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            if intent == QueryIntent.GREETING:
                system_prompt = """ë‹¹ì‹ ì€ Fanding ë°ì´í„° ì¡°íšŒ ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì¸ì‚¬ë¥¼ ê±´ë„¸ì„ ë•Œ, ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
"""
                if history_context:
                    system_prompt += history_context
                if user_name:
                    system_prompt += f"\nì¤‘ìš”: ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìì˜ ì´ë¦„ì´ '{user_name}'ìœ¼ë¡œ ì–¸ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™”ëœ ì¸ì‚¬ë¥¼ í•˜ì„¸ìš”."
                else:
                    system_prompt += "\nì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¸ì‚¬ë§ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
                
                user_prompt = f"""í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_query}

ìœ„ íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì¸ì‚¬ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
ì´ë¦„ì´ ì–¸ê¸‰ë˜ì—ˆë‹¤ë©´ ë°˜ë“œì‹œ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”."""
            
            elif intent == QueryIntent.GENERAL_CHAT:
                system_prompt = """ë‹¹ì‹ ì€ Fanding ë°ì´í„° ì¡°íšŒ ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ì˜ ì¼ë°˜ì ì¸ ëŒ€í™”ì—ì„œ, ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ë§¥ë½ì— ë§ëŠ” ì‘ë‹µì„ í•˜ì„¸ìš”.
íŠ¹íˆ ì‚¬ìš©ìê°€ ì´ì „ì— ì–¸ê¸‰í•œ ì •ë³´(ì´ë¦„, ì„ í˜¸ì‚¬í•­ ë“±)ë¥¼ ê¸°ì–µí•˜ê³  í™œìš©í•˜ì„¸ìš”.
"""
                if history_context:
                    system_prompt += history_context
                if user_name:
                    system_prompt += f"\nì¤‘ìš”: ì‚¬ìš©ìì˜ ì´ë¦„ì€ '{user_name}'ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì´ë¦„ì´ í¬í•¨ë˜ë©´ ì´ë¥¼ í™œìš©í•˜ì„¸ìš”."
                
                user_prompt = f"""í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_query}

ìœ„ íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ë§¥ë½ì— ë§ëŠ” ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
ì‚¬ìš©ìê°€ ì´ë¦„ì„ ë¬¼ì–´ë³´ë©´ ì´ì „ì— ì–¸ê¸‰ëœ ì´ë¦„ì„ ì•Œë ¤ì£¼ì„¸ìš”.
"""
            
            else:  # HELP_REQUEST
                system_prompt = """ë‹¹ì‹ ì€ Fanding ë°ì´í„° ì¡°íšŒ ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì—ê²Œ ë´‡ì˜ ê¸°ëŠ¥ê³¼ ì‚¬ìš©ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”.
"""
                if history_context:
                    system_prompt += history_context
                user_prompt = f"""í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_query}

ë´‡ì˜ ê¸°ëŠ¥ê³¼ ì‚¬ìš©ë²•ì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”."""
            
            # LLM í˜¸ì¶œ
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            
            # ì‘ë‹µ ì¶”ì¶œ
            if hasattr(response, 'content'):
                response_text = response.content
                if isinstance(response_text, str):
                    return response_text.strip()
                elif isinstance(response_text, list):
                    return " ".join(str(item) for item in response_text).strip()
                else:
                    return str(response_text).strip()
            else:
                return str(response).strip()
                
        except Exception as e:
            self.logger.warning(f"Error generating contextual response: {e}, using template")
            return self._generate_template_response(intent, user_query)
    
    def _update_conversation_history_in_state(
        self, 
        state: GraphState, 
        user_query: str, 
        assistant_response: str
    ) -> None:
        """
        Update conversation history in state.
        
        This ensures that LangGraph automatically saves the updated history to checkpointer.
        
        Args:
            state: Current graph state
            user_query: Current user query
            assistant_response: Assistant's response
        """
        try:
            # Get existing history or initialize empty list
            history = state.get("conversation_history", [])
            if not isinstance(history, list):
                history = []
            
            # Add current user query
            history.append({
                "role": "user",
                "content": user_query
            })
            
            # Add assistant response
            history.append({
                "role": "assistant",
                "content": assistant_response
            })
            
            # Limit history size to prevent token overflow
            max_history = 20  # Keep last 20 messages (10 user + 10 assistant)
            if len(history) > max_history:
                history = history[-max_history:]
            
            # Update state with updated history
            state["conversation_history"] = history
            
            self.logger.debug(f"Updated conversation history: {len(history)} messages total")
            
        except Exception as e:
            self.logger.warning(f"Failed to update conversation history in state: {str(e)}")
            # Non-critical error, continue execution
    
    def _extract_user_name(self, conversation_history: List[Dict[str, str]]) -> Optional[str]:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ì‚¬ìš©ì ì´ë¦„ ì¶”ì¶œ
        
        Args:
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            
        Returns:
            ì¶”ì¶œëœ ì´ë¦„ ë˜ëŠ” None
        """
        # ì´ë¦„ ì–¸ê¸‰ íŒ¨í„´ (í•œêµ­ì–´/ì˜ì–´)
        name_patterns = [
            r"ë‚´ ì´ë¦„ì€\s+([ê°€-í£a-zA-Z]+)",
            r"ë‚´ ì´ë¦„ì´\s+([ê°€-í£a-zA-Z]+)",
            r"ì œ ì´ë¦„ì€\s+([ê°€-í£a-zA-Z]+)",
            r"ì œ ì´ë¦„ì´\s+([ê°€-í£a-zA-Z]+)",
            r"ë‚˜ëŠ”\s+([ê°€-í£a-zA-Z]+)",
            r"ì €ëŠ”\s+([ê°€-í£a-zA-Z]+)",
            r"([ê°€-í£]{2,4})ë¼ê³ \s*(?:í•´|í•©ë‹ˆë‹¤)",
            r"([ê°€-í£]{2,4})ë¼ê³ \s*(?:ë¶ˆëŸ¬|ë¶€ë¥´)",
        ]
        
        # ìµœê·¼ ë©”ì‹œì§€ë¶€í„° ì—­ìˆœìœ¼ë¡œ ê²€ìƒ‰
        for msg in reversed(conversation_history):
            if msg.get("role") != "user":
                continue
                
            content = msg.get("content", "")
            if not content:
                continue
            
            for pattern in name_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    name = match.group(1).strip()
                    # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì´ë¦„ ì œì™¸
                    if 2 <= len(name) <= 20:
                        self.logger.info(f"Extracted user name from history: {name}")
                        return name
        
        return None
    
    def _generate_template_response(self, intent: QueryIntent, user_query: str) -> str:
        """
        í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ ìƒì„± (fallback)
        
        Args:
            intent: ì¸í…íŠ¸ íƒ€ì…
            user_query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            í…œí”Œë¦¿ ì‘ë‹µ ë¬¸ìì—´
        """
        if intent == QueryIntent.GREETING:
            return generate_greeting_response(user_query)
        elif intent == QueryIntent.HELP_REQUEST:
            return generate_help_response(user_query)
        elif intent == QueryIntent.GENERAL_CHAT:
            return generate_general_chat_response(user_query)
        else:
            return "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ“Š"
    
    def _generate_fallback_summary(self, query_result: List[Dict[str, Any]], stats: Dict[str, Any]) -> str:
        """Fallback ìš”ì•½ ìƒì„±"""
        row_count = stats.get("row_count", 0)
        columns = stats.get("columns", [])
        
        if row_count == 0:
            return "ì¿¼ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        elif row_count == 1:
            return f"ì´ 1ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
        else:
            return f"ì´ {row_count}ê°œì˜ ê²°ê³¼ê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {', '.join(columns)}"
    
    def _format_results(self, query_result: List[Dict[str, Any]], max_rows: int = 10) -> str:
        """ê²°ê³¼ ë°ì´í„°ë¥¼ í¬ë§·íŒ…"""
        # NoneType ì—ëŸ¬ ë°©ì§€
        if not query_result or query_result is None:
            return "ê²°ê³¼ ì—†ìŒ"
        
        formatted_rows = []
        for i, row in enumerate(query_result[:max_rows]):
            row_str = f"í–‰ {i+1}: {dict(row)}"
            formatted_rows.append(row_str)
        
        result = "\n".join(formatted_rows)
        
        if len(query_result) > max_rows:
            result += f"\n... ë° {len(query_result) - max_rows}ê°œ í–‰ ë”"
        
        return result
    
    def _calculate_summary_confidence(self, summary: str, stats: Dict[str, Any]) -> float:
        """ìš”ì•½ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.8
        
        # ìš”ì•½ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        if len(summary) > 50:
            base_confidence += 0.1
        
        # í†µê³„ ì •ë³´ í™œìš©ë„ì— ë”°ë¥¸ ì¡°ì •
        if stats.get("row_count", 0) > 0:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)

